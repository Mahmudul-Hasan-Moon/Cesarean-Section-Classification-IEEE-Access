{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1343052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('Cesarean scaled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e41c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c765275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>AGE</th>\n",
       "      <th>HEIGHT</th>\n",
       "      <th>WEIGHT</th>\n",
       "      <th>BMI</th>\n",
       "      <th>KG INCREASED PREGNANCY</th>\n",
       "      <th>NUMBER OF PREV CESAREAN</th>\n",
       "      <th>PREVIOUS TERM PREGNANCIES</th>\n",
       "      <th>PREVIOUS PRETERM PREGNANCIES</th>\n",
       "      <th>MISCARRIAGES</th>\n",
       "      <th>...</th>\n",
       "      <th>GROUP_ group 2a</th>\n",
       "      <th>GROUP_ group 2b</th>\n",
       "      <th>GROUP_ group 3</th>\n",
       "      <th>GROUP_ group 4a</th>\n",
       "      <th>GROUP_ group 4b</th>\n",
       "      <th>GROUP_ group 5</th>\n",
       "      <th>GROUP_ group 6</th>\n",
       "      <th>GROUP_ group 7</th>\n",
       "      <th>GROUP_ group 8</th>\n",
       "      <th>GROUP_ group 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.456522</td>\n",
       "      <td>-0.013387</td>\n",
       "      <td>-0.117853</td>\n",
       "      <td>-0.078010</td>\n",
       "      <td>1.242926</td>\n",
       "      <td>-0.336507</td>\n",
       "      <td>-0.918877</td>\n",
       "      <td>-0.175854</td>\n",
       "      <td>0.737782</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.456522</td>\n",
       "      <td>-0.056492</td>\n",
       "      <td>-0.186644</td>\n",
       "      <td>0.445331</td>\n",
       "      <td>0.740352</td>\n",
       "      <td>-0.336507</td>\n",
       "      <td>-0.918877</td>\n",
       "      <td>-0.175854</td>\n",
       "      <td>-0.538208</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.348610</td>\n",
       "      <td>0.020140</td>\n",
       "      <td>0.157309</td>\n",
       "      <td>-0.199717</td>\n",
       "      <td>0.907877</td>\n",
       "      <td>-0.336507</td>\n",
       "      <td>-0.918877</td>\n",
       "      <td>-0.175854</td>\n",
       "      <td>-0.538208</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.128605</td>\n",
       "      <td>-0.066071</td>\n",
       "      <td>-1.080921</td>\n",
       "      <td>-0.579038</td>\n",
       "      <td>-0.599846</td>\n",
       "      <td>-0.336507</td>\n",
       "      <td>-0.918877</td>\n",
       "      <td>-0.175854</td>\n",
       "      <td>-0.538208</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.574649</td>\n",
       "      <td>0.005771</td>\n",
       "      <td>0.501262</td>\n",
       "      <td>0.352022</td>\n",
       "      <td>1.913025</td>\n",
       "      <td>-0.336507</td>\n",
       "      <td>-0.918877</td>\n",
       "      <td>-0.175854</td>\n",
       "      <td>-0.538208</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       AGE    HEIGHT    WEIGHT       BMI  KG INCREASED PREGNANCY  \\\n",
       "0           0 -1.456522 -0.013387 -0.117853 -0.078010                1.242926   \n",
       "1           1 -1.456522 -0.056492 -0.186644  0.445331                0.740352   \n",
       "2           2 -0.348610  0.020140  0.157309 -0.199717                0.907877   \n",
       "3           3  1.128605 -0.066071 -1.080921 -0.579038               -0.599846   \n",
       "4           4  0.574649  0.005771  0.501262  0.352022                1.913025   \n",
       "\n",
       "   NUMBER OF PREV CESAREAN  PREVIOUS TERM PREGNANCIES  \\\n",
       "0                -0.336507                  -0.918877   \n",
       "1                -0.336507                  -0.918877   \n",
       "2                -0.336507                  -0.918877   \n",
       "3                -0.336507                  -0.918877   \n",
       "4                -0.336507                  -0.918877   \n",
       "\n",
       "   PREVIOUS PRETERM PREGNANCIES  MISCARRIAGES   ...  GROUP_ group 2a      \\\n",
       "0                     -0.175854       0.737782  ...                    0   \n",
       "1                     -0.175854      -0.538208  ...                    1   \n",
       "2                     -0.175854      -0.538208  ...                    0   \n",
       "3                     -0.175854      -0.538208  ...                    1   \n",
       "4                     -0.175854      -0.538208  ...                    1   \n",
       "\n",
       "   GROUP_ group 2b      GROUP_ group 3       GROUP_ group 4a      \\\n",
       "0                    0                    0                    0   \n",
       "1                    0                    0                    0   \n",
       "2                    0                    0                    0   \n",
       "3                    0                    0                    0   \n",
       "4                    0                    0                    0   \n",
       "\n",
       "   GROUP_ group 4b      GROUP_ group 5       GROUP_ group 6       \\\n",
       "0                    0                    0                    0   \n",
       "1                    0                    0                    0   \n",
       "2                    0                    0                    0   \n",
       "3                    0                    0                    0   \n",
       "4                    0                    0                    0   \n",
       "\n",
       "   GROUP_ group 7       GROUP_ group 8       GROUP_ group 9       \n",
       "0                    0                    0                    0  \n",
       "1                    0                    0                    0  \n",
       "2                    0                    0                    0  \n",
       "3                    0                    0                    0  \n",
       "4                    0                    0                    0  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c95b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2e410d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "676e06ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import matplotlib.pyplot as plt\\nimport seaborn as sb\\n%matplotlib inline '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "%matplotlib inline \"\"\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "181bd3fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplt.matshow(data.corr(), cmap = \"summer\")                    \\nplt.colorbar()\\nplt.xticks(list(range(len(data.columns))), data.columns, rotation = \\'vertical\\')\\nplt.yticks(list(range(len(data.columns))), data.columns, rotation = \\'horizontal\\')\\nplt.show()\\n\\nsb.set(rc = {\\'figure.figsize\\':(16,10)})\\nsb.heatmap(data.corr(),annot=True, cmap=\"Greens\")\\n\\n\\nplt.savefig(\\'Heatmap(2)\\',dpi=500)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "plt.matshow(data.corr(), cmap = \"summer\")                    \n",
    "plt.colorbar()\n",
    "plt.xticks(list(range(len(data.columns))), data.columns, rotation = 'vertical')\n",
    "plt.yticks(list(range(len(data.columns))), data.columns, rotation = 'horizontal')\n",
    "plt.show()\n",
    "\n",
    "sb.set(rc = {'figure.figsize':(16,10)})\n",
    "sb.heatmap(data.corr(),annot=True, cmap=\"Greens\")\n",
    "\n",
    "\n",
    "plt.savefig('Heatmap(2)',dpi=500)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68e38bc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    5465\n",
      "0     692\n",
      "Name: TYPE OF BIRTH    , dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data['TYPE OF BIRTH    '].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dbabb0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>AGE</th>\n",
       "      <th>HEIGHT</th>\n",
       "      <th>WEIGHT</th>\n",
       "      <th>BMI</th>\n",
       "      <th>KG INCREASED PREGNANCY</th>\n",
       "      <th>NUMBER OF PREV CESAREAN</th>\n",
       "      <th>PREVIOUS TERM PREGNANCIES</th>\n",
       "      <th>PREVIOUS PRETERM PREGNANCIES</th>\n",
       "      <th>MISCARRIAGES</th>\n",
       "      <th>...</th>\n",
       "      <th>GROUP_ group 2a</th>\n",
       "      <th>GROUP_ group 2b</th>\n",
       "      <th>GROUP_ group 3</th>\n",
       "      <th>GROUP_ group 4a</th>\n",
       "      <th>GROUP_ group 4b</th>\n",
       "      <th>GROUP_ group 5</th>\n",
       "      <th>GROUP_ group 6</th>\n",
       "      <th>GROUP_ group 7</th>\n",
       "      <th>GROUP_ group 8</th>\n",
       "      <th>GROUP_ group 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6157.000000</td>\n",
       "      <td>6.157000e+03</td>\n",
       "      <td>6.157000e+03</td>\n",
       "      <td>6.157000e+03</td>\n",
       "      <td>6.157000e+03</td>\n",
       "      <td>6.157000e+03</td>\n",
       "      <td>6.157000e+03</td>\n",
       "      <td>6.157000e+03</td>\n",
       "      <td>6.157000e+03</td>\n",
       "      <td>6.157000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>6157.000000</td>\n",
       "      <td>6157.000000</td>\n",
       "      <td>6157.000000</td>\n",
       "      <td>6157.000000</td>\n",
       "      <td>6157.000000</td>\n",
       "      <td>6157.000000</td>\n",
       "      <td>6157.000000</td>\n",
       "      <td>6157.000000</td>\n",
       "      <td>6157.000000</td>\n",
       "      <td>6157.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3078.000000</td>\n",
       "      <td>1.459501e-16</td>\n",
       "      <td>-8.600476e-17</td>\n",
       "      <td>-9.715578e-17</td>\n",
       "      <td>1.287476e-16</td>\n",
       "      <td>-1.278370e-15</td>\n",
       "      <td>3.517804e-15</td>\n",
       "      <td>-5.543920e-15</td>\n",
       "      <td>-6.787291e-16</td>\n",
       "      <td>5.387927e-17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144876</td>\n",
       "      <td>0.002924</td>\n",
       "      <td>0.351470</td>\n",
       "      <td>0.108819</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.069677</td>\n",
       "      <td>0.017703</td>\n",
       "      <td>0.017216</td>\n",
       "      <td>0.004060</td>\n",
       "      <td>0.002436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1777.517135</td>\n",
       "      <td>1.000081e+00</td>\n",
       "      <td>1.000081e+00</td>\n",
       "      <td>1.000081e+00</td>\n",
       "      <td>1.000081e+00</td>\n",
       "      <td>1.000081e+00</td>\n",
       "      <td>1.000081e+00</td>\n",
       "      <td>1.000081e+00</td>\n",
       "      <td>1.000081e+00</td>\n",
       "      <td>1.000081e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.352004</td>\n",
       "      <td>0.053995</td>\n",
       "      <td>0.477468</td>\n",
       "      <td>0.311438</td>\n",
       "      <td>0.031204</td>\n",
       "      <td>0.254622</td>\n",
       "      <td>0.131882</td>\n",
       "      <td>0.130087</td>\n",
       "      <td>0.063597</td>\n",
       "      <td>0.049302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.303040e+00</td>\n",
       "      <td>-7.844921e-01</td>\n",
       "      <td>-4.520448e+00</td>\n",
       "      <td>-5.086259e+00</td>\n",
       "      <td>-4.285390e+00</td>\n",
       "      <td>-3.365074e-01</td>\n",
       "      <td>-9.188772e-01</td>\n",
       "      <td>-1.758542e-01</td>\n",
       "      <td>-5.382077e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1539.000000</td>\n",
       "      <td>-7.179141e-01</td>\n",
       "      <td>-2.775515e-02</td>\n",
       "      <td>-5.993872e-01</td>\n",
       "      <td>-7.129156e-01</td>\n",
       "      <td>-5.998457e-01</td>\n",
       "      <td>-3.365074e-01</td>\n",
       "      <td>-9.188772e-01</td>\n",
       "      <td>-1.758542e-01</td>\n",
       "      <td>-5.382077e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3078.000000</td>\n",
       "      <td>2.069349e-02</td>\n",
       "      <td>-8.597251e-03</td>\n",
       "      <td>-1.866439e-01</td>\n",
       "      <td>-2.179732e-01</td>\n",
       "      <td>7.025318e-02</td>\n",
       "      <td>-3.365074e-01</td>\n",
       "      <td>1.583366e-01</td>\n",
       "      <td>-1.758542e-01</td>\n",
       "      <td>-5.382077e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4617.000000</td>\n",
       "      <td>7.593010e-01</td>\n",
       "      <td>1.535012e-02</td>\n",
       "      <td>5.012615e-01</td>\n",
       "      <td>5.082129e-01</td>\n",
       "      <td>5.728274e-01</td>\n",
       "      <td>-3.365074e-01</td>\n",
       "      <td>1.583366e-01</td>\n",
       "      <td>-1.758542e-01</td>\n",
       "      <td>7.377818e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6156.000000</td>\n",
       "      <td>3.159776e+00</td>\n",
       "      <td>7.824184e+01</td>\n",
       "      <td>7.105154e+00</td>\n",
       "      <td>6.916095e+00</td>\n",
       "      <td>3.923322e+00</td>\n",
       "      <td>7.874362e+00</td>\n",
       "      <td>9.853261e+00</td>\n",
       "      <td>1.890316e+01</td>\n",
       "      <td>1.349768e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0           AGE        HEIGHT        WEIGHT           BMI  \\\n",
       "count  6157.000000  6.157000e+03  6.157000e+03  6.157000e+03  6.157000e+03   \n",
       "mean   3078.000000  1.459501e-16 -8.600476e-17 -9.715578e-17  1.287476e-16   \n",
       "std    1777.517135  1.000081e+00  1.000081e+00  1.000081e+00  1.000081e+00   \n",
       "min       0.000000 -3.303040e+00 -7.844921e-01 -4.520448e+00 -5.086259e+00   \n",
       "25%    1539.000000 -7.179141e-01 -2.775515e-02 -5.993872e-01 -7.129156e-01   \n",
       "50%    3078.000000  2.069349e-02 -8.597251e-03 -1.866439e-01 -2.179732e-01   \n",
       "75%    4617.000000  7.593010e-01  1.535012e-02  5.012615e-01  5.082129e-01   \n",
       "max    6156.000000  3.159776e+00  7.824184e+01  7.105154e+00  6.916095e+00   \n",
       "\n",
       "       KG INCREASED PREGNANCY  NUMBER OF PREV CESAREAN  \\\n",
       "count            6.157000e+03             6.157000e+03   \n",
       "mean            -1.278370e-15             3.517804e-15   \n",
       "std              1.000081e+00             1.000081e+00   \n",
       "min             -4.285390e+00            -3.365074e-01   \n",
       "25%             -5.998457e-01            -3.365074e-01   \n",
       "50%              7.025318e-02            -3.365074e-01   \n",
       "75%              5.728274e-01            -3.365074e-01   \n",
       "max              3.923322e+00             7.874362e+00   \n",
       "\n",
       "       PREVIOUS TERM PREGNANCIES  PREVIOUS PRETERM PREGNANCIES  MISCARRIAGES   \\\n",
       "count               6.157000e+03                  6.157000e+03   6.157000e+03   \n",
       "mean               -5.543920e-15                 -6.787291e-16   5.387927e-17   \n",
       "std                 1.000081e+00                  1.000081e+00   1.000081e+00   \n",
       "min                -9.188772e-01                 -1.758542e-01  -5.382077e-01   \n",
       "25%                -9.188772e-01                 -1.758542e-01  -5.382077e-01   \n",
       "50%                 1.583366e-01                 -1.758542e-01  -5.382077e-01   \n",
       "75%                 1.583366e-01                 -1.758542e-01   7.377818e-01   \n",
       "max                 9.853261e+00                  1.890316e+01   1.349768e+01   \n",
       "\n",
       "       ...  GROUP_ group 2a      GROUP_ group 2b      GROUP_ group 3       \\\n",
       "count  ...          6157.000000          6157.000000          6157.000000   \n",
       "mean   ...             0.144876             0.002924             0.351470   \n",
       "std    ...             0.352004             0.053995             0.477468   \n",
       "min    ...             0.000000             0.000000             0.000000   \n",
       "25%    ...             0.000000             0.000000             0.000000   \n",
       "50%    ...             0.000000             0.000000             0.000000   \n",
       "75%    ...             0.000000             0.000000             1.000000   \n",
       "max    ...             1.000000             1.000000             1.000000   \n",
       "\n",
       "       GROUP_ group 4a      GROUP_ group 4b      GROUP_ group 5       \\\n",
       "count          6157.000000          6157.000000          6157.000000   \n",
       "mean              0.108819             0.000975             0.069677   \n",
       "std               0.311438             0.031204             0.254622   \n",
       "min               0.000000             0.000000             0.000000   \n",
       "25%               0.000000             0.000000             0.000000   \n",
       "50%               0.000000             0.000000             0.000000   \n",
       "75%               0.000000             0.000000             0.000000   \n",
       "max               1.000000             1.000000             1.000000   \n",
       "\n",
       "       GROUP_ group 6       GROUP_ group 7       GROUP_ group 8       \\\n",
       "count          6157.000000          6157.000000          6157.000000   \n",
       "mean              0.017703             0.017216             0.004060   \n",
       "std               0.131882             0.130087             0.063597   \n",
       "min               0.000000             0.000000             0.000000   \n",
       "25%               0.000000             0.000000             0.000000   \n",
       "50%               0.000000             0.000000             0.000000   \n",
       "75%               0.000000             0.000000             0.000000   \n",
       "max               1.000000             1.000000             1.000000   \n",
       "\n",
       "       GROUP_ group 9       \n",
       "count          6157.000000  \n",
       "mean              0.002436  \n",
       "std               0.049302  \n",
       "min               0.000000  \n",
       "25%               0.000000  \n",
       "50%               0.000000  \n",
       "75%               0.000000  \n",
       "max               1.000000  \n",
       "\n",
       "[8 rows x 57 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b66f30",
   "metadata": {},
   "source": [
    "# Without Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bd33ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9534506089309879\n",
      "0.9534506089309879\n",
      "[[ 284  131]\n",
      " [  41 3239]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.68      0.77       415\n",
      "           1       0.96      0.99      0.97      3280\n",
      "\n",
      "    accuracy                           0.95      3695\n",
      "   macro avg       0.92      0.84      0.87      3695\n",
      "weighted avg       0.95      0.95      0.95      3695\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "X = data.drop(['TYPE OF BIRTH    '], axis = 1)\n",
    "'''X = data[['AGE', 'WEIGHT', 'BMI', 'KG INCREASED PREGNANCY', 'PREVIOUS TERM PREGNANCIES','PARITY', 'GESTAGIONAL AGE ', 'COUPLE SITUATION ', 'ART', 'AMNIOCENTESIS', 'PREVIOUS CESAREAN',\n",
    "       'COMORBIDITY', 'PREINDUCTION', 'INDUCTION',\n",
    "       'ANESTHESIA ', 'EPISIOTOMY', 'Fetal INTRAPARTUM pH', 'COMPLICATIONS',\n",
    "        'LIQUID_ Hemorr�gico  ',\n",
    "       'LIQUID_ clear        ', 'LIQUID_ stained +    ',\n",
    "       'LIQUID_ stained ++   ', 'LIQUID_ stained +++  ', 'GROUP_ group 1     ',\n",
    "       'GROUP_ group 10    ', 'GROUP_ group 2a    ', 'GROUP_ group 2b    ',\n",
    "       'GROUP_ group 3     ', 'GROUP_ group 4a    ', 'GROUP_ group 4b    ',\n",
    "       'GROUP_ group 5     ', 'GROUP_ group 6     ', 'GROUP_ group 7     ',\n",
    "       'GROUP_ group 8     ', 'GROUP_ group 9     ']]'''\n",
    "\n",
    "Y = data['TYPE OF BIRTH    ']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.6,random_state = 109) # 80% training and 20% test\n",
    "\n",
    "\n",
    "# Simple Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "modelLR1 = LogisticRegression()\n",
    "modelLR1.fit(x_train, y_train)\n",
    "print(modelLR1.score(x_test, y_test))\n",
    "\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = modelLR1.predict(x_test)\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "print(ac)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = modelLR1.predict(x_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcb3c96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity/TPR: 0.9875\n",
      "FPR: 0.3156626506024096\n",
      "Specificity/TNR: 0.6843373493975904\n",
      "MCC: 0.7489770031237655\n",
      "Cohen Kappa: 0.7421273660505975\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "cmSVC = confusion_matrix(y_test, modelLR1.predict(x_test))\n",
    "\n",
    "#If we use TP TN FP and FN of below's comment we get range\n",
    "#FP = cmSVC.sum(axis=0) - np.diag(cmSVC)  \n",
    "#FN = cmSVC.sum(axis=1) - np.diag(cmSVC)\n",
    "#TP = np.diag(cmSVC)\n",
    "#TN = cmSVC.sum() - (FP + FN + TP)\n",
    "\n",
    "TP = cmSVC[1,1]  \n",
    "TN = cmSVC[0,0] \n",
    "FP = cmSVC[0,1] \n",
    "FN = cmSVC[1,0] \n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/float(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/float(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/float(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/float(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/float(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/float(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/float(TP+FP)\n",
    "totalSVC=sum(sum(cmSVC))\n",
    "Accuracy = (TN+TP)/totalSVC\n",
    "# MCC\n",
    "val = (TP * TN) - (FP * FN)\n",
    "MCC_SVC = val / np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "# Cohen Kappa\n",
    "Y_pred = modelLR1.predict(x_test)\n",
    "cohen_score = cohen_kappa_score(y_test, Y_pred)\n",
    "\n",
    "print(\"Sensitivity/TPR: \" + str(TPR))\n",
    "print(\"FPR: \" + str(FPR))\n",
    "print(\"Specificity/TNR: \" + str(TNR))\n",
    "print(\"MCC: \" + str(MCC_SVC))\n",
    "print(\"Cohen Kappa: \" + str(cohen_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1eca78",
   "metadata": {},
   "source": [
    "## StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e870f0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.68      0.72       692\n",
      "           1       0.96      0.97      0.97      5465\n",
      "\n",
      "    accuracy                           0.94      6157\n",
      "   macro avg       0.86      0.83      0.84      6157\n",
      "weighted avg       0.94      0.94      0.94      6157\n",
      "\n",
      "Accuracy =  [0.9788961  0.97727273 0.86688312 0.92857143 0.94155844 0.91071429\n",
      " 0.96428571 0.99512195 0.9398374  0.89918699]\n",
      "Accuracy mean and std : 0.940 (0.038)\n"
     ]
    }
   ],
   "source": [
    "# KFold cross validation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv=StratifiedKFold(n_splits = 10)\n",
    "\n",
    "class_names = ['0','1']\n",
    "y_pred = cross_val_predict(modelLR1, X, Y, cv = cv)\n",
    "\n",
    "print(classification_report(Y, y_pred, target_names=class_names))\n",
    "\n",
    "cv_score_for_LR = cross_val_score(modelLR1, X, Y, cv = 10)\n",
    "print(\"Accuracy = \",cv_score_for_LR)\n",
    "\n",
    "print('Accuracy mean and std : %.3f (%.3f)' % (mean(cv_score_for_LR), std(cv_score_for_LR)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91bb2d6",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dad2aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.33493768\n",
      "Iteration 2, loss = 2.08238020\n",
      "Iteration 3, loss = 1.61630052\n",
      "Iteration 4, loss = 0.52314400\n",
      "Iteration 5, loss = 0.34341739\n",
      "Iteration 6, loss = 0.27889063\n",
      "Iteration 7, loss = 0.27569033\n",
      "Iteration 8, loss = 0.28550830\n",
      "Iteration 9, loss = 0.25977482\n",
      "Iteration 10, loss = 0.53167098\n",
      "Iteration 11, loss = 2.53520066\n",
      "Iteration 12, loss = 0.39636195\n",
      "Iteration 13, loss = 0.27545081\n",
      "Iteration 14, loss = 0.25288708\n",
      "Iteration 15, loss = 0.24824798\n",
      "Iteration 16, loss = 0.23189367\n",
      "Iteration 17, loss = 0.22639899\n",
      "Iteration 18, loss = 0.22147258\n",
      "Iteration 19, loss = 0.23151932\n",
      "Iteration 20, loss = 0.22819477\n",
      "Iteration 21, loss = 0.23603289\n",
      "Iteration 22, loss = 0.22857352\n",
      "Iteration 23, loss = 0.22180052\n",
      "Iteration 24, loss = 0.58233174\n",
      "Iteration 25, loss = 1.63791315\n",
      "Iteration 26, loss = 1.95681017\n",
      "Iteration 27, loss = 1.75730462\n",
      "Iteration 28, loss = 1.56671443\n",
      "Iteration 29, loss = 0.98499959\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8868741542625169\n",
      "0.8868741542625169\n",
      "[[   0  415]\n",
      " [   3 3277]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       415\n",
      "           1       0.89      1.00      0.94      3280\n",
      "\n",
      "    accuracy                           0.89      3695\n",
      "   macro avg       0.44      0.50      0.47      3695\n",
      "weighted avg       0.79      0.89      0.83      3695\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "modelMLP1 = MLPClassifier(hidden_layer_sizes = (6,5), random_state = 42, verbose = True, learning_rate_init = 0.01)\n",
    "modelMLP1.fit(x_train, y_train)\n",
    "print(modelMLP1.score(x_test, y_test))\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = modelMLP1.predict(x_test)\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "print(ac)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = modelMLP1.predict(x_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03131c2b",
   "metadata": {},
   "source": [
    "## StraFied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3afe32a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.11566008\n",
      "Iteration 2, loss = 1.24299612\n",
      "Iteration 3, loss = 0.59566321\n",
      "Iteration 4, loss = 0.26043160\n",
      "Iteration 5, loss = 0.24808691\n",
      "Iteration 6, loss = 0.26520963\n",
      "Iteration 7, loss = 0.65342026\n",
      "Iteration 8, loss = 0.49675430\n",
      "Iteration 9, loss = 0.23200852\n",
      "Iteration 10, loss = 0.22403476\n",
      "Iteration 11, loss = 0.20476828\n",
      "Iteration 12, loss = 0.21489565\n",
      "Iteration 13, loss = 1.61895102\n",
      "Iteration 14, loss = 2.15497052\n",
      "Iteration 15, loss = 0.91844080\n",
      "Iteration 16, loss = 0.59995883\n",
      "Iteration 17, loss = 0.28963168\n",
      "Iteration 18, loss = 0.23193428\n",
      "Iteration 19, loss = 0.22796374\n",
      "Iteration 20, loss = 0.20788783\n",
      "Iteration 21, loss = 0.20251791\n",
      "Iteration 22, loss = 0.20295133\n",
      "Iteration 23, loss = 0.19651158\n",
      "Iteration 24, loss = 0.20336586\n",
      "Iteration 25, loss = 0.19443588\n",
      "Iteration 26, loss = 0.20077516\n",
      "Iteration 27, loss = 0.18632862\n",
      "Iteration 28, loss = 0.18792519\n",
      "Iteration 29, loss = 0.18319955\n",
      "Iteration 30, loss = 0.19796157\n",
      "Iteration 31, loss = 0.18783887\n",
      "Iteration 32, loss = 0.19350070\n",
      "Iteration 33, loss = 0.20149695\n",
      "Iteration 34, loss = 0.17341246\n",
      "Iteration 35, loss = 0.17328053\n",
      "Iteration 36, loss = 0.19291592\n",
      "Iteration 37, loss = 0.18273309\n",
      "Iteration 38, loss = 0.19261000\n",
      "Iteration 39, loss = 0.35018648\n",
      "Iteration 40, loss = 0.84124784\n",
      "Iteration 41, loss = 1.37801399\n",
      "Iteration 42, loss = 0.29602128\n",
      "Iteration 43, loss = 0.20800095\n",
      "Iteration 44, loss = 0.19964229\n",
      "Iteration 45, loss = 0.18937183\n",
      "Iteration 46, loss = 0.18712720\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.03762978\n",
      "Iteration 2, loss = 0.41148967\n",
      "Iteration 3, loss = 0.89220548\n",
      "Iteration 4, loss = 0.22279027\n",
      "Iteration 5, loss = 0.20900022\n",
      "Iteration 6, loss = 0.26223748\n",
      "Iteration 7, loss = 0.71481503\n",
      "Iteration 8, loss = 0.70966898\n",
      "Iteration 9, loss = 0.53943804\n",
      "Iteration 10, loss = 0.24566159\n",
      "Iteration 11, loss = 0.21030212\n",
      "Iteration 12, loss = 0.21571429\n",
      "Iteration 13, loss = 0.49796227\n",
      "Iteration 14, loss = 1.53318256\n",
      "Iteration 15, loss = 0.22208349\n",
      "Iteration 16, loss = 0.21238566\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.81401504\n",
      "Iteration 2, loss = 0.99300505\n",
      "Iteration 3, loss = 1.28041386\n",
      "Iteration 4, loss = 0.69624678\n",
      "Iteration 5, loss = 0.36824231\n",
      "Iteration 6, loss = 0.25352420\n",
      "Iteration 7, loss = 0.24251400\n",
      "Iteration 8, loss = 0.22617825\n",
      "Iteration 9, loss = 0.23734321\n",
      "Iteration 10, loss = 0.21715167\n",
      "Iteration 11, loss = 0.18052425\n",
      "Iteration 12, loss = 0.16738862\n",
      "Iteration 13, loss = 0.15241454\n",
      "Iteration 14, loss = 0.15622684\n",
      "Iteration 15, loss = 1.09804900\n",
      "Iteration 16, loss = 0.60070731\n",
      "Iteration 17, loss = 0.24852305\n",
      "Iteration 18, loss = 0.27373778\n",
      "Iteration 19, loss = 0.13751119\n",
      "Iteration 20, loss = 0.19692870\n",
      "Iteration 21, loss = 0.15677858\n",
      "Iteration 22, loss = 0.25272548\n",
      "Iteration 23, loss = 0.14947106\n",
      "Iteration 24, loss = 0.17414986\n",
      "Iteration 25, loss = 0.13411413\n",
      "Iteration 26, loss = 0.13045174\n",
      "Iteration 27, loss = 0.12470583\n",
      "Iteration 28, loss = 0.13493453\n",
      "Iteration 29, loss = 0.12366575\n",
      "Iteration 30, loss = 0.14164851\n",
      "Iteration 31, loss = 0.13052559\n",
      "Iteration 32, loss = 0.12247051\n",
      "Iteration 33, loss = 0.13642225\n",
      "Iteration 34, loss = 0.12473654\n",
      "Iteration 35, loss = 0.15019179\n",
      "Iteration 36, loss = 1.63290943\n",
      "Iteration 37, loss = 1.01026256\n",
      "Iteration 38, loss = 0.21455196\n",
      "Iteration 39, loss = 0.19095444\n",
      "Iteration 40, loss = 0.12255104\n",
      "Iteration 41, loss = 0.11924312\n",
      "Iteration 42, loss = 0.12345728\n",
      "Iteration 43, loss = 0.12031577\n",
      "Iteration 44, loss = 0.13517453\n",
      "Iteration 45, loss = 0.13145344\n",
      "Iteration 46, loss = 0.12222648\n",
      "Iteration 47, loss = 0.12185839\n",
      "Iteration 48, loss = 0.12494677\n",
      "Iteration 49, loss = 0.12417208\n",
      "Iteration 50, loss = 0.13304347\n",
      "Iteration 51, loss = 0.12026184\n",
      "Iteration 52, loss = 0.11674111\n",
      "Iteration 53, loss = 0.12049221\n",
      "Iteration 54, loss = 0.12725927\n",
      "Iteration 55, loss = 0.14313835\n",
      "Iteration 56, loss = 0.12665758\n",
      "Iteration 57, loss = 0.12628432\n",
      "Iteration 58, loss = 0.13483130\n",
      "Iteration 59, loss = 0.12062151\n",
      "Iteration 60, loss = 0.11723490\n",
      "Iteration 61, loss = 0.12186757\n",
      "Iteration 62, loss = 0.12350638\n",
      "Iteration 63, loss = 0.12065512\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.87795462\n",
      "Iteration 2, loss = 1.16478000\n",
      "Iteration 3, loss = 0.96222596\n",
      "Iteration 4, loss = 1.12441874\n",
      "Iteration 5, loss = 0.58990021\n",
      "Iteration 6, loss = 0.34705953\n",
      "Iteration 7, loss = 0.29095365\n",
      "Iteration 8, loss = 0.24042408\n",
      "Iteration 9, loss = 0.26286086\n",
      "Iteration 10, loss = 0.25738366\n",
      "Iteration 11, loss = 0.76109807\n",
      "Iteration 12, loss = 0.69206643\n",
      "Iteration 13, loss = 0.28332663\n",
      "Iteration 14, loss = 0.22665315\n",
      "Iteration 15, loss = 0.21554042\n",
      "Iteration 16, loss = 0.76170353\n",
      "Iteration 17, loss = 0.88907652\n",
      "Iteration 18, loss = 0.28603928\n",
      "Iteration 19, loss = 0.16222238\n",
      "Iteration 20, loss = 0.14281763\n",
      "Iteration 21, loss = 0.13864017\n",
      "Iteration 22, loss = 0.14680107\n",
      "Iteration 23, loss = 0.15292039\n",
      "Iteration 24, loss = 0.15112456\n",
      "Iteration 25, loss = 0.14271194\n",
      "Iteration 26, loss = 0.14037780\n",
      "Iteration 27, loss = 0.13890915\n",
      "Iteration 28, loss = 0.14213861\n",
      "Iteration 29, loss = 0.13844144\n",
      "Iteration 30, loss = 0.15829233\n",
      "Iteration 31, loss = 0.13363594\n",
      "Iteration 32, loss = 0.16219734\n",
      "Iteration 33, loss = 0.15774768\n",
      "Iteration 34, loss = 0.13480337\n",
      "Iteration 35, loss = 0.13705187\n",
      "Iteration 36, loss = 0.15814365\n",
      "Iteration 37, loss = 0.13619535\n",
      "Iteration 38, loss = 0.14531099\n",
      "Iteration 39, loss = 0.14297363\n",
      "Iteration 40, loss = 0.13110593\n",
      "Iteration 41, loss = 0.14091634\n",
      "Iteration 42, loss = 0.13418003\n",
      "Iteration 43, loss = 0.14555989\n",
      "Iteration 44, loss = 0.12916486\n",
      "Iteration 45, loss = 0.24236539\n",
      "Iteration 46, loss = 0.37845886\n",
      "Iteration 47, loss = 0.25775405\n",
      "Iteration 48, loss = 0.25082964\n",
      "Iteration 49, loss = 1.17245974\n",
      "Iteration 50, loss = 0.87490176\n",
      "Iteration 51, loss = 0.16144890\n",
      "Iteration 52, loss = 0.13651300\n",
      "Iteration 53, loss = 0.13117556\n",
      "Iteration 54, loss = 0.14289893\n",
      "Iteration 55, loss = 0.14771513\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.88919788\n",
      "Iteration 2, loss = 0.85078774\n",
      "Iteration 3, loss = 1.07145628\n",
      "Iteration 4, loss = 1.10038065\n",
      "Iteration 5, loss = 0.51167536\n",
      "Iteration 6, loss = 0.23602433\n",
      "Iteration 7, loss = 0.20444244\n",
      "Iteration 8, loss = 0.20622389\n",
      "Iteration 9, loss = 0.23954653\n",
      "Iteration 10, loss = 0.23558078\n",
      "Iteration 11, loss = 0.21207799\n",
      "Iteration 12, loss = 0.24328008\n",
      "Iteration 13, loss = 1.33021343\n",
      "Iteration 14, loss = 1.59960820\n",
      "Iteration 15, loss = 0.55384934\n",
      "Iteration 16, loss = 0.39853183\n",
      "Iteration 17, loss = 0.20139105\n",
      "Iteration 18, loss = 0.16319356\n",
      "Iteration 19, loss = 0.14701376\n",
      "Iteration 20, loss = 0.14861474\n",
      "Iteration 21, loss = 0.14768195\n",
      "Iteration 22, loss = 0.15414714\n",
      "Iteration 23, loss = 0.15681649\n",
      "Iteration 24, loss = 0.15874763\n",
      "Iteration 25, loss = 0.14923813\n",
      "Iteration 26, loss = 0.14981629\n",
      "Iteration 27, loss = 0.14650503\n",
      "Iteration 28, loss = 0.14844281\n",
      "Iteration 29, loss = 0.14539332\n",
      "Iteration 30, loss = 0.16657886\n",
      "Iteration 31, loss = 0.14337129\n",
      "Iteration 32, loss = 0.16653770\n",
      "Iteration 33, loss = 0.16067462\n",
      "Iteration 34, loss = 0.14851244\n",
      "Iteration 35, loss = 0.14570800\n",
      "Iteration 36, loss = 0.20856215\n",
      "Iteration 37, loss = 0.15921830\n",
      "Iteration 38, loss = 0.15051351\n",
      "Iteration 39, loss = 0.15394853\n",
      "Iteration 40, loss = 0.14127631\n",
      "Iteration 41, loss = 0.14780336\n",
      "Iteration 42, loss = 0.14602452\n",
      "Iteration 43, loss = 0.15027917\n",
      "Iteration 44, loss = 0.14645248\n",
      "Iteration 45, loss = 0.20495159\n",
      "Iteration 46, loss = 0.92332293\n",
      "Iteration 47, loss = 0.47684740\n",
      "Iteration 48, loss = 0.29644536\n",
      "Iteration 49, loss = 0.22042067\n",
      "Iteration 50, loss = 0.22103848\n",
      "Iteration 51, loss = 0.14188513\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.97164814\n",
      "Iteration 2, loss = 1.27643173\n",
      "Iteration 3, loss = 0.77064986\n",
      "Iteration 4, loss = 0.37967945\n",
      "Iteration 5, loss = 0.34683230\n",
      "Iteration 6, loss = 0.43995636\n",
      "Iteration 7, loss = 0.44893554\n",
      "Iteration 8, loss = 1.19249256\n",
      "Iteration 9, loss = 0.64757184\n",
      "Iteration 10, loss = 0.27650354\n",
      "Iteration 11, loss = 0.18314690\n",
      "Iteration 12, loss = 0.17871991\n",
      "Iteration 13, loss = 0.17149088\n",
      "Iteration 14, loss = 0.20100007\n",
      "Iteration 15, loss = 0.20062170\n",
      "Iteration 16, loss = 0.24424185\n",
      "Iteration 17, loss = 0.20650643\n",
      "Iteration 18, loss = 0.19751632\n",
      "Iteration 19, loss = 0.20319175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, loss = 0.21312116\n",
      "Iteration 21, loss = 0.30407721\n",
      "Iteration 22, loss = 0.17181981\n",
      "Iteration 23, loss = 0.23757022\n",
      "Iteration 24, loss = 0.16921654\n",
      "Iteration 25, loss = 0.39026090\n",
      "Iteration 26, loss = 1.67558154\n",
      "Iteration 27, loss = 0.51142246\n",
      "Iteration 28, loss = 0.30003756\n",
      "Iteration 29, loss = 0.23834780\n",
      "Iteration 30, loss = 0.19311545\n",
      "Iteration 31, loss = 0.14499250\n",
      "Iteration 32, loss = 0.14803005\n",
      "Iteration 33, loss = 0.15166234\n",
      "Iteration 34, loss = 0.14470310\n",
      "Iteration 35, loss = 0.15419784\n",
      "Iteration 36, loss = 0.16062386\n",
      "Iteration 37, loss = 0.14264914\n",
      "Iteration 38, loss = 0.13927893\n",
      "Iteration 39, loss = 0.13995479\n",
      "Iteration 40, loss = 0.14349921\n",
      "Iteration 41, loss = 0.13745949\n",
      "Iteration 42, loss = 0.13730172\n",
      "Iteration 43, loss = 0.13862807\n",
      "Iteration 44, loss = 0.15492039\n",
      "Iteration 45, loss = 0.16083179\n",
      "Iteration 46, loss = 0.14520481\n",
      "Iteration 47, loss = 0.13593739\n",
      "Iteration 48, loss = 0.14514944\n",
      "Iteration 49, loss = 0.14329682\n",
      "Iteration 50, loss = 0.20653586\n",
      "Iteration 51, loss = 0.23602974\n",
      "Iteration 52, loss = 0.14564837\n",
      "Iteration 53, loss = 0.13977721\n",
      "Iteration 54, loss = 0.14032272\n",
      "Iteration 55, loss = 0.16018801\n",
      "Iteration 56, loss = 0.14655584\n",
      "Iteration 57, loss = 0.13680857\n",
      "Iteration 58, loss = 0.15004130\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.47771588\n",
      "Iteration 2, loss = 0.35785748\n",
      "Iteration 3, loss = 1.20373707\n",
      "Iteration 4, loss = 1.49998183\n",
      "Iteration 5, loss = 1.17638801\n",
      "Iteration 6, loss = 1.09072846\n",
      "Iteration 7, loss = 0.62756761\n",
      "Iteration 8, loss = 0.28943991\n",
      "Iteration 9, loss = 0.27018879\n",
      "Iteration 10, loss = 0.25525755\n",
      "Iteration 11, loss = 0.26027681\n",
      "Iteration 12, loss = 0.27237895\n",
      "Iteration 13, loss = 0.22157759\n",
      "Iteration 14, loss = 0.21723455\n",
      "Iteration 15, loss = 0.22765602\n",
      "Iteration 16, loss = 0.22898854\n",
      "Iteration 17, loss = 0.21707830\n",
      "Iteration 18, loss = 0.18452281\n",
      "Iteration 19, loss = 0.17148546\n",
      "Iteration 20, loss = 0.17666714\n",
      "Iteration 21, loss = 0.27335548\n",
      "Iteration 22, loss = 1.21648123\n",
      "Iteration 23, loss = 1.21315054\n",
      "Iteration 24, loss = 0.66346930\n",
      "Iteration 25, loss = 0.30855644\n",
      "Iteration 26, loss = 0.20966761\n",
      "Iteration 27, loss = 0.15915398\n",
      "Iteration 28, loss = 0.15383588\n",
      "Iteration 29, loss = 0.16430162\n",
      "Iteration 30, loss = 0.17270597\n",
      "Iteration 31, loss = 0.15584343\n",
      "Iteration 32, loss = 0.15595342\n",
      "Iteration 33, loss = 0.14823812\n",
      "Iteration 34, loss = 0.14813207\n",
      "Iteration 35, loss = 0.16006133\n",
      "Iteration 36, loss = 0.17765577\n",
      "Iteration 37, loss = 0.15117308\n",
      "Iteration 38, loss = 0.14896944\n",
      "Iteration 39, loss = 0.14933508\n",
      "Iteration 40, loss = 0.15653872\n",
      "Iteration 41, loss = 0.14481722\n",
      "Iteration 42, loss = 0.14964807\n",
      "Iteration 43, loss = 0.15314707\n",
      "Iteration 44, loss = 0.16534222\n",
      "Iteration 45, loss = 0.16876006\n",
      "Iteration 46, loss = 0.16011280\n",
      "Iteration 47, loss = 0.14415103\n",
      "Iteration 48, loss = 0.15230452\n",
      "Iteration 49, loss = 0.17218162\n",
      "Iteration 50, loss = 0.20835955\n",
      "Iteration 51, loss = 0.15139917\n",
      "Iteration 52, loss = 0.14878360\n",
      "Iteration 53, loss = 0.15480601\n",
      "Iteration 54, loss = 0.15529424\n",
      "Iteration 55, loss = 0.16070763\n",
      "Iteration 56, loss = 0.15197608\n",
      "Iteration 57, loss = 0.14726117\n",
      "Iteration 58, loss = 0.15914114\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.51978772\n",
      "Iteration 2, loss = 1.14878425\n",
      "Iteration 3, loss = 0.43151411\n",
      "Iteration 4, loss = 0.29483071\n",
      "Iteration 5, loss = 0.33928586\n",
      "Iteration 6, loss = 0.83072516\n",
      "Iteration 7, loss = 0.36661076\n",
      "Iteration 8, loss = 0.29853474\n",
      "Iteration 9, loss = 0.24822594\n",
      "Iteration 10, loss = 0.41492325\n",
      "Iteration 11, loss = 1.78379272\n",
      "Iteration 12, loss = 1.34693437\n",
      "Iteration 13, loss = 0.57947102\n",
      "Iteration 14, loss = 0.31350430\n",
      "Iteration 15, loss = 0.24643458\n",
      "Iteration 16, loss = 0.22758208\n",
      "Iteration 17, loss = 0.24473287\n",
      "Iteration 18, loss = 0.21632534\n",
      "Iteration 19, loss = 0.21184125\n",
      "Iteration 20, loss = 0.22172344\n",
      "Iteration 21, loss = 0.20449432\n",
      "Iteration 22, loss = 0.21216585\n",
      "Iteration 23, loss = 0.20162846\n",
      "Iteration 24, loss = 0.17773195\n",
      "Iteration 25, loss = 0.18042404\n",
      "Iteration 26, loss = 0.16763818\n",
      "Iteration 27, loss = 0.22990041\n",
      "Iteration 28, loss = 0.33161881\n",
      "Iteration 29, loss = 0.18198030\n",
      "Iteration 30, loss = 0.18104927\n",
      "Iteration 31, loss = 0.16149376\n",
      "Iteration 32, loss = 0.15790364\n",
      "Iteration 33, loss = 0.17166079\n",
      "Iteration 34, loss = 0.16628571\n",
      "Iteration 35, loss = 0.17376077\n",
      "Iteration 36, loss = 0.19128738\n",
      "Iteration 37, loss = 0.15263435\n",
      "Iteration 38, loss = 0.16134032\n",
      "Iteration 39, loss = 0.16199240\n",
      "Iteration 40, loss = 0.15683049\n",
      "Iteration 41, loss = 0.26643479\n",
      "Iteration 42, loss = 0.97394114\n",
      "Iteration 43, loss = 0.81218562\n",
      "Iteration 44, loss = 0.20562009\n",
      "Iteration 45, loss = 0.16083486\n",
      "Iteration 46, loss = 0.15555449\n",
      "Iteration 47, loss = 0.15044480\n",
      "Iteration 48, loss = 0.15149854\n",
      "Iteration 49, loss = 0.15253323\n",
      "Iteration 50, loss = 0.14916566\n",
      "Iteration 51, loss = 0.15411271\n",
      "Iteration 52, loss = 0.14823849\n",
      "Iteration 53, loss = 0.15567227\n",
      "Iteration 54, loss = 0.16133104\n",
      "Iteration 55, loss = 0.14827631\n",
      "Iteration 56, loss = 0.14873610\n",
      "Iteration 57, loss = 0.15235610\n",
      "Iteration 58, loss = 0.15154208\n",
      "Iteration 59, loss = 0.14983203\n",
      "Iteration 60, loss = 0.15420210\n",
      "Iteration 61, loss = 0.16013581\n",
      "Iteration 62, loss = 0.16370119\n",
      "Iteration 63, loss = 0.15009410\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.58520356\n",
      "Iteration 2, loss = 0.69282103\n",
      "Iteration 3, loss = 0.34908418\n",
      "Iteration 4, loss = 0.26711031\n",
      "Iteration 5, loss = 0.45093593\n",
      "Iteration 6, loss = 0.53670484\n",
      "Iteration 7, loss = 1.08309977\n",
      "Iteration 8, loss = 0.25645601\n",
      "Iteration 9, loss = 0.22690729\n",
      "Iteration 10, loss = 0.25817336\n",
      "Iteration 11, loss = 0.20841617\n",
      "Iteration 12, loss = 0.19825610\n",
      "Iteration 13, loss = 0.18964123\n",
      "Iteration 14, loss = 0.23091004\n",
      "Iteration 15, loss = 0.19846482\n",
      "Iteration 16, loss = 0.48800122\n",
      "Iteration 17, loss = 0.21064513\n",
      "Iteration 18, loss = 0.78039765\n",
      "Iteration 19, loss = 0.39209170\n",
      "Iteration 20, loss = 0.24329094\n",
      "Iteration 21, loss = 0.17877355\n",
      "Iteration 22, loss = 0.24441914\n",
      "Iteration 23, loss = 1.07016614\n",
      "Iteration 24, loss = 0.41990489\n",
      "Iteration 25, loss = 0.15375139\n",
      "Iteration 26, loss = 0.14612194\n",
      "Iteration 27, loss = 0.15859904\n",
      "Iteration 28, loss = 0.15070535\n",
      "Iteration 29, loss = 0.15048850\n",
      "Iteration 30, loss = 0.14862173\n",
      "Iteration 31, loss = 0.14832089\n",
      "Iteration 32, loss = 0.15426400\n",
      "Iteration 33, loss = 0.15986206\n",
      "Iteration 34, loss = 0.15482896\n",
      "Iteration 35, loss = 0.15359349\n",
      "Iteration 36, loss = 0.17582356\n",
      "Iteration 37, loss = 0.14643638\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.94959570\n",
      "Iteration 2, loss = 0.99405438\n",
      "Iteration 3, loss = 0.30693570\n",
      "Iteration 4, loss = 0.71838183\n",
      "Iteration 5, loss = 2.00404006\n",
      "Iteration 6, loss = 0.75998893\n",
      "Iteration 7, loss = 0.30469863\n",
      "Iteration 8, loss = 0.22477354\n",
      "Iteration 9, loss = 0.19885145\n",
      "Iteration 10, loss = 0.17697913\n",
      "Iteration 11, loss = 0.18093333\n",
      "Iteration 12, loss = 0.17258723\n",
      "Iteration 13, loss = 0.16157724\n",
      "Iteration 14, loss = 0.15363671\n",
      "Iteration 15, loss = 0.15269298\n",
      "Iteration 16, loss = 0.14120870\n",
      "Iteration 17, loss = 0.13355693\n",
      "Iteration 18, loss = 0.11733827\n",
      "Iteration 19, loss = 0.11687323\n",
      "Iteration 20, loss = 0.11313836\n",
      "Iteration 21, loss = 0.10996651\n",
      "Iteration 22, loss = 0.10389587\n",
      "Iteration 23, loss = 0.10294207\n",
      "Iteration 24, loss = 0.08465362\n",
      "Iteration 25, loss = 0.08530181\n",
      "Iteration 26, loss = 0.06965665\n",
      "Iteration 27, loss = 0.07402426\n",
      "Iteration 28, loss = 0.07796792\n",
      "Iteration 29, loss = 0.07372804\n",
      "Iteration 30, loss = 0.06856814\n",
      "Iteration 31, loss = 0.06450901\n",
      "Iteration 32, loss = 0.06146261\n",
      "Iteration 33, loss = 0.05960154\n",
      "Iteration 34, loss = 0.05810631\n",
      "Iteration 35, loss = 0.05425148\n",
      "Iteration 36, loss = 0.05461473\n",
      "Iteration 37, loss = 0.05346897\n",
      "Iteration 38, loss = 0.04831520\n",
      "Iteration 39, loss = 0.04407031\n",
      "Iteration 40, loss = 0.04678838\n",
      "Iteration 41, loss = 0.04182475\n",
      "Iteration 42, loss = 0.05383491\n",
      "Iteration 43, loss = 0.03898719\n",
      "Iteration 44, loss = 0.67707394\n",
      "Iteration 45, loss = 1.84483871\n",
      "Iteration 46, loss = 0.24989543\n",
      "Iteration 47, loss = 0.13215706\n",
      "Iteration 48, loss = 0.10611250\n",
      "Iteration 49, loss = 0.09369331\n",
      "Iteration 50, loss = 0.08578773\n",
      "Iteration 51, loss = 0.07978358\n",
      "Iteration 52, loss = 0.07565518\n",
      "Iteration 53, loss = 0.07197897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 54, loss = 0.06830458\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.11566008\n",
      "Iteration 2, loss = 1.24299612\n",
      "Iteration 3, loss = 0.59566321\n",
      "Iteration 4, loss = 0.26043160\n",
      "Iteration 5, loss = 0.24808691\n",
      "Iteration 6, loss = 0.26520963\n",
      "Iteration 7, loss = 0.65342026\n",
      "Iteration 8, loss = 0.49675430\n",
      "Iteration 9, loss = 0.23200852\n",
      "Iteration 10, loss = 0.22403476\n",
      "Iteration 11, loss = 0.20476828\n",
      "Iteration 12, loss = 0.21489565\n",
      "Iteration 13, loss = 1.61895102\n",
      "Iteration 14, loss = 2.15497052\n",
      "Iteration 15, loss = 0.91844080\n",
      "Iteration 16, loss = 0.59995883\n",
      "Iteration 17, loss = 0.28963168\n",
      "Iteration 18, loss = 0.23193428\n",
      "Iteration 19, loss = 0.22796374\n",
      "Iteration 20, loss = 0.20788783\n",
      "Iteration 21, loss = 0.20251791\n",
      "Iteration 22, loss = 0.20295133\n",
      "Iteration 23, loss = 0.19651158\n",
      "Iteration 24, loss = 0.20336586\n",
      "Iteration 25, loss = 0.19443588\n",
      "Iteration 26, loss = 0.20077516\n",
      "Iteration 27, loss = 0.18632862\n",
      "Iteration 28, loss = 0.18792519\n",
      "Iteration 29, loss = 0.18319955\n",
      "Iteration 30, loss = 0.19796157\n",
      "Iteration 31, loss = 0.18783887\n",
      "Iteration 32, loss = 0.19350070\n",
      "Iteration 33, loss = 0.20149695\n",
      "Iteration 34, loss = 0.17341246\n",
      "Iteration 35, loss = 0.17328053\n",
      "Iteration 36, loss = 0.19291592\n",
      "Iteration 37, loss = 0.18273309\n",
      "Iteration 38, loss = 0.19261000\n",
      "Iteration 39, loss = 0.35018648\n",
      "Iteration 40, loss = 0.84124784\n",
      "Iteration 41, loss = 1.37801399\n",
      "Iteration 42, loss = 0.29602128\n",
      "Iteration 43, loss = 0.20800095\n",
      "Iteration 44, loss = 0.19964229\n",
      "Iteration 45, loss = 0.18937183\n",
      "Iteration 46, loss = 0.18712720\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.03762978\n",
      "Iteration 2, loss = 0.41148967\n",
      "Iteration 3, loss = 0.89220548\n",
      "Iteration 4, loss = 0.22279027\n",
      "Iteration 5, loss = 0.20900022\n",
      "Iteration 6, loss = 0.26223748\n",
      "Iteration 7, loss = 0.71481503\n",
      "Iteration 8, loss = 0.70966898\n",
      "Iteration 9, loss = 0.53943804\n",
      "Iteration 10, loss = 0.24566159\n",
      "Iteration 11, loss = 0.21030212\n",
      "Iteration 12, loss = 0.21571429\n",
      "Iteration 13, loss = 0.49796227\n",
      "Iteration 14, loss = 1.53318256\n",
      "Iteration 15, loss = 0.22208349\n",
      "Iteration 16, loss = 0.21238566\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.81401504\n",
      "Iteration 2, loss = 0.99300505\n",
      "Iteration 3, loss = 1.28041386\n",
      "Iteration 4, loss = 0.69624678\n",
      "Iteration 5, loss = 0.36824231\n",
      "Iteration 6, loss = 0.25352420\n",
      "Iteration 7, loss = 0.24251400\n",
      "Iteration 8, loss = 0.22617825\n",
      "Iteration 9, loss = 0.23734321\n",
      "Iteration 10, loss = 0.21715167\n",
      "Iteration 11, loss = 0.18052425\n",
      "Iteration 12, loss = 0.16738862\n",
      "Iteration 13, loss = 0.15241454\n",
      "Iteration 14, loss = 0.15622684\n",
      "Iteration 15, loss = 1.09804900\n",
      "Iteration 16, loss = 0.60070731\n",
      "Iteration 17, loss = 0.24852305\n",
      "Iteration 18, loss = 0.27373778\n",
      "Iteration 19, loss = 0.13751119\n",
      "Iteration 20, loss = 0.19692870\n",
      "Iteration 21, loss = 0.15677858\n",
      "Iteration 22, loss = 0.25272548\n",
      "Iteration 23, loss = 0.14947106\n",
      "Iteration 24, loss = 0.17414986\n",
      "Iteration 25, loss = 0.13411413\n",
      "Iteration 26, loss = 0.13045174\n",
      "Iteration 27, loss = 0.12470583\n",
      "Iteration 28, loss = 0.13493453\n",
      "Iteration 29, loss = 0.12366575\n",
      "Iteration 30, loss = 0.14164851\n",
      "Iteration 31, loss = 0.13052559\n",
      "Iteration 32, loss = 0.12247051\n",
      "Iteration 33, loss = 0.13642225\n",
      "Iteration 34, loss = 0.12473654\n",
      "Iteration 35, loss = 0.15019179\n",
      "Iteration 36, loss = 1.63290943\n",
      "Iteration 37, loss = 1.01026256\n",
      "Iteration 38, loss = 0.21455196\n",
      "Iteration 39, loss = 0.19095444\n",
      "Iteration 40, loss = 0.12255104\n",
      "Iteration 41, loss = 0.11924312\n",
      "Iteration 42, loss = 0.12345728\n",
      "Iteration 43, loss = 0.12031577\n",
      "Iteration 44, loss = 0.13517453\n",
      "Iteration 45, loss = 0.13145344\n",
      "Iteration 46, loss = 0.12222648\n",
      "Iteration 47, loss = 0.12185839\n",
      "Iteration 48, loss = 0.12494677\n",
      "Iteration 49, loss = 0.12417208\n",
      "Iteration 50, loss = 0.13304347\n",
      "Iteration 51, loss = 0.12026184\n",
      "Iteration 52, loss = 0.11674111\n",
      "Iteration 53, loss = 0.12049221\n",
      "Iteration 54, loss = 0.12725927\n",
      "Iteration 55, loss = 0.14313835\n",
      "Iteration 56, loss = 0.12665758\n",
      "Iteration 57, loss = 0.12628432\n",
      "Iteration 58, loss = 0.13483130\n",
      "Iteration 59, loss = 0.12062151\n",
      "Iteration 60, loss = 0.11723490\n",
      "Iteration 61, loss = 0.12186757\n",
      "Iteration 62, loss = 0.12350638\n",
      "Iteration 63, loss = 0.12065512\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.87795462\n",
      "Iteration 2, loss = 1.16478000\n",
      "Iteration 3, loss = 0.96222596\n",
      "Iteration 4, loss = 1.12441874\n",
      "Iteration 5, loss = 0.58990021\n",
      "Iteration 6, loss = 0.34705953\n",
      "Iteration 7, loss = 0.29095365\n",
      "Iteration 8, loss = 0.24042408\n",
      "Iteration 9, loss = 0.26286086\n",
      "Iteration 10, loss = 0.25738366\n",
      "Iteration 11, loss = 0.76109807\n",
      "Iteration 12, loss = 0.69206643\n",
      "Iteration 13, loss = 0.28332663\n",
      "Iteration 14, loss = 0.22665315\n",
      "Iteration 15, loss = 0.21554042\n",
      "Iteration 16, loss = 0.76170353\n",
      "Iteration 17, loss = 0.88907652\n",
      "Iteration 18, loss = 0.28603928\n",
      "Iteration 19, loss = 0.16222238\n",
      "Iteration 20, loss = 0.14281763\n",
      "Iteration 21, loss = 0.13864017\n",
      "Iteration 22, loss = 0.14680107\n",
      "Iteration 23, loss = 0.15292039\n",
      "Iteration 24, loss = 0.15112456\n",
      "Iteration 25, loss = 0.14271194\n",
      "Iteration 26, loss = 0.14037780\n",
      "Iteration 27, loss = 0.13890915\n",
      "Iteration 28, loss = 0.14213861\n",
      "Iteration 29, loss = 0.13844144\n",
      "Iteration 30, loss = 0.15829233\n",
      "Iteration 31, loss = 0.13363594\n",
      "Iteration 32, loss = 0.16219734\n",
      "Iteration 33, loss = 0.15774768\n",
      "Iteration 34, loss = 0.13480337\n",
      "Iteration 35, loss = 0.13705187\n",
      "Iteration 36, loss = 0.15814365\n",
      "Iteration 37, loss = 0.13619535\n",
      "Iteration 38, loss = 0.14531099\n",
      "Iteration 39, loss = 0.14297363\n",
      "Iteration 40, loss = 0.13110593\n",
      "Iteration 41, loss = 0.14091634\n",
      "Iteration 42, loss = 0.13418003\n",
      "Iteration 43, loss = 0.14555989\n",
      "Iteration 44, loss = 0.12916486\n",
      "Iteration 45, loss = 0.24236539\n",
      "Iteration 46, loss = 0.37845886\n",
      "Iteration 47, loss = 0.25775405\n",
      "Iteration 48, loss = 0.25082964\n",
      "Iteration 49, loss = 1.17245974\n",
      "Iteration 50, loss = 0.87490176\n",
      "Iteration 51, loss = 0.16144890\n",
      "Iteration 52, loss = 0.13651300\n",
      "Iteration 53, loss = 0.13117556\n",
      "Iteration 54, loss = 0.14289893\n",
      "Iteration 55, loss = 0.14771513\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.88919788\n",
      "Iteration 2, loss = 0.85078774\n",
      "Iteration 3, loss = 1.07145628\n",
      "Iteration 4, loss = 1.10038065\n",
      "Iteration 5, loss = 0.51167536\n",
      "Iteration 6, loss = 0.23602433\n",
      "Iteration 7, loss = 0.20444244\n",
      "Iteration 8, loss = 0.20622389\n",
      "Iteration 9, loss = 0.23954653\n",
      "Iteration 10, loss = 0.23558078\n",
      "Iteration 11, loss = 0.21207799\n",
      "Iteration 12, loss = 0.24328008\n",
      "Iteration 13, loss = 1.33021343\n",
      "Iteration 14, loss = 1.59960820\n",
      "Iteration 15, loss = 0.55384934\n",
      "Iteration 16, loss = 0.39853183\n",
      "Iteration 17, loss = 0.20139105\n",
      "Iteration 18, loss = 0.16319356\n",
      "Iteration 19, loss = 0.14701376\n",
      "Iteration 20, loss = 0.14861474\n",
      "Iteration 21, loss = 0.14768195\n",
      "Iteration 22, loss = 0.15414714\n",
      "Iteration 23, loss = 0.15681649\n",
      "Iteration 24, loss = 0.15874763\n",
      "Iteration 25, loss = 0.14923813\n",
      "Iteration 26, loss = 0.14981629\n",
      "Iteration 27, loss = 0.14650503\n",
      "Iteration 28, loss = 0.14844281\n",
      "Iteration 29, loss = 0.14539332\n",
      "Iteration 30, loss = 0.16657886\n",
      "Iteration 31, loss = 0.14337129\n",
      "Iteration 32, loss = 0.16653770\n",
      "Iteration 33, loss = 0.16067462\n",
      "Iteration 34, loss = 0.14851244\n",
      "Iteration 35, loss = 0.14570800\n",
      "Iteration 36, loss = 0.20856215\n",
      "Iteration 37, loss = 0.15921830\n",
      "Iteration 38, loss = 0.15051351\n",
      "Iteration 39, loss = 0.15394853\n",
      "Iteration 40, loss = 0.14127631\n",
      "Iteration 41, loss = 0.14780336\n",
      "Iteration 42, loss = 0.14602452\n",
      "Iteration 43, loss = 0.15027917\n",
      "Iteration 44, loss = 0.14645248\n",
      "Iteration 45, loss = 0.20495159\n",
      "Iteration 46, loss = 0.92332293\n",
      "Iteration 47, loss = 0.47684740\n",
      "Iteration 48, loss = 0.29644536\n",
      "Iteration 49, loss = 0.22042067\n",
      "Iteration 50, loss = 0.22103848\n",
      "Iteration 51, loss = 0.14188513\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.97164814\n",
      "Iteration 2, loss = 1.27643173\n",
      "Iteration 3, loss = 0.77064986\n",
      "Iteration 4, loss = 0.37967945\n",
      "Iteration 5, loss = 0.34683230\n",
      "Iteration 6, loss = 0.43995636\n",
      "Iteration 7, loss = 0.44893554\n",
      "Iteration 8, loss = 1.19249256\n",
      "Iteration 9, loss = 0.64757184\n",
      "Iteration 10, loss = 0.27650354\n",
      "Iteration 11, loss = 0.18314690\n",
      "Iteration 12, loss = 0.17871991\n",
      "Iteration 13, loss = 0.17149088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = 0.20100007\n",
      "Iteration 15, loss = 0.20062170\n",
      "Iteration 16, loss = 0.24424185\n",
      "Iteration 17, loss = 0.20650643\n",
      "Iteration 18, loss = 0.19751632\n",
      "Iteration 19, loss = 0.20319175\n",
      "Iteration 20, loss = 0.21312116\n",
      "Iteration 21, loss = 0.30407721\n",
      "Iteration 22, loss = 0.17181981\n",
      "Iteration 23, loss = 0.23757022\n",
      "Iteration 24, loss = 0.16921654\n",
      "Iteration 25, loss = 0.39026090\n",
      "Iteration 26, loss = 1.67558154\n",
      "Iteration 27, loss = 0.51142246\n",
      "Iteration 28, loss = 0.30003756\n",
      "Iteration 29, loss = 0.23834780\n",
      "Iteration 30, loss = 0.19311545\n",
      "Iteration 31, loss = 0.14499250\n",
      "Iteration 32, loss = 0.14803005\n",
      "Iteration 33, loss = 0.15166234\n",
      "Iteration 34, loss = 0.14470310\n",
      "Iteration 35, loss = 0.15419784\n",
      "Iteration 36, loss = 0.16062386\n",
      "Iteration 37, loss = 0.14264914\n",
      "Iteration 38, loss = 0.13927893\n",
      "Iteration 39, loss = 0.13995479\n",
      "Iteration 40, loss = 0.14349921\n",
      "Iteration 41, loss = 0.13745949\n",
      "Iteration 42, loss = 0.13730172\n",
      "Iteration 43, loss = 0.13862807\n",
      "Iteration 44, loss = 0.15492039\n",
      "Iteration 45, loss = 0.16083179\n",
      "Iteration 46, loss = 0.14520481\n",
      "Iteration 47, loss = 0.13593739\n",
      "Iteration 48, loss = 0.14514944\n",
      "Iteration 49, loss = 0.14329682\n",
      "Iteration 50, loss = 0.20653586\n",
      "Iteration 51, loss = 0.23602974\n",
      "Iteration 52, loss = 0.14564837\n",
      "Iteration 53, loss = 0.13977721\n",
      "Iteration 54, loss = 0.14032272\n",
      "Iteration 55, loss = 0.16018801\n",
      "Iteration 56, loss = 0.14655584\n",
      "Iteration 57, loss = 0.13680857\n",
      "Iteration 58, loss = 0.15004130\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.47771588\n",
      "Iteration 2, loss = 0.35785748\n",
      "Iteration 3, loss = 1.20373707\n",
      "Iteration 4, loss = 1.49998183\n",
      "Iteration 5, loss = 1.17638801\n",
      "Iteration 6, loss = 1.09072846\n",
      "Iteration 7, loss = 0.62756761\n",
      "Iteration 8, loss = 0.28943991\n",
      "Iteration 9, loss = 0.27018879\n",
      "Iteration 10, loss = 0.25525755\n",
      "Iteration 11, loss = 0.26027681\n",
      "Iteration 12, loss = 0.27237895\n",
      "Iteration 13, loss = 0.22157759\n",
      "Iteration 14, loss = 0.21723455\n",
      "Iteration 15, loss = 0.22765602\n",
      "Iteration 16, loss = 0.22898854\n",
      "Iteration 17, loss = 0.21707830\n",
      "Iteration 18, loss = 0.18452281\n",
      "Iteration 19, loss = 0.17148546\n",
      "Iteration 20, loss = 0.17666714\n",
      "Iteration 21, loss = 0.27335548\n",
      "Iteration 22, loss = 1.21648123\n",
      "Iteration 23, loss = 1.21315054\n",
      "Iteration 24, loss = 0.66346930\n",
      "Iteration 25, loss = 0.30855644\n",
      "Iteration 26, loss = 0.20966761\n",
      "Iteration 27, loss = 0.15915398\n",
      "Iteration 28, loss = 0.15383588\n",
      "Iteration 29, loss = 0.16430162\n",
      "Iteration 30, loss = 0.17270597\n",
      "Iteration 31, loss = 0.15584343\n",
      "Iteration 32, loss = 0.15595342\n",
      "Iteration 33, loss = 0.14823812\n",
      "Iteration 34, loss = 0.14813207\n",
      "Iteration 35, loss = 0.16006133\n",
      "Iteration 36, loss = 0.17765577\n",
      "Iteration 37, loss = 0.15117308\n",
      "Iteration 38, loss = 0.14896944\n",
      "Iteration 39, loss = 0.14933508\n",
      "Iteration 40, loss = 0.15653872\n",
      "Iteration 41, loss = 0.14481722\n",
      "Iteration 42, loss = 0.14964807\n",
      "Iteration 43, loss = 0.15314707\n",
      "Iteration 44, loss = 0.16534222\n",
      "Iteration 45, loss = 0.16876006\n",
      "Iteration 46, loss = 0.16011280\n",
      "Iteration 47, loss = 0.14415103\n",
      "Iteration 48, loss = 0.15230452\n",
      "Iteration 49, loss = 0.17218162\n",
      "Iteration 50, loss = 0.20835955\n",
      "Iteration 51, loss = 0.15139917\n",
      "Iteration 52, loss = 0.14878360\n",
      "Iteration 53, loss = 0.15480601\n",
      "Iteration 54, loss = 0.15529424\n",
      "Iteration 55, loss = 0.16070763\n",
      "Iteration 56, loss = 0.15197608\n",
      "Iteration 57, loss = 0.14726117\n",
      "Iteration 58, loss = 0.15914114\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.51978772\n",
      "Iteration 2, loss = 1.14878425\n",
      "Iteration 3, loss = 0.43151411\n",
      "Iteration 4, loss = 0.29483071\n",
      "Iteration 5, loss = 0.33928586\n",
      "Iteration 6, loss = 0.83072516\n",
      "Iteration 7, loss = 0.36661076\n",
      "Iteration 8, loss = 0.29853474\n",
      "Iteration 9, loss = 0.24822594\n",
      "Iteration 10, loss = 0.41492325\n",
      "Iteration 11, loss = 1.78379272\n",
      "Iteration 12, loss = 1.34693437\n",
      "Iteration 13, loss = 0.57947102\n",
      "Iteration 14, loss = 0.31350430\n",
      "Iteration 15, loss = 0.24643458\n",
      "Iteration 16, loss = 0.22758208\n",
      "Iteration 17, loss = 0.24473287\n",
      "Iteration 18, loss = 0.21632534\n",
      "Iteration 19, loss = 0.21184125\n",
      "Iteration 20, loss = 0.22172344\n",
      "Iteration 21, loss = 0.20449432\n",
      "Iteration 22, loss = 0.21216585\n",
      "Iteration 23, loss = 0.20162846\n",
      "Iteration 24, loss = 0.17773195\n",
      "Iteration 25, loss = 0.18042404\n",
      "Iteration 26, loss = 0.16763818\n",
      "Iteration 27, loss = 0.22990041\n",
      "Iteration 28, loss = 0.33161881\n",
      "Iteration 29, loss = 0.18198030\n",
      "Iteration 30, loss = 0.18104927\n",
      "Iteration 31, loss = 0.16149376\n",
      "Iteration 32, loss = 0.15790364\n",
      "Iteration 33, loss = 0.17166079\n",
      "Iteration 34, loss = 0.16628571\n",
      "Iteration 35, loss = 0.17376077\n",
      "Iteration 36, loss = 0.19128738\n",
      "Iteration 37, loss = 0.15263435\n",
      "Iteration 38, loss = 0.16134032\n",
      "Iteration 39, loss = 0.16199240\n",
      "Iteration 40, loss = 0.15683049\n",
      "Iteration 41, loss = 0.26643479\n",
      "Iteration 42, loss = 0.97394114\n",
      "Iteration 43, loss = 0.81218562\n",
      "Iteration 44, loss = 0.20562009\n",
      "Iteration 45, loss = 0.16083486\n",
      "Iteration 46, loss = 0.15555449\n",
      "Iteration 47, loss = 0.15044480\n",
      "Iteration 48, loss = 0.15149854\n",
      "Iteration 49, loss = 0.15253323\n",
      "Iteration 50, loss = 0.14916566\n",
      "Iteration 51, loss = 0.15411271\n",
      "Iteration 52, loss = 0.14823849\n",
      "Iteration 53, loss = 0.15567227\n",
      "Iteration 54, loss = 0.16133104\n",
      "Iteration 55, loss = 0.14827631\n",
      "Iteration 56, loss = 0.14873610\n",
      "Iteration 57, loss = 0.15235610\n",
      "Iteration 58, loss = 0.15154208\n",
      "Iteration 59, loss = 0.14983203\n",
      "Iteration 60, loss = 0.15420210\n",
      "Iteration 61, loss = 0.16013581\n",
      "Iteration 62, loss = 0.16370119\n",
      "Iteration 63, loss = 0.15009410\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.58520356\n",
      "Iteration 2, loss = 0.69282103\n",
      "Iteration 3, loss = 0.34908418\n",
      "Iteration 4, loss = 0.26711031\n",
      "Iteration 5, loss = 0.45093593\n",
      "Iteration 6, loss = 0.53670484\n",
      "Iteration 7, loss = 1.08309977\n",
      "Iteration 8, loss = 0.25645601\n",
      "Iteration 9, loss = 0.22690729\n",
      "Iteration 10, loss = 0.25817336\n",
      "Iteration 11, loss = 0.20841617\n",
      "Iteration 12, loss = 0.19825610\n",
      "Iteration 13, loss = 0.18964123\n",
      "Iteration 14, loss = 0.23091004\n",
      "Iteration 15, loss = 0.19846482\n",
      "Iteration 16, loss = 0.48800122\n",
      "Iteration 17, loss = 0.21064513\n",
      "Iteration 18, loss = 0.78039765\n",
      "Iteration 19, loss = 0.39209170\n",
      "Iteration 20, loss = 0.24329094\n",
      "Iteration 21, loss = 0.17877355\n",
      "Iteration 22, loss = 0.24441914\n",
      "Iteration 23, loss = 1.07016614\n",
      "Iteration 24, loss = 0.41990489\n",
      "Iteration 25, loss = 0.15375139\n",
      "Iteration 26, loss = 0.14612194\n",
      "Iteration 27, loss = 0.15859904\n",
      "Iteration 28, loss = 0.15070535\n",
      "Iteration 29, loss = 0.15048850\n",
      "Iteration 30, loss = 0.14862173\n",
      "Iteration 31, loss = 0.14832089\n",
      "Iteration 32, loss = 0.15426400\n",
      "Iteration 33, loss = 0.15986206\n",
      "Iteration 34, loss = 0.15482896\n",
      "Iteration 35, loss = 0.15359349\n",
      "Iteration 36, loss = 0.17582356\n",
      "Iteration 37, loss = 0.14643638\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.94959570\n",
      "Iteration 2, loss = 0.99405438\n",
      "Iteration 3, loss = 0.30693570\n",
      "Iteration 4, loss = 0.71838183\n",
      "Iteration 5, loss = 2.00404006\n",
      "Iteration 6, loss = 0.75998893\n",
      "Iteration 7, loss = 0.30469863\n",
      "Iteration 8, loss = 0.22477354\n",
      "Iteration 9, loss = 0.19885145\n",
      "Iteration 10, loss = 0.17697913\n",
      "Iteration 11, loss = 0.18093333\n",
      "Iteration 12, loss = 0.17258723\n",
      "Iteration 13, loss = 0.16157724\n",
      "Iteration 14, loss = 0.15363671\n",
      "Iteration 15, loss = 0.15269298\n",
      "Iteration 16, loss = 0.14120870\n",
      "Iteration 17, loss = 0.13355693\n",
      "Iteration 18, loss = 0.11733827\n",
      "Iteration 19, loss = 0.11687323\n",
      "Iteration 20, loss = 0.11313836\n",
      "Iteration 21, loss = 0.10996651\n",
      "Iteration 22, loss = 0.10389587\n",
      "Iteration 23, loss = 0.10294207\n",
      "Iteration 24, loss = 0.08465362\n",
      "Iteration 25, loss = 0.08530181\n",
      "Iteration 26, loss = 0.06965665\n",
      "Iteration 27, loss = 0.07402426\n",
      "Iteration 28, loss = 0.07796792\n",
      "Iteration 29, loss = 0.07372804\n",
      "Iteration 30, loss = 0.06856814\n",
      "Iteration 31, loss = 0.06450901\n",
      "Iteration 32, loss = 0.06146261\n",
      "Iteration 33, loss = 0.05960154\n",
      "Iteration 34, loss = 0.05810631\n",
      "Iteration 35, loss = 0.05425148\n",
      "Iteration 36, loss = 0.05461473\n",
      "Iteration 37, loss = 0.05346897\n",
      "Iteration 38, loss = 0.04831520\n",
      "Iteration 39, loss = 0.04407031\n",
      "Iteration 40, loss = 0.04678838\n",
      "Iteration 41, loss = 0.04182475\n",
      "Iteration 42, loss = 0.05383491\n",
      "Iteration 43, loss = 0.03898719\n",
      "Iteration 44, loss = 0.67707394\n",
      "Iteration 45, loss = 1.84483871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46, loss = 0.24989543\n",
      "Iteration 47, loss = 0.13215706\n",
      "Iteration 48, loss = 0.10611250\n",
      "Iteration 49, loss = 0.09369331\n",
      "Iteration 50, loss = 0.08578773\n",
      "Iteration 51, loss = 0.07978358\n",
      "Iteration 52, loss = 0.07565518\n",
      "Iteration 53, loss = 0.07197897\n",
      "Iteration 54, loss = 0.06830458\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy =  [0.84415584 0.89123377 0.8262987  0.93668831 0.93344156 0.88798701\n",
      " 0.98701299 0.97398374 0.95447154 0.89593496]\n",
      "Accuracy mean and std : 0.913 (0.051)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.70      0.64       692\n",
      "           1       0.96      0.94      0.95      5465\n",
      "\n",
      "    accuracy                           0.91      6157\n",
      "   macro avg       0.78      0.82      0.80      6157\n",
      "weighted avg       0.92      0.91      0.92      6157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KFold cross validation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv=StratifiedKFold(n_splits = 10)\n",
    "\n",
    "class_names = ['0','1']\n",
    "y_pred = cross_val_predict(modelMLP1, X, Y, cv = cv)\n",
    "\n",
    "\n",
    "\n",
    "cv_score_for_LR = cross_val_score(modelMLP1, X, Y, cv = 10)\n",
    "print(\"Accuracy = \",cv_score_for_LR)\n",
    "\n",
    "print('Accuracy mean and std : %.3f (%.3f)' % (mean(cv_score_for_LR), std(cv_score_for_LR)))\n",
    "\n",
    "print(classification_report(Y, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b00f79",
   "metadata": {},
   "source": [
    "## Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ae795a",
   "metadata": {},
   "source": [
    "### Smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dcb4b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Counter({1: 5465, 0: 692})\n",
      "After Counter({1: 5465, 0: 5465})\n",
      "0.9364135407136323\n",
      "0.9364135407136323\n",
      "[[1025   65]\n",
      " [  74 1022]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94      1090\n",
      "           1       0.94      0.93      0.94      1096\n",
      "\n",
      "    accuracy                           0.94      2186\n",
      "   macro avg       0.94      0.94      0.94      2186\n",
      "weighted avg       0.94      0.94      0.94      2186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "X = data.drop(['TYPE OF BIRTH    '], axis = 1)\n",
    "'''X = data[['AGE', 'WEIGHT', 'BMI', 'KG INCREASED PREGNANCY', 'PREVIOUS TERM PREGNANCIES','PARITY', 'GESTAGIONAL AGE ', 'COUPLE SITUATION ', 'ART', 'AMNIOCENTESIS', 'PREVIOUS CESAREAN',\n",
    "       'COMORBIDITY', 'PREINDUCTION', 'INDUCTION',\n",
    "       'ANESTHESIA ', 'EPISIOTOMY', 'Fetal INTRAPARTUM pH', 'COMPLICATIONS',\n",
    "        'LIQUID_ Hemorr�gico  ',\n",
    "       'LIQUID_ clear        ', 'LIQUID_ stained +    ',\n",
    "       'LIQUID_ stained ++   ', 'LIQUID_ stained +++  ', 'GROUP_ group 1     ',\n",
    "       'GROUP_ group 10    ', 'GROUP_ group 2a    ', 'GROUP_ group 2b    ',\n",
    "       'GROUP_ group 3     ', 'GROUP_ group 4a    ', 'GROUP_ group 4b    ',\n",
    "       'GROUP_ group 5     ', 'GROUP_ group 6     ', 'GROUP_ group 7     ',\n",
    "       'GROUP_ group 8     ', 'GROUP_ group 9     ']]'''\n",
    "\n",
    "Y = data['TYPE OF BIRTH    ']\n",
    "\n",
    "#SMOTE Balancing\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "counter = Counter(Y)\n",
    "print('Before', counter)\n",
    "smt = SMOTE()\n",
    "X, Y = smt.fit_resample(X, Y)\n",
    "counter = Counter(Y)\n",
    "print('After', counter)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2,random_state = 109) # 80% training and 20% test\n",
    "\n",
    "\n",
    "# Simple Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "modelLR2 = LogisticRegression()\n",
    "modelLR2.fit(x_train, y_train)\n",
    "print(modelLR2.score(x_test, y_test))\n",
    "\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = modelLR2.predict(x_test)\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "print(ac)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = modelLR2.predict(x_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bc9478",
   "metadata": {},
   "source": [
    "## K FOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1b5d51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.92      0.86      5465\n",
      "           1       0.90      0.78      0.84      5465\n",
      "\n",
      "    accuracy                           0.85     10930\n",
      "   macro avg       0.86      0.85      0.85     10930\n",
      "weighted avg       0.86      0.85      0.85     10930\n",
      "\n",
      "Accuracy =  [0.84720952 0.91125343 0.87374199 0.89844465 0.91857274 0.83989021\n",
      " 0.94784995 0.97621226 0.97072278 0.95882891]\n",
      "Accuracy mean and std : 0.914 (0.047)\n"
     ]
    }
   ],
   "source": [
    "# KFold cross validation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=10)\n",
    "class_names = ['0','1']\n",
    "y_pred = cross_val_predict(modelLR2, X, Y, cv = cv)\n",
    "\n",
    "print(classification_report(Y, y_pred, target_names=class_names))\n",
    "\n",
    "cv_score_for_LR = cross_val_score(modelLR2, X, Y, cv = 10)\n",
    "print(\"Accuracy = \",cv_score_for_LR)\n",
    "print('Accuracy mean and std : %.3f (%.3f)' % (mean(cv_score_for_LR), std(cv_score_for_LR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386eacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5458e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8dc15609",
   "metadata": {},
   "source": [
    "## MLP SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7a2df4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.10177112\n",
      "Iteration 2, loss = 1.87695191\n",
      "Iteration 3, loss = 0.62795477\n",
      "Iteration 4, loss = 0.77433721\n",
      "Iteration 5, loss = 0.41672500\n",
      "Iteration 6, loss = 0.43172653\n",
      "Iteration 7, loss = 0.37326511\n",
      "Iteration 8, loss = 0.39393011\n",
      "Iteration 9, loss = 0.40920717\n",
      "Iteration 10, loss = 0.31384286\n",
      "Iteration 11, loss = 0.44298102\n",
      "Iteration 12, loss = 0.37524322\n",
      "Iteration 13, loss = 0.27173535\n",
      "Iteration 14, loss = 0.36457058\n",
      "Iteration 15, loss = 0.53070138\n",
      "Iteration 16, loss = 0.27942546\n",
      "Iteration 17, loss = 0.28849796\n",
      "Iteration 18, loss = 0.28248308\n",
      "Iteration 19, loss = 0.27834200\n",
      "Iteration 20, loss = 0.32894553\n",
      "Iteration 21, loss = 0.50964786\n",
      "Iteration 22, loss = 0.35249861\n",
      "Iteration 23, loss = 0.26293458\n",
      "Iteration 24, loss = 0.25341408\n",
      "Iteration 25, loss = 0.22109721\n",
      "Iteration 26, loss = 0.23943656\n",
      "Iteration 27, loss = 0.24613540\n",
      "Iteration 28, loss = 0.20476065\n",
      "Iteration 29, loss = 0.20549022\n",
      "Iteration 30, loss = 0.19454225\n",
      "Iteration 31, loss = 0.38825856\n",
      "Iteration 32, loss = 0.49529733\n",
      "Iteration 33, loss = 0.29784572\n",
      "Iteration 34, loss = 0.20766257\n",
      "Iteration 35, loss = 0.22200838\n",
      "Iteration 36, loss = 0.18997647\n",
      "Iteration 37, loss = 0.21785306\n",
      "Iteration 38, loss = 0.18909946\n",
      "Iteration 39, loss = 0.18088525\n",
      "Iteration 40, loss = 0.19952813\n",
      "Iteration 41, loss = 0.22447191\n",
      "Iteration 42, loss = 0.19094869\n",
      "Iteration 43, loss = 0.19265373\n",
      "Iteration 44, loss = 0.17889434\n",
      "Iteration 45, loss = 0.17938182\n",
      "Iteration 46, loss = 0.19168884\n",
      "Iteration 47, loss = 0.21939826\n",
      "Iteration 48, loss = 0.22178794\n",
      "Iteration 49, loss = 0.16328313\n",
      "Iteration 50, loss = 0.22147150\n",
      "Iteration 51, loss = 0.21472327\n",
      "Iteration 52, loss = 0.20633707\n",
      "Iteration 53, loss = 0.22315270\n",
      "Iteration 54, loss = 0.39155290\n",
      "Iteration 55, loss = 0.32505258\n",
      "Iteration 56, loss = 0.18160479\n",
      "Iteration 57, loss = 0.16175683\n",
      "Iteration 58, loss = 0.17311879\n",
      "Iteration 59, loss = 0.17421844\n",
      "Iteration 60, loss = 0.18108726\n",
      "Iteration 61, loss = 0.16227235\n",
      "Iteration 62, loss = 0.21937250\n",
      "Iteration 63, loss = 0.17481337\n",
      "Iteration 64, loss = 0.15974523\n",
      "Iteration 65, loss = 0.16964452\n",
      "Iteration 66, loss = 0.15767068\n",
      "Iteration 67, loss = 0.16010324\n",
      "Iteration 68, loss = 0.15797394\n",
      "Iteration 69, loss = 0.19244276\n",
      "Iteration 70, loss = 0.24781352\n",
      "Iteration 71, loss = 0.15720303\n",
      "Iteration 72, loss = 0.16777991\n",
      "Iteration 73, loss = 0.36582124\n",
      "Iteration 74, loss = 0.23288503\n",
      "Iteration 75, loss = 0.15508508\n",
      "Iteration 76, loss = 0.17111820\n",
      "Iteration 77, loss = 0.15964328\n",
      "Iteration 78, loss = 0.15815155\n",
      "Iteration 79, loss = 0.14961204\n",
      "Iteration 80, loss = 0.18748398\n",
      "Iteration 81, loss = 0.15814925\n",
      "Iteration 82, loss = 0.17397107\n",
      "Iteration 83, loss = 0.17006944\n",
      "Iteration 84, loss = 0.27535253\n",
      "Iteration 85, loss = 0.18411422\n",
      "Iteration 86, loss = 0.45119645\n",
      "Iteration 87, loss = 0.17547979\n",
      "Iteration 88, loss = 0.14468062\n",
      "Iteration 89, loss = 0.15365757\n",
      "Iteration 90, loss = 0.16454826\n",
      "Iteration 91, loss = 0.17870793\n",
      "Iteration 92, loss = 0.18949215\n",
      "Iteration 93, loss = 0.15499392\n",
      "Iteration 94, loss = 0.13564437\n",
      "Iteration 95, loss = 0.17289406\n",
      "Iteration 96, loss = 0.15795270\n",
      "Iteration 97, loss = 0.14581965\n",
      "Iteration 98, loss = 0.33848866\n",
      "Iteration 99, loss = 0.32952088\n",
      "Iteration 100, loss = 0.15296639\n",
      "Iteration 101, loss = 0.15543732\n",
      "Iteration 102, loss = 0.16439560\n",
      "Iteration 103, loss = 0.17622636\n",
      "Iteration 104, loss = 0.17352220\n",
      "Iteration 105, loss = 0.26347478\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.9405306495882891\n",
      "0.9405306495882891\n",
      "[[ 983  107]\n",
      " [  23 1073]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.90      0.94      1090\n",
      "           1       0.91      0.98      0.94      1096\n",
      "\n",
      "    accuracy                           0.94      2186\n",
      "   macro avg       0.94      0.94      0.94      2186\n",
      "weighted avg       0.94      0.94      0.94      2186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "modelMLP2 = MLPClassifier(hidden_layer_sizes = (6,5), random_state = 42, verbose = True, learning_rate_init = 0.01)\n",
    "modelMLP2.fit(x_train, y_train)\n",
    "print(modelMLP2.score(x_test, y_test))\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = modelMLP2.predict(x_test)\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "print(ac)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = modelMLP2.predict(x_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e34f41",
   "metadata": {},
   "source": [
    "### K FLOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65d9b665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.32460802\n",
      "Iteration 2, loss = 0.63956780\n",
      "Iteration 3, loss = 0.46992244\n",
      "Iteration 4, loss = 0.41912789\n",
      "Iteration 5, loss = 0.39146912\n",
      "Iteration 6, loss = 0.39292632\n",
      "Iteration 7, loss = 0.49121418\n",
      "Iteration 8, loss = 0.37809514\n",
      "Iteration 9, loss = 0.41740097\n",
      "Iteration 10, loss = 0.94799564\n",
      "Iteration 11, loss = 0.49030227\n",
      "Iteration 12, loss = 0.39032333\n",
      "Iteration 13, loss = 0.31936994\n",
      "Iteration 14, loss = 0.30876525\n",
      "Iteration 15, loss = 0.28310798\n",
      "Iteration 16, loss = 0.31154040\n",
      "Iteration 17, loss = 0.33173225\n",
      "Iteration 18, loss = 0.27642748\n",
      "Iteration 19, loss = 0.26301600\n",
      "Iteration 20, loss = 0.25764431\n",
      "Iteration 21, loss = 0.26914102\n",
      "Iteration 22, loss = 0.28958745\n",
      "Iteration 23, loss = 0.23013264\n",
      "Iteration 24, loss = 0.22905874\n",
      "Iteration 25, loss = 0.23546110\n",
      "Iteration 26, loss = 0.45012461\n",
      "Iteration 27, loss = 0.63535291\n",
      "Iteration 28, loss = 0.28703884\n",
      "Iteration 29, loss = 0.24555183\n",
      "Iteration 30, loss = 0.22322882\n",
      "Iteration 31, loss = 0.23752582\n",
      "Iteration 32, loss = 0.23365786\n",
      "Iteration 33, loss = 0.27695854\n",
      "Iteration 34, loss = 0.19311183\n",
      "Iteration 35, loss = 0.25129166\n",
      "Iteration 36, loss = 0.21246717\n",
      "Iteration 37, loss = 0.21779810\n",
      "Iteration 38, loss = 0.21127127\n",
      "Iteration 39, loss = 0.19506764\n",
      "Iteration 40, loss = 0.19316590\n",
      "Iteration 41, loss = 0.19598704\n",
      "Iteration 42, loss = 0.19916111\n",
      "Iteration 43, loss = 0.20858814\n",
      "Iteration 44, loss = 0.20539674\n",
      "Iteration 45, loss = 0.18167744\n",
      "Iteration 46, loss = 0.16994821\n",
      "Iteration 47, loss = 0.22149887\n",
      "Iteration 48, loss = 0.35514374\n",
      "Iteration 49, loss = 0.30356640\n",
      "Iteration 50, loss = 0.36663689\n",
      "Iteration 51, loss = 0.22452019\n",
      "Iteration 52, loss = 0.20029172\n",
      "Iteration 53, loss = 0.18034785\n",
      "Iteration 54, loss = 0.17939934\n",
      "Iteration 55, loss = 0.18303000\n",
      "Iteration 56, loss = 0.18480010\n",
      "Iteration 57, loss = 0.26341875\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.44290515\n",
      "Iteration 2, loss = 0.64871364\n",
      "Iteration 3, loss = 0.65377465\n",
      "Iteration 4, loss = 1.25211161\n",
      "Iteration 5, loss = 0.70105029\n",
      "Iteration 6, loss = 0.33175527\n",
      "Iteration 7, loss = 0.36038916\n",
      "Iteration 8, loss = 0.30369460\n",
      "Iteration 9, loss = 0.27006821\n",
      "Iteration 10, loss = 0.40498708\n",
      "Iteration 11, loss = 0.24925828\n",
      "Iteration 12, loss = 0.26089663\n",
      "Iteration 13, loss = 0.28996757\n",
      "Iteration 14, loss = 0.31983070\n",
      "Iteration 15, loss = 0.24178219\n",
      "Iteration 16, loss = 0.27857171\n",
      "Iteration 17, loss = 0.24012902\n",
      "Iteration 18, loss = 0.19578151\n",
      "Iteration 19, loss = 0.28681943\n",
      "Iteration 20, loss = 0.23223963\n",
      "Iteration 21, loss = 0.47722590\n",
      "Iteration 22, loss = 0.17834588\n",
      "Iteration 23, loss = 0.18609843\n",
      "Iteration 24, loss = 0.20630746\n",
      "Iteration 25, loss = 0.33898592\n",
      "Iteration 26, loss = 0.20288859\n",
      "Iteration 27, loss = 0.19750621\n",
      "Iteration 28, loss = 0.18019483\n",
      "Iteration 29, loss = 0.19950792\n",
      "Iteration 30, loss = 0.21324039\n",
      "Iteration 31, loss = 0.17314928\n",
      "Iteration 32, loss = 0.16236850\n",
      "Iteration 33, loss = 0.18461184\n",
      "Iteration 34, loss = 0.14554829\n",
      "Iteration 35, loss = 0.16950447\n",
      "Iteration 36, loss = 0.16528401\n",
      "Iteration 37, loss = 0.20956241\n",
      "Iteration 38, loss = 0.15864323\n",
      "Iteration 39, loss = 0.15333993\n",
      "Iteration 40, loss = 0.18165422\n",
      "Iteration 41, loss = 0.16019287\n",
      "Iteration 42, loss = 0.14065496\n",
      "Iteration 43, loss = 0.15926255\n",
      "Iteration 44, loss = 0.13218568\n",
      "Iteration 45, loss = 0.13060938\n",
      "Iteration 46, loss = 0.12045195\n",
      "Iteration 47, loss = 0.13566534\n",
      "Iteration 48, loss = 0.13588793\n",
      "Iteration 49, loss = 0.16295939\n",
      "Iteration 50, loss = 0.30289983\n",
      "Iteration 51, loss = 0.39271478\n",
      "Iteration 52, loss = 0.15010334\n",
      "Iteration 53, loss = 0.12742632\n",
      "Iteration 54, loss = 0.13061665\n",
      "Iteration 55, loss = 0.12577822\n",
      "Iteration 56, loss = 0.13868343\n",
      "Iteration 57, loss = 0.32250595\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.22818368\n",
      "Iteration 2, loss = 0.75812878\n",
      "Iteration 3, loss = 0.59723750\n",
      "Iteration 4, loss = 0.42512846\n",
      "Iteration 5, loss = 0.39556239\n",
      "Iteration 6, loss = 0.53371536\n",
      "Iteration 7, loss = 0.49132066\n",
      "Iteration 8, loss = 0.38872765\n",
      "Iteration 9, loss = 0.47394066\n",
      "Iteration 10, loss = 0.65418074\n",
      "Iteration 11, loss = 0.30398544\n",
      "Iteration 12, loss = 0.25689126\n",
      "Iteration 13, loss = 0.26388778\n",
      "Iteration 14, loss = 0.32911057\n",
      "Iteration 15, loss = 0.28845682\n",
      "Iteration 16, loss = 0.32382813\n",
      "Iteration 17, loss = 0.19680338\n",
      "Iteration 18, loss = 0.24042651\n",
      "Iteration 19, loss = 0.22199654\n",
      "Iteration 20, loss = 0.20907557\n",
      "Iteration 21, loss = 0.23840190\n",
      "Iteration 22, loss = 0.25551690\n",
      "Iteration 23, loss = 0.19487985\n",
      "Iteration 24, loss = 0.19344632\n",
      "Iteration 25, loss = 0.21712156\n",
      "Iteration 26, loss = 0.36300483\n",
      "Iteration 27, loss = 0.18518401\n",
      "Iteration 28, loss = 0.23387740\n",
      "Iteration 29, loss = 0.22920232\n",
      "Iteration 30, loss = 0.19101424\n",
      "Iteration 31, loss = 0.28065104\n",
      "Iteration 32, loss = 0.20745370\n",
      "Iteration 33, loss = 0.17829560\n",
      "Iteration 34, loss = 0.15125220\n",
      "Iteration 35, loss = 0.19071066\n",
      "Iteration 36, loss = 0.24031751\n",
      "Iteration 37, loss = 0.16183737\n",
      "Iteration 38, loss = 0.22509781\n",
      "Iteration 39, loss = 0.16266995\n",
      "Iteration 40, loss = 0.16732243\n",
      "Iteration 41, loss = 0.20051809\n",
      "Iteration 42, loss = 0.16056609\n",
      "Iteration 43, loss = 0.25532488\n",
      "Iteration 44, loss = 0.16477087\n",
      "Iteration 45, loss = 0.24314517\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.16741612\n",
      "Iteration 2, loss = 0.54866020\n",
      "Iteration 3, loss = 0.50848261\n",
      "Iteration 4, loss = 0.51176890\n",
      "Iteration 5, loss = 0.48192500\n",
      "Iteration 6, loss = 0.52070939\n",
      "Iteration 7, loss = 0.78694852\n",
      "Iteration 8, loss = 0.33267160\n",
      "Iteration 9, loss = 0.28465729\n",
      "Iteration 10, loss = 0.53991978\n",
      "Iteration 11, loss = 0.26530412\n",
      "Iteration 12, loss = 0.21454851\n",
      "Iteration 13, loss = 0.23253400\n",
      "Iteration 14, loss = 0.27977403\n",
      "Iteration 15, loss = 0.29565439\n",
      "Iteration 16, loss = 0.26513355\n",
      "Iteration 17, loss = 0.31512648\n",
      "Iteration 18, loss = 0.26274173\n",
      "Iteration 19, loss = 0.24859378\n",
      "Iteration 20, loss = 0.19405536\n",
      "Iteration 21, loss = 0.26982523\n",
      "Iteration 22, loss = 0.27454782\n",
      "Iteration 23, loss = 0.20729652\n",
      "Iteration 24, loss = 0.18074623\n",
      "Iteration 25, loss = 0.18084191\n",
      "Iteration 26, loss = 0.21047960\n",
      "Iteration 27, loss = 0.27147729\n",
      "Iteration 28, loss = 0.25262694\n",
      "Iteration 29, loss = 0.19803655\n",
      "Iteration 30, loss = 0.19700633\n",
      "Iteration 31, loss = 0.19632611\n",
      "Iteration 32, loss = 0.17758285\n",
      "Iteration 33, loss = 0.28662252\n",
      "Iteration 34, loss = 0.16786118\n",
      "Iteration 35, loss = 0.27231366\n",
      "Iteration 36, loss = 0.15140509\n",
      "Iteration 37, loss = 0.17239232\n",
      "Iteration 38, loss = 0.33746099\n",
      "Iteration 39, loss = 0.18616132\n",
      "Iteration 40, loss = 0.16335766\n",
      "Iteration 41, loss = 0.18136245\n",
      "Iteration 42, loss = 0.15252766\n",
      "Iteration 43, loss = 0.21534070\n",
      "Iteration 44, loss = 0.17737149\n",
      "Iteration 45, loss = 0.24634708\n",
      "Iteration 46, loss = 0.15139084\n",
      "Iteration 47, loss = 0.20822467\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.32845595\n",
      "Iteration 2, loss = 1.10078651\n",
      "Iteration 3, loss = 0.90437616\n",
      "Iteration 4, loss = 0.66573790\n",
      "Iteration 5, loss = 0.43725343\n",
      "Iteration 6, loss = 0.43139715\n",
      "Iteration 7, loss = 0.43950479\n",
      "Iteration 8, loss = 0.37354725\n",
      "Iteration 9, loss = 0.33907558\n",
      "Iteration 10, loss = 0.41068908\n",
      "Iteration 11, loss = 0.31403054\n",
      "Iteration 12, loss = 0.27859662\n",
      "Iteration 13, loss = 0.28523695\n",
      "Iteration 14, loss = 0.36663573\n",
      "Iteration 15, loss = 0.27772332\n",
      "Iteration 16, loss = 0.41894642\n",
      "Iteration 17, loss = 0.35718378\n",
      "Iteration 18, loss = 0.24314491\n",
      "Iteration 19, loss = 0.28880118\n",
      "Iteration 20, loss = 0.53961345\n",
      "Iteration 21, loss = 0.23376771\n",
      "Iteration 22, loss = 0.29877391\n",
      "Iteration 23, loss = 0.21712868\n",
      "Iteration 24, loss = 0.21558277\n",
      "Iteration 25, loss = 0.23692633\n",
      "Iteration 26, loss = 0.24159439\n",
      "Iteration 27, loss = 0.27846716\n",
      "Iteration 28, loss = 0.23020804\n",
      "Iteration 29, loss = 0.32193021\n",
      "Iteration 30, loss = 0.21964882\n",
      "Iteration 31, loss = 0.21537863\n",
      "Iteration 32, loss = 0.20592871\n",
      "Iteration 33, loss = 0.28671717\n",
      "Iteration 34, loss = 0.20806068\n",
      "Iteration 35, loss = 0.32416207\n",
      "Iteration 36, loss = 0.21413810\n",
      "Iteration 37, loss = 0.18217372\n",
      "Iteration 38, loss = 0.21692265\n",
      "Iteration 39, loss = 0.20269806\n",
      "Iteration 40, loss = 0.19284588\n",
      "Iteration 41, loss = 0.20154042\n",
      "Iteration 42, loss = 0.23236248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43, loss = 0.19372081\n",
      "Iteration 44, loss = 0.25412265\n",
      "Iteration 45, loss = 0.34980442\n",
      "Iteration 46, loss = 0.22148947\n",
      "Iteration 47, loss = 0.24045957\n",
      "Iteration 48, loss = 0.32758313\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.98427266\n",
      "Iteration 2, loss = 0.57155299\n",
      "Iteration 3, loss = 0.50269253\n",
      "Iteration 4, loss = 0.42741131\n",
      "Iteration 5, loss = 0.39064228\n",
      "Iteration 6, loss = 0.39042657\n",
      "Iteration 7, loss = 0.42730034\n",
      "Iteration 8, loss = 0.64099237\n",
      "Iteration 9, loss = 0.33710548\n",
      "Iteration 10, loss = 0.72637132\n",
      "Iteration 11, loss = 0.34130994\n",
      "Iteration 12, loss = 0.25730857\n",
      "Iteration 13, loss = 0.23402041\n",
      "Iteration 14, loss = 0.28855310\n",
      "Iteration 15, loss = 0.22746364\n",
      "Iteration 16, loss = 0.23953646\n",
      "Iteration 17, loss = 0.21038469\n",
      "Iteration 18, loss = 0.43360434\n",
      "Iteration 19, loss = 0.23291649\n",
      "Iteration 20, loss = 0.27051617\n",
      "Iteration 21, loss = 0.23027128\n",
      "Iteration 22, loss = 0.23651257\n",
      "Iteration 23, loss = 0.20068752\n",
      "Iteration 24, loss = 0.20305642\n",
      "Iteration 25, loss = 0.21569538\n",
      "Iteration 26, loss = 0.24210576\n",
      "Iteration 27, loss = 0.62207072\n",
      "Iteration 28, loss = 0.27312959\n",
      "Iteration 29, loss = 0.19803927\n",
      "Iteration 30, loss = 0.22184535\n",
      "Iteration 31, loss = 0.21777440\n",
      "Iteration 32, loss = 0.22787523\n",
      "Iteration 33, loss = 0.20551954\n",
      "Iteration 34, loss = 0.19797999\n",
      "Iteration 35, loss = 0.24840684\n",
      "Iteration 36, loss = 0.16699490\n",
      "Iteration 37, loss = 0.16106899\n",
      "Iteration 38, loss = 0.36728763\n",
      "Iteration 39, loss = 0.22612264\n",
      "Iteration 40, loss = 0.24387917\n",
      "Iteration 41, loss = 0.19698449\n",
      "Iteration 42, loss = 0.16430689\n",
      "Iteration 43, loss = 0.21174336\n",
      "Iteration 44, loss = 0.17224308\n",
      "Iteration 45, loss = 0.28086391\n",
      "Iteration 46, loss = 0.15550759\n",
      "Iteration 47, loss = 0.18842694\n",
      "Iteration 48, loss = 0.22396835\n",
      "Iteration 49, loss = 0.20935236\n",
      "Iteration 50, loss = 0.31089760\n",
      "Iteration 51, loss = 0.16004662\n",
      "Iteration 52, loss = 0.16455766\n",
      "Iteration 53, loss = 0.14641075\n",
      "Iteration 54, loss = 0.35088066\n",
      "Iteration 55, loss = 0.14582509\n",
      "Iteration 56, loss = 0.18497835\n",
      "Iteration 57, loss = 0.13661780\n",
      "Iteration 58, loss = 0.22970042\n",
      "Iteration 59, loss = 0.15679652\n",
      "Iteration 60, loss = 0.24855521\n",
      "Iteration 61, loss = 0.34844781\n",
      "Iteration 62, loss = 0.18418983\n",
      "Iteration 63, loss = 0.24499412\n",
      "Iteration 64, loss = 0.14577799\n",
      "Iteration 65, loss = 0.14530823\n",
      "Iteration 66, loss = 0.16573729\n",
      "Iteration 67, loss = 0.13912761\n",
      "Iteration 68, loss = 0.14050390\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.98327671\n",
      "Iteration 2, loss = 0.72143252\n",
      "Iteration 3, loss = 0.46514653\n",
      "Iteration 4, loss = 0.44938950\n",
      "Iteration 5, loss = 0.49034908\n",
      "Iteration 6, loss = 0.52801953\n",
      "Iteration 7, loss = 0.41890406\n",
      "Iteration 8, loss = 0.36884200\n",
      "Iteration 9, loss = 0.43890846\n",
      "Iteration 10, loss = 0.43276848\n",
      "Iteration 11, loss = 0.42679736\n",
      "Iteration 12, loss = 0.29532427\n",
      "Iteration 13, loss = 0.27136695\n",
      "Iteration 14, loss = 0.23945083\n",
      "Iteration 15, loss = 0.24496457\n",
      "Iteration 16, loss = 0.28720787\n",
      "Iteration 17, loss = 0.28893158\n",
      "Iteration 18, loss = 0.33981481\n",
      "Iteration 19, loss = 0.32869753\n",
      "Iteration 20, loss = 0.25215629\n",
      "Iteration 21, loss = 0.27677702\n",
      "Iteration 22, loss = 0.21829072\n",
      "Iteration 23, loss = 0.21386925\n",
      "Iteration 24, loss = 0.35187279\n",
      "Iteration 25, loss = 0.39026421\n",
      "Iteration 26, loss = 0.22406848\n",
      "Iteration 27, loss = 0.18746275\n",
      "Iteration 28, loss = 0.45299492\n",
      "Iteration 29, loss = 0.20953777\n",
      "Iteration 30, loss = 0.24492805\n",
      "Iteration 31, loss = 0.18258934\n",
      "Iteration 32, loss = 0.31656323\n",
      "Iteration 33, loss = 0.18651607\n",
      "Iteration 34, loss = 0.19093629\n",
      "Iteration 35, loss = 0.25321245\n",
      "Iteration 36, loss = 0.31569292\n",
      "Iteration 37, loss = 0.18443179\n",
      "Iteration 38, loss = 0.16692162\n",
      "Iteration 39, loss = 0.20695616\n",
      "Iteration 40, loss = 0.31258440\n",
      "Iteration 41, loss = 0.18648797\n",
      "Iteration 42, loss = 0.19450647\n",
      "Iteration 43, loss = 0.26792186\n",
      "Iteration 44, loss = 0.16377818\n",
      "Iteration 45, loss = 0.20554484\n",
      "Iteration 46, loss = 0.18749872\n",
      "Iteration 47, loss = 0.20089988\n",
      "Iteration 48, loss = 0.23087640\n",
      "Iteration 49, loss = 0.20357156\n",
      "Iteration 50, loss = 0.36383524\n",
      "Iteration 51, loss = 0.20922709\n",
      "Iteration 52, loss = 0.22945193\n",
      "Iteration 53, loss = 0.18353434\n",
      "Iteration 54, loss = 0.22544865\n",
      "Iteration 55, loss = 0.38189265\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.27280641\n",
      "Iteration 2, loss = 0.69997157\n",
      "Iteration 3, loss = 0.48963167\n",
      "Iteration 4, loss = 1.13721101\n",
      "Iteration 5, loss = 0.76193211\n",
      "Iteration 6, loss = 0.37129848\n",
      "Iteration 7, loss = 0.36344205\n",
      "Iteration 8, loss = 0.41630563\n",
      "Iteration 9, loss = 0.29970574\n",
      "Iteration 10, loss = 0.76815277\n",
      "Iteration 11, loss = 0.39581472\n",
      "Iteration 12, loss = 0.32655290\n",
      "Iteration 13, loss = 0.37545046\n",
      "Iteration 14, loss = 0.31923886\n",
      "Iteration 15, loss = 0.29210405\n",
      "Iteration 16, loss = 0.43051235\n",
      "Iteration 17, loss = 0.23735158\n",
      "Iteration 18, loss = 0.22188007\n",
      "Iteration 19, loss = 0.28157145\n",
      "Iteration 20, loss = 0.23633844\n",
      "Iteration 21, loss = 0.28254659\n",
      "Iteration 22, loss = 0.22405023\n",
      "Iteration 23, loss = 0.23303187\n",
      "Iteration 24, loss = 0.29981790\n",
      "Iteration 25, loss = 0.24352873\n",
      "Iteration 26, loss = 0.30336685\n",
      "Iteration 27, loss = 0.25628888\n",
      "Iteration 28, loss = 0.35068337\n",
      "Iteration 29, loss = 0.22501884\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.89877172\n",
      "Iteration 2, loss = 0.56331748\n",
      "Iteration 3, loss = 0.50113368\n",
      "Iteration 4, loss = 0.43551937\n",
      "Iteration 5, loss = 0.55843518\n",
      "Iteration 6, loss = 0.80903905\n",
      "Iteration 7, loss = 0.42187741\n",
      "Iteration 8, loss = 0.54061658\n",
      "Iteration 9, loss = 0.33602588\n",
      "Iteration 10, loss = 0.29106336\n",
      "Iteration 11, loss = 0.26616791\n",
      "Iteration 12, loss = 0.29166322\n",
      "Iteration 13, loss = 0.25954817\n",
      "Iteration 14, loss = 0.41937187\n",
      "Iteration 15, loss = 0.35136219\n",
      "Iteration 16, loss = 0.47602619\n",
      "Iteration 17, loss = 0.40097191\n",
      "Iteration 18, loss = 0.23044330\n",
      "Iteration 19, loss = 0.29796144\n",
      "Iteration 20, loss = 0.22573665\n",
      "Iteration 21, loss = 0.33849877\n",
      "Iteration 22, loss = 0.22749170\n",
      "Iteration 23, loss = 0.24586979\n",
      "Iteration 24, loss = 0.25068240\n",
      "Iteration 25, loss = 0.19920221\n",
      "Iteration 26, loss = 0.20350467\n",
      "Iteration 27, loss = 0.19659420\n",
      "Iteration 28, loss = 0.37699240\n",
      "Iteration 29, loss = 0.22878121\n",
      "Iteration 30, loss = 0.41206613\n",
      "Iteration 31, loss = 0.19776221\n",
      "Iteration 32, loss = 0.27949678\n",
      "Iteration 33, loss = 0.20452608\n",
      "Iteration 34, loss = 0.36538592\n",
      "Iteration 35, loss = 0.26098697\n",
      "Iteration 36, loss = 0.20319415\n",
      "Iteration 37, loss = 0.18710997\n",
      "Iteration 38, loss = 0.17506466\n",
      "Iteration 39, loss = 0.30804279\n",
      "Iteration 40, loss = 0.20423095\n",
      "Iteration 41, loss = 0.18639463\n",
      "Iteration 42, loss = 0.20939161\n",
      "Iteration 43, loss = 0.21789349\n",
      "Iteration 44, loss = 0.18000806\n",
      "Iteration 45, loss = 0.22998158\n",
      "Iteration 46, loss = 0.39260270\n",
      "Iteration 47, loss = 0.25284568\n",
      "Iteration 48, loss = 0.17350616\n",
      "Iteration 49, loss = 0.21955211\n",
      "Iteration 50, loss = 0.18773105\n",
      "Iteration 51, loss = 0.16075087\n",
      "Iteration 52, loss = 0.20831878\n",
      "Iteration 53, loss = 0.18168568\n",
      "Iteration 54, loss = 0.38067419\n",
      "Iteration 55, loss = 0.17234648\n",
      "Iteration 56, loss = 0.17581328\n",
      "Iteration 57, loss = 0.18453137\n",
      "Iteration 58, loss = 0.28853444\n",
      "Iteration 59, loss = 0.17966072\n",
      "Iteration 60, loss = 0.24172903\n",
      "Iteration 61, loss = 0.24781602\n",
      "Iteration 62, loss = 0.17929867\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.17537085\n",
      "Iteration 2, loss = 0.51485888\n",
      "Iteration 3, loss = 0.50740647\n",
      "Iteration 4, loss = 0.65730314\n",
      "Iteration 5, loss = 0.42632155\n",
      "Iteration 6, loss = 0.68907481\n",
      "Iteration 7, loss = 0.37521165\n",
      "Iteration 8, loss = 0.34536585\n",
      "Iteration 9, loss = 0.39472991\n",
      "Iteration 10, loss = 0.34120701\n",
      "Iteration 11, loss = 0.48894358\n",
      "Iteration 12, loss = 0.28969212\n",
      "Iteration 13, loss = 0.60135857\n",
      "Iteration 14, loss = 0.35293621\n",
      "Iteration 15, loss = 0.25787997\n",
      "Iteration 16, loss = 0.27348577\n",
      "Iteration 17, loss = 0.31550526\n",
      "Iteration 18, loss = 0.35206548\n",
      "Iteration 19, loss = 0.24339431\n",
      "Iteration 20, loss = 0.21896196\n",
      "Iteration 21, loss = 0.23769197\n",
      "Iteration 22, loss = 0.21763939\n",
      "Iteration 23, loss = 0.26486931\n",
      "Iteration 24, loss = 0.23452430\n",
      "Iteration 25, loss = 0.19322064\n",
      "Iteration 26, loss = 0.20386847\n",
      "Iteration 27, loss = 0.22123221\n",
      "Iteration 28, loss = 0.28775480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29, loss = 0.21923907\n",
      "Iteration 30, loss = 0.28825805\n",
      "Iteration 31, loss = 0.22382915\n",
      "Iteration 32, loss = 0.40985720\n",
      "Iteration 33, loss = 0.20858949\n",
      "Iteration 34, loss = 0.27868021\n",
      "Iteration 35, loss = 0.19680808\n",
      "Iteration 36, loss = 0.18669127\n",
      "Iteration 37, loss = 0.22741297\n",
      "Iteration 38, loss = 0.18089764\n",
      "Iteration 39, loss = 0.19728649\n",
      "Iteration 40, loss = 0.21173787\n",
      "Iteration 41, loss = 0.19776101\n",
      "Iteration 42, loss = 0.25638789\n",
      "Iteration 43, loss = 0.18900424\n",
      "Iteration 44, loss = 0.16922267\n",
      "Iteration 45, loss = 0.26491347\n",
      "Iteration 46, loss = 0.28420753\n",
      "Iteration 47, loss = 0.26827213\n",
      "Iteration 48, loss = 0.18070551\n",
      "Iteration 49, loss = 0.26212353\n",
      "Iteration 50, loss = 0.30658406\n",
      "Iteration 51, loss = 0.16878762\n",
      "Iteration 52, loss = 0.18934289\n",
      "Iteration 53, loss = 0.23158139\n",
      "Iteration 54, loss = 0.32287095\n",
      "Iteration 55, loss = 0.17743800\n",
      "Iteration 56, loss = 0.16943826\n",
      "Iteration 57, loss = 0.34132330\n",
      "Iteration 58, loss = 0.19953504\n",
      "Iteration 59, loss = 0.16974338\n",
      "Iteration 60, loss = 0.24727841\n",
      "Iteration 61, loss = 0.24210514\n",
      "Iteration 62, loss = 0.17535299\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.51875700\n",
      "Iteration 2, loss = 0.52984367\n",
      "Iteration 3, loss = 0.52859851\n",
      "Iteration 4, loss = 0.48140550\n",
      "Iteration 5, loss = 0.59663284\n",
      "Iteration 6, loss = 0.53446627\n",
      "Iteration 7, loss = 0.51490950\n",
      "Iteration 8, loss = 0.46139868\n",
      "Iteration 9, loss = 0.31776422\n",
      "Iteration 10, loss = 0.65214494\n",
      "Iteration 11, loss = 0.42181191\n",
      "Iteration 12, loss = 0.28497013\n",
      "Iteration 13, loss = 0.27063203\n",
      "Iteration 14, loss = 0.34748993\n",
      "Iteration 15, loss = 0.23602871\n",
      "Iteration 16, loss = 0.32052582\n",
      "Iteration 17, loss = 0.23538213\n",
      "Iteration 18, loss = 0.22466463\n",
      "Iteration 19, loss = 0.27693443\n",
      "Iteration 20, loss = 0.20758176\n",
      "Iteration 21, loss = 0.29817988\n",
      "Iteration 22, loss = 0.23659559\n",
      "Iteration 23, loss = 0.18806341\n",
      "Iteration 24, loss = 0.18074045\n",
      "Iteration 25, loss = 0.23305494\n",
      "Iteration 26, loss = 0.23706688\n",
      "Iteration 27, loss = 0.30312006\n",
      "Iteration 28, loss = 0.26818577\n",
      "Iteration 29, loss = 0.18556990\n",
      "Iteration 30, loss = 0.23017600\n",
      "Iteration 31, loss = 0.18012650\n",
      "Iteration 32, loss = 0.19821328\n",
      "Iteration 33, loss = 0.28927820\n",
      "Iteration 34, loss = 0.17010954\n",
      "Iteration 35, loss = 0.24086313\n",
      "Iteration 36, loss = 0.20469775\n",
      "Iteration 37, loss = 0.26071928\n",
      "Iteration 38, loss = 0.17365804\n",
      "Iteration 39, loss = 0.16617912\n",
      "Iteration 40, loss = 0.16920905\n",
      "Iteration 41, loss = 0.15019400\n",
      "Iteration 42, loss = 0.16891430\n",
      "Iteration 43, loss = 0.16348800\n",
      "Iteration 44, loss = 0.16513122\n",
      "Iteration 45, loss = 0.20356070\n",
      "Iteration 46, loss = 0.13614275\n",
      "Iteration 47, loss = 0.15416679\n",
      "Iteration 48, loss = 0.31686845\n",
      "Iteration 49, loss = 0.31113956\n",
      "Iteration 50, loss = 0.19297757\n",
      "Iteration 51, loss = 0.18916753\n",
      "Iteration 52, loss = 0.17540652\n",
      "Iteration 53, loss = 0.28283415\n",
      "Iteration 54, loss = 0.18075783\n",
      "Iteration 55, loss = 0.13686142\n",
      "Iteration 56, loss = 0.14868647\n",
      "Iteration 57, loss = 0.29082342\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.82758264\n",
      "Iteration 2, loss = 1.04431482\n",
      "Iteration 3, loss = 0.43671302\n",
      "Iteration 4, loss = 0.44417746\n",
      "Iteration 5, loss = 0.48116388\n",
      "Iteration 6, loss = 0.41300207\n",
      "Iteration 7, loss = 0.30518779\n",
      "Iteration 8, loss = 0.41919260\n",
      "Iteration 9, loss = 0.40737245\n",
      "Iteration 10, loss = 0.73878594\n",
      "Iteration 11, loss = 0.36731489\n",
      "Iteration 12, loss = 0.25185563\n",
      "Iteration 13, loss = 0.24453282\n",
      "Iteration 14, loss = 0.27780752\n",
      "Iteration 15, loss = 0.24612184\n",
      "Iteration 16, loss = 0.22002274\n",
      "Iteration 17, loss = 0.30150801\n",
      "Iteration 18, loss = 0.20863973\n",
      "Iteration 19, loss = 0.31579184\n",
      "Iteration 20, loss = 0.21823507\n",
      "Iteration 21, loss = 0.25588170\n",
      "Iteration 22, loss = 0.39299754\n",
      "Iteration 23, loss = 0.26753552\n",
      "Iteration 24, loss = 0.25221041\n",
      "Iteration 25, loss = 0.62448351\n",
      "Iteration 26, loss = 0.25232899\n",
      "Iteration 27, loss = 0.21728150\n",
      "Iteration 28, loss = 0.17308717\n",
      "Iteration 29, loss = 0.17561665\n",
      "Iteration 30, loss = 0.40268716\n",
      "Iteration 31, loss = 0.24898854\n",
      "Iteration 32, loss = 0.29121144\n",
      "Iteration 33, loss = 0.19197042\n",
      "Iteration 34, loss = 0.17661743\n",
      "Iteration 35, loss = 0.18373946\n",
      "Iteration 36, loss = 0.15995377\n",
      "Iteration 37, loss = 0.16775713\n",
      "Iteration 38, loss = 0.19137898\n",
      "Iteration 39, loss = 0.16188650\n",
      "Iteration 40, loss = 0.19758720\n",
      "Iteration 41, loss = 0.17128323\n",
      "Iteration 42, loss = 0.18741340\n",
      "Iteration 43, loss = 0.26088225\n",
      "Iteration 44, loss = 0.18299343\n",
      "Iteration 45, loss = 0.24110214\n",
      "Iteration 46, loss = 0.15355175\n",
      "Iteration 47, loss = 0.16118932\n",
      "Iteration 48, loss = 0.17959629\n",
      "Iteration 49, loss = 0.16189333\n",
      "Iteration 50, loss = 0.42124420\n",
      "Iteration 51, loss = 0.61490832\n",
      "Iteration 52, loss = 0.22622023\n",
      "Iteration 53, loss = 0.17631995\n",
      "Iteration 54, loss = 0.17752601\n",
      "Iteration 55, loss = 0.14605438\n",
      "Iteration 56, loss = 0.14523188\n",
      "Iteration 57, loss = 0.14839119\n",
      "Iteration 58, loss = 0.15477362\n",
      "Iteration 59, loss = 0.13819305\n",
      "Iteration 60, loss = 0.22554734\n",
      "Iteration 61, loss = 0.34330257\n",
      "Iteration 62, loss = 0.15642400\n",
      "Iteration 63, loss = 0.13380804\n",
      "Iteration 64, loss = 0.14848203\n",
      "Iteration 65, loss = 0.13314863\n",
      "Iteration 66, loss = 0.12643805\n",
      "Iteration 67, loss = 0.15542714\n",
      "Iteration 68, loss = 0.16895439\n",
      "Iteration 69, loss = 0.12908317\n",
      "Iteration 70, loss = 0.17106154\n",
      "Iteration 71, loss = 0.14984714\n",
      "Iteration 72, loss = 0.14375422\n",
      "Iteration 73, loss = 0.15620203\n",
      "Iteration 74, loss = 0.12698327\n",
      "Iteration 75, loss = 0.15280421\n",
      "Iteration 76, loss = 0.12043616\n",
      "Iteration 77, loss = 0.13326382\n",
      "Iteration 78, loss = 0.12077442\n",
      "Iteration 79, loss = 0.44831495\n",
      "Iteration 80, loss = 0.14751221\n",
      "Iteration 81, loss = 0.12815593\n",
      "Iteration 82, loss = 0.14526550\n",
      "Iteration 83, loss = 0.19080054\n",
      "Iteration 84, loss = 0.12615028\n",
      "Iteration 85, loss = 0.12633731\n",
      "Iteration 86, loss = 0.15709641\n",
      "Iteration 87, loss = 0.12677898\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.19657869\n",
      "Iteration 2, loss = 1.31269397\n",
      "Iteration 3, loss = 0.67520650\n",
      "Iteration 4, loss = 0.66664182\n",
      "Iteration 5, loss = 0.38622338\n",
      "Iteration 6, loss = 0.63522423\n",
      "Iteration 7, loss = 0.27495125\n",
      "Iteration 8, loss = 0.54565006\n",
      "Iteration 9, loss = 0.50117323\n",
      "Iteration 10, loss = 0.26323552\n",
      "Iteration 11, loss = 0.52658399\n",
      "Iteration 12, loss = 0.30172196\n",
      "Iteration 13, loss = 0.25613039\n",
      "Iteration 14, loss = 0.36741484\n",
      "Iteration 15, loss = 0.22952420\n",
      "Iteration 16, loss = 0.20701848\n",
      "Iteration 17, loss = 0.70971957\n",
      "Iteration 18, loss = 0.30699400\n",
      "Iteration 19, loss = 0.23882685\n",
      "Iteration 20, loss = 0.33224773\n",
      "Iteration 21, loss = 0.25096503\n",
      "Iteration 22, loss = 0.22892985\n",
      "Iteration 23, loss = 0.19154450\n",
      "Iteration 24, loss = 0.34887008\n",
      "Iteration 25, loss = 0.21192749\n",
      "Iteration 26, loss = 0.17264769\n",
      "Iteration 27, loss = 0.30062596\n",
      "Iteration 28, loss = 0.21458860\n",
      "Iteration 29, loss = 0.19425950\n",
      "Iteration 30, loss = 0.16688671\n",
      "Iteration 31, loss = 0.18027794\n",
      "Iteration 32, loss = 0.69131664\n",
      "Iteration 33, loss = 0.21123694\n",
      "Iteration 34, loss = 0.22900936\n",
      "Iteration 35, loss = 0.17281240\n",
      "Iteration 36, loss = 0.16370666\n",
      "Iteration 37, loss = 0.20693886\n",
      "Iteration 38, loss = 0.17249717\n",
      "Iteration 39, loss = 0.17407406\n",
      "Iteration 40, loss = 0.18335351\n",
      "Iteration 41, loss = 0.16940212\n",
      "Iteration 42, loss = 0.23832531\n",
      "Iteration 43, loss = 0.67565089\n",
      "Iteration 44, loss = 0.21800515\n",
      "Iteration 45, loss = 0.20060767\n",
      "Iteration 46, loss = 0.14810664\n",
      "Iteration 47, loss = 0.14851885\n",
      "Iteration 48, loss = 0.14651616\n",
      "Iteration 49, loss = 0.17271613\n",
      "Iteration 50, loss = 0.23280378\n",
      "Iteration 51, loss = 0.19249066\n",
      "Iteration 52, loss = 0.16570916\n",
      "Iteration 53, loss = 0.15954404\n",
      "Iteration 54, loss = 0.14787970\n",
      "Iteration 55, loss = 0.22584069\n",
      "Iteration 56, loss = 0.14895906\n",
      "Iteration 57, loss = 0.17242550\n",
      "Iteration 58, loss = 0.19980351\n",
      "Iteration 59, loss = 0.15266898\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.65905874\n",
      "Iteration 2, loss = 0.59485501\n",
      "Iteration 3, loss = 0.76934699\n",
      "Iteration 4, loss = 0.52501464\n",
      "Iteration 5, loss = 0.50306192\n",
      "Iteration 6, loss = 0.37145381\n",
      "Iteration 7, loss = 0.48888742\n",
      "Iteration 8, loss = 0.39638288\n",
      "Iteration 9, loss = 0.29279424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.42085600\n",
      "Iteration 11, loss = 0.47828013\n",
      "Iteration 12, loss = 0.27761957\n",
      "Iteration 13, loss = 0.32580146\n",
      "Iteration 14, loss = 0.28748538\n",
      "Iteration 15, loss = 0.23781757\n",
      "Iteration 16, loss = 0.31216412\n",
      "Iteration 17, loss = 0.26088921\n",
      "Iteration 18, loss = 0.41671033\n",
      "Iteration 19, loss = 0.23485341\n",
      "Iteration 20, loss = 0.29110024\n",
      "Iteration 21, loss = 0.26026892\n",
      "Iteration 22, loss = 0.24038549\n",
      "Iteration 23, loss = 0.26218058\n",
      "Iteration 24, loss = 0.52895892\n",
      "Iteration 25, loss = 0.23014512\n",
      "Iteration 26, loss = 0.19436348\n",
      "Iteration 27, loss = 0.19508666\n",
      "Iteration 28, loss = 0.20488366\n",
      "Iteration 29, loss = 0.20454893\n",
      "Iteration 30, loss = 0.27943969\n",
      "Iteration 31, loss = 0.21828284\n",
      "Iteration 32, loss = 0.44151520\n",
      "Iteration 33, loss = 0.23334467\n",
      "Iteration 34, loss = 0.23127727\n",
      "Iteration 35, loss = 0.18747286\n",
      "Iteration 36, loss = 0.19302069\n",
      "Iteration 37, loss = 0.18198838\n",
      "Iteration 38, loss = 0.17129365\n",
      "Iteration 39, loss = 0.23152757\n",
      "Iteration 40, loss = 0.17946071\n",
      "Iteration 41, loss = 0.15241134\n",
      "Iteration 42, loss = 0.25482183\n",
      "Iteration 43, loss = 0.42080099\n",
      "Iteration 44, loss = 0.17076436\n",
      "Iteration 45, loss = 0.18968735\n",
      "Iteration 46, loss = 0.17351355\n",
      "Iteration 47, loss = 0.20385718\n",
      "Iteration 48, loss = 0.17068653\n",
      "Iteration 49, loss = 0.17174947\n",
      "Iteration 50, loss = 0.17512401\n",
      "Iteration 51, loss = 0.21280548\n",
      "Iteration 52, loss = 0.15845306\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.13906645\n",
      "Iteration 2, loss = 0.63646557\n",
      "Iteration 3, loss = 0.51473373\n",
      "Iteration 4, loss = 0.61322504\n",
      "Iteration 5, loss = 0.50230476\n",
      "Iteration 6, loss = 0.36204885\n",
      "Iteration 7, loss = 0.47845326\n",
      "Iteration 8, loss = 0.31309154\n",
      "Iteration 9, loss = 0.37403248\n",
      "Iteration 10, loss = 0.36748600\n",
      "Iteration 11, loss = 0.32393291\n",
      "Iteration 12, loss = 0.23870856\n",
      "Iteration 13, loss = 0.23592419\n",
      "Iteration 14, loss = 0.40380507\n",
      "Iteration 15, loss = 0.43086390\n",
      "Iteration 16, loss = 0.33006364\n",
      "Iteration 17, loss = 0.35398726\n",
      "Iteration 18, loss = 0.23147407\n",
      "Iteration 19, loss = 0.24374635\n",
      "Iteration 20, loss = 0.41733719\n",
      "Iteration 21, loss = 0.21406381\n",
      "Iteration 22, loss = 0.20834639\n",
      "Iteration 23, loss = 0.22882559\n",
      "Iteration 24, loss = 0.25647242\n",
      "Iteration 25, loss = 0.23592238\n",
      "Iteration 26, loss = 0.21642808\n",
      "Iteration 27, loss = 0.23933379\n",
      "Iteration 28, loss = 0.20075788\n",
      "Iteration 29, loss = 0.24631606\n",
      "Iteration 30, loss = 0.27131262\n",
      "Iteration 31, loss = 0.22977601\n",
      "Iteration 32, loss = 0.22697257\n",
      "Iteration 33, loss = 0.21862279\n",
      "Iteration 34, loss = 0.51680608\n",
      "Iteration 35, loss = 0.22448441\n",
      "Iteration 36, loss = 0.17325125\n",
      "Iteration 37, loss = 0.16826867\n",
      "Iteration 38, loss = 0.19671736\n",
      "Iteration 39, loss = 0.20637409\n",
      "Iteration 40, loss = 0.18463245\n",
      "Iteration 41, loss = 0.16925585\n",
      "Iteration 42, loss = 0.30555895\n",
      "Iteration 43, loss = 0.28344924\n",
      "Iteration 44, loss = 0.17989771\n",
      "Iteration 45, loss = 0.23451210\n",
      "Iteration 46, loss = 0.16242137\n",
      "Iteration 47, loss = 0.15462799\n",
      "Iteration 48, loss = 0.32881939\n",
      "Iteration 49, loss = 0.18155403\n",
      "Iteration 50, loss = 0.35248090\n",
      "Iteration 51, loss = 0.21335362\n",
      "Iteration 52, loss = 0.17753408\n",
      "Iteration 53, loss = 0.20027590\n",
      "Iteration 54, loss = 0.23377024\n",
      "Iteration 55, loss = 0.31441890\n",
      "Iteration 56, loss = 0.18394514\n",
      "Iteration 57, loss = 0.19256961\n",
      "Iteration 58, loss = 0.23486259\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.36559201\n",
      "Iteration 2, loss = 0.87387402\n",
      "Iteration 3, loss = 0.56459607\n",
      "Iteration 4, loss = 0.56267480\n",
      "Iteration 5, loss = 0.49392898\n",
      "Iteration 6, loss = 0.36821623\n",
      "Iteration 7, loss = 0.33024245\n",
      "Iteration 8, loss = 0.45935267\n",
      "Iteration 9, loss = 0.45562719\n",
      "Iteration 10, loss = 0.30856256\n",
      "Iteration 11, loss = 0.27557328\n",
      "Iteration 12, loss = 0.28787692\n",
      "Iteration 13, loss = 0.31176863\n",
      "Iteration 14, loss = 0.26893283\n",
      "Iteration 15, loss = 0.32525205\n",
      "Iteration 16, loss = 0.48098718\n",
      "Iteration 17, loss = 0.24894007\n",
      "Iteration 18, loss = 0.22142087\n",
      "Iteration 19, loss = 0.44012112\n",
      "Iteration 20, loss = 0.36631857\n",
      "Iteration 21, loss = 0.20131554\n",
      "Iteration 22, loss = 0.21216029\n",
      "Iteration 23, loss = 0.29530277\n",
      "Iteration 24, loss = 0.26487820\n",
      "Iteration 25, loss = 0.19645876\n",
      "Iteration 26, loss = 0.21027420\n",
      "Iteration 27, loss = 0.18556095\n",
      "Iteration 28, loss = 0.18470041\n",
      "Iteration 29, loss = 0.20568954\n",
      "Iteration 30, loss = 0.27483960\n",
      "Iteration 31, loss = 0.24304740\n",
      "Iteration 32, loss = 0.17430820\n",
      "Iteration 33, loss = 0.24720794\n",
      "Iteration 34, loss = 0.18490075\n",
      "Iteration 35, loss = 0.24422393\n",
      "Iteration 36, loss = 0.17002881\n",
      "Iteration 37, loss = 0.19635322\n",
      "Iteration 38, loss = 0.17931893\n",
      "Iteration 39, loss = 0.16875819\n",
      "Iteration 40, loss = 0.17514844\n",
      "Iteration 41, loss = 0.15451720\n",
      "Iteration 42, loss = 0.27970497\n",
      "Iteration 43, loss = 0.33801967\n",
      "Iteration 44, loss = 0.19361635\n",
      "Iteration 45, loss = 0.28006085\n",
      "Iteration 46, loss = 0.17872257\n",
      "Iteration 47, loss = 0.18403367\n",
      "Iteration 48, loss = 0.18690650\n",
      "Iteration 49, loss = 0.19718709\n",
      "Iteration 50, loss = 0.20864058\n",
      "Iteration 51, loss = 0.18593314\n",
      "Iteration 52, loss = 0.17382223\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39842634\n",
      "Iteration 2, loss = 1.41568372\n",
      "Iteration 3, loss = 0.53192710\n",
      "Iteration 4, loss = 0.54614606\n",
      "Iteration 5, loss = 0.43351281\n",
      "Iteration 6, loss = 0.41534927\n",
      "Iteration 7, loss = 0.43013087\n",
      "Iteration 8, loss = 0.54070095\n",
      "Iteration 9, loss = 0.53998441\n",
      "Iteration 10, loss = 0.30354166\n",
      "Iteration 11, loss = 0.27891592\n",
      "Iteration 12, loss = 0.31428053\n",
      "Iteration 13, loss = 0.39521298\n",
      "Iteration 14, loss = 0.33904679\n",
      "Iteration 15, loss = 0.24419266\n",
      "Iteration 16, loss = 0.38526657\n",
      "Iteration 17, loss = 0.35418910\n",
      "Iteration 18, loss = 0.25028694\n",
      "Iteration 19, loss = 0.41449595\n",
      "Iteration 20, loss = 0.23345423\n",
      "Iteration 21, loss = 0.31600791\n",
      "Iteration 22, loss = 0.41603182\n",
      "Iteration 23, loss = 0.27596204\n",
      "Iteration 24, loss = 0.20693586\n",
      "Iteration 25, loss = 0.20480225\n",
      "Iteration 26, loss = 0.23084586\n",
      "Iteration 27, loss = 0.20714581\n",
      "Iteration 28, loss = 0.25338907\n",
      "Iteration 29, loss = 0.25122778\n",
      "Iteration 30, loss = 0.30937163\n",
      "Iteration 31, loss = 0.23179752\n",
      "Iteration 32, loss = 0.23076621\n",
      "Iteration 33, loss = 0.24231316\n",
      "Iteration 34, loss = 0.18781675\n",
      "Iteration 35, loss = 0.25087526\n",
      "Iteration 36, loss = 0.34999974\n",
      "Iteration 37, loss = 0.21208811\n",
      "Iteration 38, loss = 0.18740476\n",
      "Iteration 39, loss = 0.18695880\n",
      "Iteration 40, loss = 0.21139075\n",
      "Iteration 41, loss = 0.24563020\n",
      "Iteration 42, loss = 0.22685007\n",
      "Iteration 43, loss = 0.35765818\n",
      "Iteration 44, loss = 0.19695443\n",
      "Iteration 45, loss = 0.20068844\n",
      "Iteration 46, loss = 0.32068672\n",
      "Iteration 47, loss = 0.18121911\n",
      "Iteration 48, loss = 0.19872007\n",
      "Iteration 49, loss = 0.19565093\n",
      "Iteration 50, loss = 0.21039828\n",
      "Iteration 51, loss = 0.23659422\n",
      "Iteration 52, loss = 0.17657219\n",
      "Iteration 53, loss = 0.23120695\n",
      "Iteration 54, loss = 0.35678315\n",
      "Iteration 55, loss = 0.21664574\n",
      "Iteration 56, loss = 0.17093339\n",
      "Iteration 57, loss = 0.18210775\n",
      "Iteration 58, loss = 0.17994385\n",
      "Iteration 59, loss = 0.17637051\n",
      "Iteration 60, loss = 0.26733387\n",
      "Iteration 61, loss = 0.19999124\n",
      "Iteration 62, loss = 0.21872768\n",
      "Iteration 63, loss = 0.27884928\n",
      "Iteration 64, loss = 0.18078744\n",
      "Iteration 65, loss = 0.17878659\n",
      "Iteration 66, loss = 0.34959164\n",
      "Iteration 67, loss = 0.17000861\n",
      "Iteration 68, loss = 0.15651461\n",
      "Iteration 69, loss = 0.14530016\n",
      "Iteration 70, loss = 0.19514498\n",
      "Iteration 71, loss = 0.14871155\n",
      "Iteration 72, loss = 0.15813765\n",
      "Iteration 73, loss = 0.21876808\n",
      "Iteration 74, loss = 0.14936083\n",
      "Iteration 75, loss = 0.22892910\n",
      "Iteration 76, loss = 0.24355231\n",
      "Iteration 77, loss = 0.16617997\n",
      "Iteration 78, loss = 0.14427386\n",
      "Iteration 79, loss = 0.17522918\n",
      "Iteration 80, loss = 0.29310839\n",
      "Iteration 81, loss = 0.15822154\n",
      "Iteration 82, loss = 0.15725274\n",
      "Iteration 83, loss = 0.14637969\n",
      "Iteration 84, loss = 0.22322295\n",
      "Iteration 85, loss = 0.15773513\n",
      "Iteration 86, loss = 0.16556868\n",
      "Iteration 87, loss = 0.15651501\n",
      "Iteration 88, loss = 0.15800974\n",
      "Iteration 89, loss = 0.23129155\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.32301338\n",
      "Iteration 2, loss = 1.08018982\n",
      "Iteration 3, loss = 0.47541673\n",
      "Iteration 4, loss = 0.44205589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.48504239\n",
      "Iteration 6, loss = 0.51996966\n",
      "Iteration 7, loss = 0.41655735\n",
      "Iteration 8, loss = 0.42520198\n",
      "Iteration 9, loss = 0.49081949\n",
      "Iteration 10, loss = 0.31974445\n",
      "Iteration 11, loss = 0.32605897\n",
      "Iteration 12, loss = 0.37918219\n",
      "Iteration 13, loss = 0.32158051\n",
      "Iteration 14, loss = 0.29572645\n",
      "Iteration 15, loss = 0.33237206\n",
      "Iteration 16, loss = 0.24851696\n",
      "Iteration 17, loss = 0.26204674\n",
      "Iteration 18, loss = 0.28971351\n",
      "Iteration 19, loss = 0.44936007\n",
      "Iteration 20, loss = 0.30777873\n",
      "Iteration 21, loss = 0.36674855\n",
      "Iteration 22, loss = 0.34252521\n",
      "Iteration 23, loss = 0.24223872\n",
      "Iteration 24, loss = 0.25250032\n",
      "Iteration 25, loss = 0.23433815\n",
      "Iteration 26, loss = 0.23776317\n",
      "Iteration 27, loss = 0.26799422\n",
      "Iteration 28, loss = 0.25439182\n",
      "Iteration 29, loss = 0.26410893\n",
      "Iteration 30, loss = 0.29355033\n",
      "Iteration 31, loss = 0.27438074\n",
      "Iteration 32, loss = 0.21093679\n",
      "Iteration 33, loss = 0.27389989\n",
      "Iteration 34, loss = 0.19251896\n",
      "Iteration 35, loss = 0.24946868\n",
      "Iteration 36, loss = 0.19565029\n",
      "Iteration 37, loss = 0.26035531\n",
      "Iteration 38, loss = 0.20416810\n",
      "Iteration 39, loss = 0.19440473\n",
      "Iteration 40, loss = 0.25469439\n",
      "Iteration 41, loss = 0.26013070\n",
      "Iteration 42, loss = 0.23839183\n",
      "Iteration 43, loss = 0.22053784\n",
      "Iteration 44, loss = 0.17226050\n",
      "Iteration 45, loss = 0.25671906\n",
      "Iteration 46, loss = 0.37354482\n",
      "Iteration 47, loss = 0.31478083\n",
      "Iteration 48, loss = 0.19881806\n",
      "Iteration 49, loss = 0.21055150\n",
      "Iteration 50, loss = 0.32554868\n",
      "Iteration 51, loss = 0.24157294\n",
      "Iteration 52, loss = 0.19288631\n",
      "Iteration 53, loss = 0.21854070\n",
      "Iteration 54, loss = 0.28120319\n",
      "Iteration 55, loss = 0.18548867\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.02160773\n",
      "Iteration 2, loss = 0.60173696\n",
      "Iteration 3, loss = 0.68120822\n",
      "Iteration 4, loss = 0.46628011\n",
      "Iteration 5, loss = 0.39803123\n",
      "Iteration 6, loss = 0.50434612\n",
      "Iteration 7, loss = 0.32670237\n",
      "Iteration 8, loss = 0.32374144\n",
      "Iteration 9, loss = 0.49603067\n",
      "Iteration 10, loss = 0.34865121\n",
      "Iteration 11, loss = 0.27821957\n",
      "Iteration 12, loss = 0.40837260\n",
      "Iteration 13, loss = 0.46267071\n",
      "Iteration 14, loss = 0.43601947\n",
      "Iteration 15, loss = 0.27058000\n",
      "Iteration 16, loss = 0.25799279\n",
      "Iteration 17, loss = 0.27928953\n",
      "Iteration 18, loss = 0.23658205\n",
      "Iteration 19, loss = 0.32715918\n",
      "Iteration 20, loss = 0.25619953\n",
      "Iteration 21, loss = 0.21092669\n",
      "Iteration 22, loss = 0.21327838\n",
      "Iteration 23, loss = 0.30431627\n",
      "Iteration 24, loss = 0.28393924\n",
      "Iteration 25, loss = 0.21721586\n",
      "Iteration 26, loss = 0.26220509\n",
      "Iteration 27, loss = 0.24826126\n",
      "Iteration 28, loss = 0.24977501\n",
      "Iteration 29, loss = 0.26790319\n",
      "Iteration 30, loss = 0.19769469\n",
      "Iteration 31, loss = 0.22423751\n",
      "Iteration 32, loss = 0.21767974\n",
      "Iteration 33, loss = 0.21587186\n",
      "Iteration 34, loss = 0.21066043\n",
      "Iteration 35, loss = 0.28380179\n",
      "Iteration 36, loss = 0.21177425\n",
      "Iteration 37, loss = 0.26279707\n",
      "Iteration 38, loss = 0.24510950\n",
      "Iteration 39, loss = 0.20115038\n",
      "Iteration 40, loss = 0.28185952\n",
      "Iteration 41, loss = 0.20780967\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.05946581\n",
      "Iteration 2, loss = 0.61147722\n",
      "Iteration 3, loss = 0.67299532\n",
      "Iteration 4, loss = 0.47801736\n",
      "Iteration 5, loss = 0.39634213\n",
      "Iteration 6, loss = 0.39150823\n",
      "Iteration 7, loss = 0.83665950\n",
      "Iteration 8, loss = 0.56580033\n",
      "Iteration 9, loss = 0.32554070\n",
      "Iteration 10, loss = 0.43051462\n",
      "Iteration 11, loss = 0.28525264\n",
      "Iteration 12, loss = 0.25217450\n",
      "Iteration 13, loss = 0.28143987\n",
      "Iteration 14, loss = 0.30236418\n",
      "Iteration 15, loss = 0.30077829\n",
      "Iteration 16, loss = 0.33020105\n",
      "Iteration 17, loss = 0.32807868\n",
      "Iteration 18, loss = 0.23920304\n",
      "Iteration 19, loss = 0.31379006\n",
      "Iteration 20, loss = 0.31085025\n",
      "Iteration 21, loss = 0.22816767\n",
      "Iteration 22, loss = 0.47908684\n",
      "Iteration 23, loss = 0.25285122\n",
      "Iteration 24, loss = 0.24009607\n",
      "Iteration 25, loss = 0.25730497\n",
      "Iteration 26, loss = 0.27856679\n",
      "Iteration 27, loss = 0.49824310\n",
      "Iteration 28, loss = 0.24366866\n",
      "Iteration 29, loss = 0.23172500\n",
      "Iteration 30, loss = 0.19946930\n",
      "Iteration 31, loss = 0.23009694\n",
      "Iteration 32, loss = 0.22015241\n",
      "Iteration 33, loss = 0.23882082\n",
      "Iteration 34, loss = 0.18364011\n",
      "Iteration 35, loss = 0.25995323\n",
      "Iteration 36, loss = 0.19564089\n",
      "Iteration 37, loss = 0.19288409\n",
      "Iteration 38, loss = 0.17756999\n",
      "Iteration 39, loss = 0.19266584\n",
      "Iteration 40, loss = 0.28323695\n",
      "Iteration 41, loss = 0.17870323\n",
      "Iteration 42, loss = 0.18873838\n",
      "Iteration 43, loss = 0.26664229\n",
      "Iteration 44, loss = 0.26326493\n",
      "Iteration 45, loss = 0.45530509\n",
      "Iteration 46, loss = 0.23214815\n",
      "Iteration 47, loss = 0.18000406\n",
      "Iteration 48, loss = 0.28010544\n",
      "Iteration 49, loss = 0.20653611\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy =  [0.90118939 0.91491308 0.86642269 0.92955169 0.95059469 0.90484904\n",
      " 0.74016468 0.95242452 0.915828   0.86916743]\n",
      "Accuracy mean and std : 0.895 (0.058)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93      5465\n",
      "           1       0.94      0.93      0.93      5465\n",
      "\n",
      "    accuracy                           0.93     10930\n",
      "   macro avg       0.93      0.93      0.93     10930\n",
      "weighted avg       0.93      0.93      0.93     10930\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KFold cross validation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=10)\n",
    "class_names = ['0','1']\n",
    "y_pred = cross_val_predict(modelMLP2, X, Y, cv = cv)\n",
    "\n",
    "cv_score_for_LR = cross_val_score(modelMLP2, X, Y, cv = 10)\n",
    "print(\"Accuracy = \",cv_score_for_LR)\n",
    "print('Accuracy mean and std : %.3f (%.3f)' % (mean(cv_score_for_LR), std(cv_score_for_LR)))\n",
    "\n",
    "print(classification_report(Y, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5585d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4541e4df",
   "metadata": {},
   "source": [
    "## Nearmiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd128c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 5465, 0: 692})\n",
      "Counter({0: 692, 1: 692})\n",
      "0.8953068592057761\n",
      "0.8953068592057761\n",
      "[[122  12]\n",
      " [ 17 126]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89       134\n",
      "           1       0.91      0.88      0.90       143\n",
      "\n",
      "    accuracy                           0.90       277\n",
      "   macro avg       0.90      0.90      0.90       277\n",
      "weighted avg       0.90      0.90      0.90       277\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "X = data.drop(['TYPE OF BIRTH    '], axis = 1)\n",
    "'''X = data[['AGE', 'WEIGHT', 'BMI', 'KG INCREASED PREGNANCY', 'PREVIOUS TERM PREGNANCIES','PARITY', 'GESTAGIONAL AGE ', 'COUPLE SITUATION ', 'ART', 'AMNIOCENTESIS', 'PREVIOUS CESAREAN',\n",
    "       'COMORBIDITY', 'PREINDUCTION', 'INDUCTION',\n",
    "       'ANESTHESIA ', 'EPISIOTOMY', 'Fetal INTRAPARTUM pH', 'COMPLICATIONS',\n",
    "        'LIQUID_ Hemorr�gico  ',\n",
    "       'LIQUID_ clear        ', 'LIQUID_ stained +    ',\n",
    "       'LIQUID_ stained ++   ', 'LIQUID_ stained +++  ', 'GROUP_ group 1     ',\n",
    "       'GROUP_ group 10    ', 'GROUP_ group 2a    ', 'GROUP_ group 2b    ',\n",
    "       'GROUP_ group 3     ', 'GROUP_ group 4a    ', 'GROUP_ group 4b    ',\n",
    "       'GROUP_ group 5     ', 'GROUP_ group 6     ', 'GROUP_ group 7     ',\n",
    "       'GROUP_ group 8     ', 'GROUP_ group 9     ']]'''\n",
    "\n",
    "Y = data['TYPE OF BIRTH    ']\n",
    "\n",
    "# Undersample imbalanced dataset with NearMiss-3\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "counter = Counter(Y)\n",
    "print(counter)\n",
    "# define the undersampling method\n",
    "undersample = NearMiss(version=1, n_neighbors_ver3=3)\n",
    "# transform the dataset\n",
    "X, Y = undersample.fit_resample(X, Y)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(Y)\n",
    "print(counter)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2,random_state = 109) # 80% training and 20% test\n",
    "\n",
    "\n",
    "# Simple Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "modelLR3 = LogisticRegression()\n",
    "modelLR3.fit(x_train, y_train)\n",
    "print(modelLR3.score(x_test, y_test))\n",
    "\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = modelLR3.predict(x_test)\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "print(ac)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = modelLR3.predict(x_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43305ca3",
   "metadata": {},
   "source": [
    "### K Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "998e393b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  [0.92086331 0.9352518  0.85611511 0.85611511 0.87681159 0.88405797\n",
      " 0.88405797 0.9057971  0.88405797 0.57971014]\n",
      "Accuracy mean and std : 0.858 (0.096)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.79      0.81       692\n",
      "           1       0.80      0.84      0.82       692\n",
      "\n",
      "    accuracy                           0.82      1384\n",
      "   macro avg       0.82      0.82      0.82      1384\n",
      "weighted avg       0.82      0.82      0.82      1384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KFold cross validation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=10)\n",
    "class_names = ['0','1']\n",
    "y_pred = cross_val_predict(modelLR3, X, Y, cv = cv)\n",
    "\n",
    "cv_score_for_LR = cross_val_score(modelLR3, X, Y, cv = 10)\n",
    "print(\"Accuracy = \",cv_score_for_LR)\n",
    "print('Accuracy mean and std : %.3f (%.3f)' % (mean(cv_score_for_LR), std(cv_score_for_LR)))\n",
    "\n",
    "print(classification_report(Y, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111cdc0e",
   "metadata": {},
   "source": [
    "## MLP Nearmiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62b3582e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 7.79073034\n",
      "Iteration 2, loss = 6.58851627\n",
      "Iteration 3, loss = 5.50665533\n",
      "Iteration 4, loss = 3.32502663\n",
      "Iteration 5, loss = 3.18472102\n",
      "Iteration 6, loss = 2.86481415\n",
      "Iteration 7, loss = 2.16411692\n",
      "Iteration 8, loss = 1.91111898\n",
      "Iteration 9, loss = 1.69060629\n",
      "Iteration 10, loss = 1.85479001\n",
      "Iteration 11, loss = 1.36165936\n",
      "Iteration 12, loss = 1.01064279\n",
      "Iteration 13, loss = 0.99201918\n",
      "Iteration 14, loss = 0.66949394\n",
      "Iteration 15, loss = 0.53289803\n",
      "Iteration 16, loss = 0.51975642\n",
      "Iteration 17, loss = 0.50089880\n",
      "Iteration 18, loss = 0.54000996\n",
      "Iteration 19, loss = 0.49424828\n",
      "Iteration 20, loss = 0.48628495\n",
      "Iteration 21, loss = 0.48283014\n",
      "Iteration 22, loss = 0.46223120\n",
      "Iteration 23, loss = 0.48218086\n",
      "Iteration 24, loss = 0.45018293\n",
      "Iteration 25, loss = 0.45517143\n",
      "Iteration 26, loss = 0.53110473\n",
      "Iteration 27, loss = 0.49250026\n",
      "Iteration 28, loss = 0.47388562\n",
      "Iteration 29, loss = 0.48501916\n",
      "Iteration 30, loss = 0.63837371\n",
      "Iteration 31, loss = 0.71393171\n",
      "Iteration 32, loss = 0.63458575\n",
      "Iteration 33, loss = 0.65289161\n",
      "Iteration 34, loss = 0.65113176\n",
      "Iteration 35, loss = 0.55566077\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8086642599277978\n",
      "0.8086642599277978\n",
      "[[104  30]\n",
      " [ 23 120]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.78      0.80       134\n",
      "           1       0.80      0.84      0.82       143\n",
      "\n",
      "    accuracy                           0.81       277\n",
      "   macro avg       0.81      0.81      0.81       277\n",
      "weighted avg       0.81      0.81      0.81       277\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "modelMLP3 = MLPClassifier(hidden_layer_sizes = (6,5), random_state = 42, verbose = True, learning_rate_init = 0.01)\n",
    "modelMLP3.fit(x_train, y_train)\n",
    "print(modelMLP3.score(x_test, y_test))\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = modelMLP3.predict(x_test)\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "print(ac)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = modelMLP3.predict(x_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b930ff",
   "metadata": {},
   "source": [
    "### K Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15317ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 8.32537756\n",
      "Iteration 2, loss = 6.09662121\n",
      "Iteration 3, loss = 6.15122842\n",
      "Iteration 4, loss = 3.97574806\n",
      "Iteration 5, loss = 4.21054718\n",
      "Iteration 6, loss = 1.26162490\n",
      "Iteration 7, loss = 0.76934494\n",
      "Iteration 8, loss = 0.70419643\n",
      "Iteration 9, loss = 0.78084187\n",
      "Iteration 10, loss = 1.21503275\n",
      "Iteration 11, loss = 1.39741370\n",
      "Iteration 12, loss = 1.76206836\n",
      "Iteration 13, loss = 1.71824308\n",
      "Iteration 14, loss = 0.87639812\n",
      "Iteration 15, loss = 0.72260355\n",
      "Iteration 16, loss = 0.61467654\n",
      "Iteration 17, loss = 0.60318878\n",
      "Iteration 18, loss = 0.58225876\n",
      "Iteration 19, loss = 0.54842783\n",
      "Iteration 20, loss = 0.49989134\n",
      "Iteration 21, loss = 0.49949752\n",
      "Iteration 22, loss = 0.54993242\n",
      "Iteration 23, loss = 0.56981602\n",
      "Iteration 24, loss = 0.59721205\n",
      "Iteration 25, loss = 0.46642918\n",
      "Iteration 26, loss = 0.46468871\n",
      "Iteration 27, loss = 0.44792783\n",
      "Iteration 28, loss = 0.46925693\n",
      "Iteration 29, loss = 0.43046230\n",
      "Iteration 30, loss = 0.42996757\n",
      "Iteration 31, loss = 0.61791457\n",
      "Iteration 32, loss = 0.54730821\n",
      "Iteration 33, loss = 0.69341964\n",
      "Iteration 34, loss = 0.57144775\n",
      "Iteration 35, loss = 0.46702717\n",
      "Iteration 36, loss = 0.54440637\n",
      "Iteration 37, loss = 1.16700815\n",
      "Iteration 38, loss = 1.00898369\n",
      "Iteration 39, loss = 0.55308236\n",
      "Iteration 40, loss = 0.72401764\n",
      "Iteration 41, loss = 2.41959841\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 8.24808150\n",
      "Iteration 2, loss = 7.64696635\n",
      "Iteration 3, loss = 4.75195666\n",
      "Iteration 4, loss = 2.63257706\n",
      "Iteration 5, loss = 1.95854920\n",
      "Iteration 6, loss = 1.81711546\n",
      "Iteration 7, loss = 0.82847490\n",
      "Iteration 8, loss = 0.59635600\n",
      "Iteration 9, loss = 0.69546295\n",
      "Iteration 10, loss = 0.62342988\n",
      "Iteration 11, loss = 0.83738084\n",
      "Iteration 12, loss = 0.76789937\n",
      "Iteration 13, loss = 0.53201125\n",
      "Iteration 14, loss = 0.96303511\n",
      "Iteration 15, loss = 1.03292942\n",
      "Iteration 16, loss = 0.83995933\n",
      "Iteration 17, loss = 1.09262888\n",
      "Iteration 18, loss = 1.12838345\n",
      "Iteration 19, loss = 0.73946320\n",
      "Iteration 20, loss = 0.85277121\n",
      "Iteration 21, loss = 0.58045891\n",
      "Iteration 22, loss = 0.49507641\n",
      "Iteration 23, loss = 0.47913947\n",
      "Iteration 24, loss = 0.45994631\n",
      "Iteration 25, loss = 0.43962644\n",
      "Iteration 26, loss = 0.49534025\n",
      "Iteration 27, loss = 0.44520236\n",
      "Iteration 28, loss = 0.55140391\n",
      "Iteration 29, loss = 0.41042955\n",
      "Iteration 30, loss = 0.41651682\n",
      "Iteration 31, loss = 0.86948133\n",
      "Iteration 32, loss = 1.14090587\n",
      "Iteration 33, loss = 1.31747610\n",
      "Iteration 34, loss = 0.83048213\n",
      "Iteration 35, loss = 0.99698445\n",
      "Iteration 36, loss = 0.90386752\n",
      "Iteration 37, loss = 1.03905964\n",
      "Iteration 38, loss = 0.88690173\n",
      "Iteration 39, loss = 0.71027003\n",
      "Iteration 40, loss = 0.79887904\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 8.17352308\n",
      "Iteration 2, loss = 7.39357018\n",
      "Iteration 3, loss = 3.52028152\n",
      "Iteration 4, loss = 3.00675991\n",
      "Iteration 5, loss = 1.83075044\n",
      "Iteration 6, loss = 2.19035619\n",
      "Iteration 7, loss = 1.37893987\n",
      "Iteration 8, loss = 1.09423618\n",
      "Iteration 9, loss = 1.29836197\n",
      "Iteration 10, loss = 0.74785083\n",
      "Iteration 11, loss = 0.54469911\n",
      "Iteration 12, loss = 0.51966166\n",
      "Iteration 13, loss = 0.68445185\n",
      "Iteration 14, loss = 0.72975571\n",
      "Iteration 15, loss = 0.54526939\n",
      "Iteration 16, loss = 0.54029076\n",
      "Iteration 17, loss = 0.50259919\n",
      "Iteration 18, loss = 0.47976445\n",
      "Iteration 19, loss = 0.47756027\n",
      "Iteration 20, loss = 0.46719002\n",
      "Iteration 21, loss = 0.45638240\n",
      "Iteration 22, loss = 0.49486770\n",
      "Iteration 23, loss = 0.45884021\n",
      "Iteration 24, loss = 0.55575882\n",
      "Iteration 25, loss = 0.61021334\n",
      "Iteration 26, loss = 0.82293646\n",
      "Iteration 27, loss = 1.13052044\n",
      "Iteration 28, loss = 0.56743947\n",
      "Iteration 29, loss = 0.99515149\n",
      "Iteration 30, loss = 0.79788191\n",
      "Iteration 31, loss = 0.64260666\n",
      "Iteration 32, loss = 0.67728060\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 8.10798169\n",
      "Iteration 2, loss = 6.94630518\n",
      "Iteration 3, loss = 3.58711294\n",
      "Iteration 4, loss = 3.51138643\n",
      "Iteration 5, loss = 2.47777258\n",
      "Iteration 6, loss = 3.45789024\n",
      "Iteration 7, loss = 1.42168100\n",
      "Iteration 8, loss = 1.21369027\n",
      "Iteration 9, loss = 0.83223450\n",
      "Iteration 10, loss = 0.72164857\n",
      "Iteration 11, loss = 0.90236225\n",
      "Iteration 12, loss = 0.68501623\n",
      "Iteration 13, loss = 0.73991417\n",
      "Iteration 14, loss = 0.62442291\n",
      "Iteration 15, loss = 0.50946382\n",
      "Iteration 16, loss = 0.49287662\n",
      "Iteration 17, loss = 0.48983906\n",
      "Iteration 18, loss = 0.47743397\n",
      "Iteration 19, loss = 0.52877052\n",
      "Iteration 20, loss = 0.50433297\n",
      "Iteration 21, loss = 0.60028065\n",
      "Iteration 22, loss = 0.45865132\n",
      "Iteration 23, loss = 0.44761507\n",
      "Iteration 24, loss = 0.52199253\n",
      "Iteration 25, loss = 1.03906595\n",
      "Iteration 26, loss = 1.15743246\n",
      "Iteration 27, loss = 1.39744381\n",
      "Iteration 28, loss = 0.86436490\n",
      "Iteration 29, loss = 0.80316549\n",
      "Iteration 30, loss = 0.49652837\n",
      "Iteration 31, loss = 0.56616191\n",
      "Iteration 32, loss = 0.53358831\n",
      "Iteration 33, loss = 0.43621393\n",
      "Iteration 34, loss = 0.42384970\n",
      "Iteration 35, loss = 0.41645472\n",
      "Iteration 36, loss = 0.44113836\n",
      "Iteration 37, loss = 0.65038908\n",
      "Iteration 38, loss = 0.51664653\n",
      "Iteration 39, loss = 0.61157575\n",
      "Iteration 40, loss = 0.63219963\n",
      "Iteration 41, loss = 0.64815595\n",
      "Iteration 42, loss = 0.43487507\n",
      "Iteration 43, loss = 0.45965275\n",
      "Iteration 44, loss = 0.38686960\n",
      "Iteration 45, loss = 0.38378530\n",
      "Iteration 46, loss = 0.50612532\n",
      "Iteration 47, loss = 0.46989544\n",
      "Iteration 48, loss = 0.41660306\n",
      "Iteration 49, loss = 0.36793839\n",
      "Iteration 50, loss = 0.36896984\n",
      "Iteration 51, loss = 0.48622240\n",
      "Iteration 52, loss = 1.32679555\n",
      "Iteration 53, loss = 0.88304217\n",
      "Iteration 54, loss = 0.83209226\n",
      "Iteration 55, loss = 1.04821563\n",
      "Iteration 56, loss = 0.90980759\n",
      "Iteration 57, loss = 0.49303170\n",
      "Iteration 58, loss = 0.39875361\n",
      "Iteration 59, loss = 0.45845583\n",
      "Iteration 60, loss = 0.40677353\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 8.11945004\n",
      "Iteration 2, loss = 7.67140368\n",
      "Iteration 3, loss = 5.11523363\n",
      "Iteration 4, loss = 3.01910540\n",
      "Iteration 5, loss = 1.69191528\n",
      "Iteration 6, loss = 1.22850793\n",
      "Iteration 7, loss = 0.87525500\n",
      "Iteration 8, loss = 0.54366250\n",
      "Iteration 9, loss = 0.46708560\n",
      "Iteration 10, loss = 0.42164262\n",
      "Iteration 11, loss = 0.41579553\n",
      "Iteration 12, loss = 0.40974143\n",
      "Iteration 13, loss = 0.43085858\n",
      "Iteration 14, loss = 0.40597473\n",
      "Iteration 15, loss = 0.38804066\n",
      "Iteration 16, loss = 0.37328300\n",
      "Iteration 17, loss = 0.36772283\n",
      "Iteration 18, loss = 0.36173360\n",
      "Iteration 19, loss = 0.36692438\n",
      "Iteration 20, loss = 0.35056042\n",
      "Iteration 21, loss = 0.34679231\n",
      "Iteration 22, loss = 0.35301031\n",
      "Iteration 23, loss = 0.33981110\n",
      "Iteration 24, loss = 0.33676109\n",
      "Iteration 25, loss = 0.33487810\n",
      "Iteration 26, loss = 0.32745552\n",
      "Iteration 27, loss = 0.32953461\n",
      "Iteration 28, loss = 0.32849440\n",
      "Iteration 29, loss = 0.31853229\n",
      "Iteration 30, loss = 0.31375517\n",
      "Iteration 31, loss = 0.31319270\n",
      "Iteration 32, loss = 0.31235103\n",
      "Iteration 33, loss = 0.32538416\n",
      "Iteration 34, loss = 0.30962378\n",
      "Iteration 35, loss = 0.29981818\n",
      "Iteration 36, loss = 0.29242163\n",
      "Iteration 37, loss = 0.30153959\n",
      "Iteration 38, loss = 0.29302972\n",
      "Iteration 39, loss = 0.29128914\n",
      "Iteration 40, loss = 0.30142693\n",
      "Iteration 41, loss = 0.29839788\n",
      "Iteration 42, loss = 0.28674489\n",
      "Iteration 43, loss = 0.28181751\n",
      "Iteration 44, loss = 0.28807912\n",
      "Iteration 45, loss = 0.29992904\n",
      "Iteration 46, loss = 0.27044555\n",
      "Iteration 47, loss = 0.26409966\n",
      "Iteration 48, loss = 0.26128779\n",
      "Iteration 49, loss = 0.26093403\n",
      "Iteration 50, loss = 0.25790277\n",
      "Iteration 51, loss = 0.25344383\n",
      "Iteration 52, loss = 0.26771514\n",
      "Iteration 53, loss = 0.25976722\n",
      "Iteration 54, loss = 0.24815354\n",
      "Iteration 55, loss = 0.24363276\n",
      "Iteration 56, loss = 0.24354158\n",
      "Iteration 57, loss = 0.24567704\n",
      "Iteration 58, loss = 0.28432081\n",
      "Iteration 59, loss = 0.27290369\n",
      "Iteration 60, loss = 0.37493361\n",
      "Iteration 61, loss = 0.38565561\n",
      "Iteration 62, loss = 0.26678900\n",
      "Iteration 63, loss = 0.26176617\n",
      "Iteration 64, loss = 0.25287825\n",
      "Iteration 65, loss = 0.25455509\n",
      "Iteration 66, loss = 0.23795937\n",
      "Iteration 67, loss = 0.23125611\n",
      "Iteration 68, loss = 0.22602767\n",
      "Iteration 69, loss = 0.24478684\n",
      "Iteration 70, loss = 0.22846839\n",
      "Iteration 71, loss = 0.21289647\n",
      "Iteration 72, loss = 0.21271195\n",
      "Iteration 73, loss = 0.21085582\n",
      "Iteration 74, loss = 0.20550471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 75, loss = 0.21343376\n",
      "Iteration 76, loss = 0.21598904\n",
      "Iteration 77, loss = 0.20197309\n",
      "Iteration 78, loss = 0.20311193\n",
      "Iteration 79, loss = 0.22193540\n",
      "Iteration 80, loss = 0.20109863\n",
      "Iteration 81, loss = 0.19478897\n",
      "Iteration 82, loss = 0.18987238\n",
      "Iteration 83, loss = 0.18973364\n",
      "Iteration 84, loss = 0.20250724\n",
      "Iteration 85, loss = 0.20265283\n",
      "Iteration 86, loss = 0.19327361\n",
      "Iteration 87, loss = 0.19900710\n",
      "Iteration 88, loss = 0.21431172\n",
      "Iteration 89, loss = 0.26372120\n",
      "Iteration 90, loss = 0.24791222\n",
      "Iteration 91, loss = 0.19426975\n",
      "Iteration 92, loss = 0.18385069\n",
      "Iteration 93, loss = 0.25519379\n",
      "Iteration 94, loss = 0.19855585\n",
      "Iteration 95, loss = 0.17244279\n",
      "Iteration 96, loss = 0.17226930\n",
      "Iteration 97, loss = 0.18518529\n",
      "Iteration 98, loss = 0.17265214\n",
      "Iteration 99, loss = 0.18576064\n",
      "Iteration 100, loss = 0.17414208\n",
      "Iteration 101, loss = 0.18743784\n",
      "Iteration 102, loss = 0.16746140\n",
      "Iteration 103, loss = 0.16924509\n",
      "Iteration 104, loss = 0.17910322\n",
      "Iteration 105, loss = 0.19468645\n",
      "Iteration 106, loss = 0.20141894\n",
      "Iteration 107, loss = 0.21466791\n",
      "Iteration 108, loss = 0.18498091\n",
      "Iteration 109, loss = 0.17128320\n",
      "Iteration 110, loss = 0.18626063\n",
      "Iteration 111, loss = 0.15725461\n",
      "Iteration 112, loss = 0.16898412\n",
      "Iteration 113, loss = 0.16306462\n",
      "Iteration 114, loss = 0.17844819\n",
      "Iteration 115, loss = 0.17234490\n",
      "Iteration 116, loss = 0.18138845\n",
      "Iteration 117, loss = 0.20784854\n",
      "Iteration 118, loss = 0.28137324\n",
      "Iteration 119, loss = 0.23226201\n",
      "Iteration 120, loss = 0.19720396\n",
      "Iteration 121, loss = 0.16600475\n",
      "Iteration 122, loss = 0.17217791\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 6.86619094\n",
      "Iteration 2, loss = 1.59875730\n",
      "Iteration 3, loss = 1.31902013\n",
      "Iteration 4, loss = 1.63847388\n",
      "Iteration 5, loss = 0.89610313\n",
      "Iteration 6, loss = 0.85302241\n",
      "Iteration 7, loss = 0.74922363\n",
      "Iteration 8, loss = 1.67723922\n",
      "Iteration 9, loss = 2.11531772\n",
      "Iteration 10, loss = 0.96103240\n",
      "Iteration 11, loss = 0.68747605\n",
      "Iteration 12, loss = 0.71581583\n",
      "Iteration 13, loss = 0.95565932\n",
      "Iteration 14, loss = 0.66638738\n",
      "Iteration 15, loss = 0.84446233\n",
      "Iteration 16, loss = 0.77458309\n",
      "Iteration 17, loss = 0.58603582\n",
      "Iteration 18, loss = 0.52335821\n",
      "Iteration 19, loss = 0.49510923\n",
      "Iteration 20, loss = 0.47277425\n",
      "Iteration 21, loss = 0.48488210\n",
      "Iteration 22, loss = 0.42963492\n",
      "Iteration 23, loss = 0.46288221\n",
      "Iteration 24, loss = 0.43395611\n",
      "Iteration 25, loss = 0.44604447\n",
      "Iteration 26, loss = 0.51138149\n",
      "Iteration 27, loss = 0.43715546\n",
      "Iteration 28, loss = 0.40759962\n",
      "Iteration 29, loss = 0.42322878\n",
      "Iteration 30, loss = 0.45869100\n",
      "Iteration 31, loss = 0.43217471\n",
      "Iteration 32, loss = 0.68164495\n",
      "Iteration 33, loss = 0.48922578\n",
      "Iteration 34, loss = 0.42309901\n",
      "Iteration 35, loss = 0.38858047\n",
      "Iteration 36, loss = 0.45172813\n",
      "Iteration 37, loss = 0.46671610\n",
      "Iteration 38, loss = 0.38271284\n",
      "Iteration 39, loss = 0.41396151\n",
      "Iteration 40, loss = 1.02472334\n",
      "Iteration 41, loss = 1.47359571\n",
      "Iteration 42, loss = 1.04803851\n",
      "Iteration 43, loss = 0.56979293\n",
      "Iteration 44, loss = 0.39878598\n",
      "Iteration 45, loss = 0.40263567\n",
      "Iteration 46, loss = 0.47447299\n",
      "Iteration 47, loss = 0.97230780\n",
      "Iteration 48, loss = 0.73091268\n",
      "Iteration 49, loss = 0.51482107\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 6.76386041\n",
      "Iteration 2, loss = 1.76444329\n",
      "Iteration 3, loss = 1.35660397\n",
      "Iteration 4, loss = 0.95339308\n",
      "Iteration 5, loss = 1.64582819\n",
      "Iteration 6, loss = 1.07023093\n",
      "Iteration 7, loss = 1.19430941\n",
      "Iteration 8, loss = 0.66613523\n",
      "Iteration 9, loss = 0.86134535\n",
      "Iteration 10, loss = 0.69340498\n",
      "Iteration 11, loss = 0.77097403\n",
      "Iteration 12, loss = 0.96441445\n",
      "Iteration 13, loss = 0.97288477\n",
      "Iteration 14, loss = 0.64600531\n",
      "Iteration 15, loss = 0.63466576\n",
      "Iteration 16, loss = 0.65640878\n",
      "Iteration 17, loss = 0.48907447\n",
      "Iteration 18, loss = 0.58954940\n",
      "Iteration 19, loss = 0.55930588\n",
      "Iteration 20, loss = 0.72649375\n",
      "Iteration 21, loss = 0.52309351\n",
      "Iteration 22, loss = 0.54815499\n",
      "Iteration 23, loss = 0.53519347\n",
      "Iteration 24, loss = 0.47058668\n",
      "Iteration 25, loss = 0.48923739\n",
      "Iteration 26, loss = 0.50060923\n",
      "Iteration 27, loss = 0.52673260\n",
      "Iteration 28, loss = 0.46472265\n",
      "Iteration 29, loss = 0.76057548\n",
      "Iteration 30, loss = 1.15384428\n",
      "Iteration 31, loss = 1.43155379\n",
      "Iteration 32, loss = 1.37987575\n",
      "Iteration 33, loss = 0.93554119\n",
      "Iteration 34, loss = 1.03332186\n",
      "Iteration 35, loss = 0.68400590\n",
      "Iteration 36, loss = 0.48921857\n",
      "Iteration 37, loss = 0.46517456\n",
      "Iteration 38, loss = 0.42492091\n",
      "Iteration 39, loss = 0.51486307\n",
      "Iteration 40, loss = 0.39605204\n",
      "Iteration 41, loss = 0.38975549\n",
      "Iteration 42, loss = 0.40231162\n",
      "Iteration 43, loss = 0.41543215\n",
      "Iteration 44, loss = 0.44603616\n",
      "Iteration 45, loss = 0.36550983\n",
      "Iteration 46, loss = 0.38998824\n",
      "Iteration 47, loss = 0.48694182\n",
      "Iteration 48, loss = 0.36959006\n",
      "Iteration 49, loss = 0.37428805\n",
      "Iteration 50, loss = 0.53186523\n",
      "Iteration 51, loss = 0.60735968\n",
      "Iteration 52, loss = 0.57097922\n",
      "Iteration 53, loss = 0.65692770\n",
      "Iteration 54, loss = 0.60508032\n",
      "Iteration 55, loss = 0.54233050\n",
      "Iteration 56, loss = 0.66379926\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.02000392\n",
      "Iteration 2, loss = 3.00021619\n",
      "Iteration 3, loss = 2.36312772\n",
      "Iteration 4, loss = 1.01601183\n",
      "Iteration 5, loss = 1.57035713\n",
      "Iteration 6, loss = 1.14076353\n",
      "Iteration 7, loss = 1.39626037\n",
      "Iteration 8, loss = 1.10208247\n",
      "Iteration 9, loss = 0.93747531\n",
      "Iteration 10, loss = 0.80788960\n",
      "Iteration 11, loss = 0.71723608\n",
      "Iteration 12, loss = 0.73429534\n",
      "Iteration 13, loss = 0.88281980\n",
      "Iteration 14, loss = 0.57889782\n",
      "Iteration 15, loss = 0.53922684\n",
      "Iteration 16, loss = 0.54273710\n",
      "Iteration 17, loss = 0.46340775\n",
      "Iteration 18, loss = 0.55177809\n",
      "Iteration 19, loss = 0.58455515\n",
      "Iteration 20, loss = 0.57891801\n",
      "Iteration 21, loss = 0.65435095\n",
      "Iteration 22, loss = 0.49033132\n",
      "Iteration 23, loss = 0.51062902\n",
      "Iteration 24, loss = 0.48700425\n",
      "Iteration 25, loss = 0.45204046\n",
      "Iteration 26, loss = 0.54785507\n",
      "Iteration 27, loss = 0.58120705\n",
      "Iteration 28, loss = 0.47528725\n",
      "Iteration 29, loss = 0.40027007\n",
      "Iteration 30, loss = 0.39524200\n",
      "Iteration 31, loss = 0.38734471\n",
      "Iteration 32, loss = 0.39425137\n",
      "Iteration 33, loss = 0.44210688\n",
      "Iteration 34, loss = 0.44583761\n",
      "Iteration 35, loss = 0.39348513\n",
      "Iteration 36, loss = 0.44067862\n",
      "Iteration 37, loss = 0.39909725\n",
      "Iteration 38, loss = 0.39565839\n",
      "Iteration 39, loss = 0.40169822\n",
      "Iteration 40, loss = 1.05444336\n",
      "Iteration 41, loss = 1.82579227\n",
      "Iteration 42, loss = 1.82663811\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 6.95044025\n",
      "Iteration 2, loss = 1.55451439\n",
      "Iteration 3, loss = 1.59073218\n",
      "Iteration 4, loss = 1.22272148\n",
      "Iteration 5, loss = 0.77811160\n",
      "Iteration 6, loss = 0.59607844\n",
      "Iteration 7, loss = 0.72793405\n",
      "Iteration 8, loss = 0.58584147\n",
      "Iteration 9, loss = 0.49786079\n",
      "Iteration 10, loss = 0.48323462\n",
      "Iteration 11, loss = 0.50240301\n",
      "Iteration 12, loss = 0.51562576\n",
      "Iteration 13, loss = 0.85913458\n",
      "Iteration 14, loss = 1.03798751\n",
      "Iteration 15, loss = 0.84143769\n",
      "Iteration 16, loss = 0.45808652\n",
      "Iteration 17, loss = 0.44198423\n",
      "Iteration 18, loss = 0.70838751\n",
      "Iteration 19, loss = 0.83783471\n",
      "Iteration 20, loss = 0.72787745\n",
      "Iteration 21, loss = 0.55241463\n",
      "Iteration 22, loss = 0.51205505\n",
      "Iteration 23, loss = 0.45264539\n",
      "Iteration 24, loss = 0.47177392\n",
      "Iteration 25, loss = 0.67714843\n",
      "Iteration 26, loss = 0.88655581\n",
      "Iteration 27, loss = 0.69412388\n",
      "Iteration 28, loss = 0.79393327\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 6.79817355\n",
      "Iteration 2, loss = 1.73087698\n",
      "Iteration 3, loss = 1.45258763\n",
      "Iteration 4, loss = 1.72018160\n",
      "Iteration 5, loss = 1.25359303\n",
      "Iteration 6, loss = 0.70699013\n",
      "Iteration 7, loss = 0.73808157\n",
      "Iteration 8, loss = 0.56895641\n",
      "Iteration 9, loss = 0.80647482\n",
      "Iteration 10, loss = 0.84709250\n",
      "Iteration 11, loss = 0.58201906\n",
      "Iteration 12, loss = 0.67494836\n",
      "Iteration 13, loss = 0.70850194\n",
      "Iteration 14, loss = 0.98251144\n",
      "Iteration 15, loss = 0.54674441\n",
      "Iteration 16, loss = 0.55083089\n",
      "Iteration 17, loss = 0.60859202\n",
      "Iteration 18, loss = 0.44997152\n",
      "Iteration 19, loss = 0.43623907\n",
      "Iteration 20, loss = 0.45226409\n",
      "Iteration 21, loss = 0.44363035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, loss = 0.42379951\n",
      "Iteration 23, loss = 0.69344013\n",
      "Iteration 24, loss = 0.83993670\n",
      "Iteration 25, loss = 0.61879974\n",
      "Iteration 26, loss = 0.68200074\n",
      "Iteration 27, loss = 0.55085918\n",
      "Iteration 28, loss = 0.72220519\n",
      "Iteration 29, loss = 1.19144942\n",
      "Iteration 30, loss = 0.69248949\n",
      "Iteration 31, loss = 1.09745980\n",
      "Iteration 32, loss = 0.92515018\n",
      "Iteration 33, loss = 1.03365198\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.19018459\n",
      "Iteration 2, loss = 1.76278322\n",
      "Iteration 3, loss = 1.13381912\n",
      "Iteration 4, loss = 1.25160747\n",
      "Iteration 5, loss = 0.82089107\n",
      "Iteration 6, loss = 1.03214827\n",
      "Iteration 7, loss = 0.78308740\n",
      "Iteration 8, loss = 0.55984644\n",
      "Iteration 9, loss = 0.81564520\n",
      "Iteration 10, loss = 0.61387348\n",
      "Iteration 11, loss = 1.32140847\n",
      "Iteration 12, loss = 1.83708134\n",
      "Iteration 13, loss = 0.87591370\n",
      "Iteration 14, loss = 0.72305585\n",
      "Iteration 15, loss = 0.80348525\n",
      "Iteration 16, loss = 0.60798517\n",
      "Iteration 17, loss = 0.60247299\n",
      "Iteration 18, loss = 0.59487247\n",
      "Iteration 19, loss = 0.56045498\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 6.90751339\n",
      "Iteration 2, loss = 2.36376319\n",
      "Iteration 3, loss = 2.15245601\n",
      "Iteration 4, loss = 1.44910245\n",
      "Iteration 5, loss = 1.54424586\n",
      "Iteration 6, loss = 1.32666488\n",
      "Iteration 7, loss = 0.92856151\n",
      "Iteration 8, loss = 0.89795500\n",
      "Iteration 9, loss = 0.74853816\n",
      "Iteration 10, loss = 0.65768047\n",
      "Iteration 11, loss = 0.81569707\n",
      "Iteration 12, loss = 0.71373418\n",
      "Iteration 13, loss = 0.62168520\n",
      "Iteration 14, loss = 0.52304130\n",
      "Iteration 15, loss = 0.47053107\n",
      "Iteration 16, loss = 0.49301550\n",
      "Iteration 17, loss = 0.48293880\n",
      "Iteration 18, loss = 0.45261533\n",
      "Iteration 19, loss = 0.47572669\n",
      "Iteration 20, loss = 0.69466291\n",
      "Iteration 21, loss = 0.44378165\n",
      "Iteration 22, loss = 0.47633300\n",
      "Iteration 23, loss = 0.45343474\n",
      "Iteration 24, loss = 0.41901744\n",
      "Iteration 25, loss = 0.44144641\n",
      "Iteration 26, loss = 0.91859179\n",
      "Iteration 27, loss = 0.46758254\n",
      "Iteration 28, loss = 0.57420315\n",
      "Iteration 29, loss = 0.72421334\n",
      "Iteration 30, loss = 0.61252267\n",
      "Iteration 31, loss = 0.73676923\n",
      "Iteration 32, loss = 1.15374616\n",
      "Iteration 33, loss = 0.64032696\n",
      "Iteration 34, loss = 0.59725339\n",
      "Iteration 35, loss = 0.51271008\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 6.82822554\n",
      "Iteration 2, loss = 2.57810674\n",
      "Iteration 3, loss = 1.80426957\n",
      "Iteration 4, loss = 1.48598709\n",
      "Iteration 5, loss = 1.72134660\n",
      "Iteration 6, loss = 1.29995641\n",
      "Iteration 7, loss = 1.36129211\n",
      "Iteration 8, loss = 0.98348539\n",
      "Iteration 9, loss = 0.67427839\n",
      "Iteration 10, loss = 0.74377644\n",
      "Iteration 11, loss = 0.59075368\n",
      "Iteration 12, loss = 0.64591642\n",
      "Iteration 13, loss = 0.87989727\n",
      "Iteration 14, loss = 0.65216194\n",
      "Iteration 15, loss = 0.64006085\n",
      "Iteration 16, loss = 0.65379428\n",
      "Iteration 17, loss = 0.60406284\n",
      "Iteration 18, loss = 0.59000101\n",
      "Iteration 19, loss = 0.58448862\n",
      "Iteration 20, loss = 0.87561061\n",
      "Iteration 21, loss = 1.11768343\n",
      "Iteration 22, loss = 0.86039990\n",
      "Iteration 23, loss = 0.88653393\n",
      "Iteration 24, loss = 0.56780402\n",
      "Iteration 25, loss = 0.53437464\n",
      "Iteration 26, loss = 0.86366507\n",
      "Iteration 27, loss = 0.65960532\n",
      "Iteration 28, loss = 0.50602072\n",
      "Iteration 29, loss = 0.46526026\n",
      "Iteration 30, loss = 0.43751331\n",
      "Iteration 31, loss = 0.51726068\n",
      "Iteration 32, loss = 0.40203591\n",
      "Iteration 33, loss = 0.40077528\n",
      "Iteration 34, loss = 0.40658117\n",
      "Iteration 35, loss = 0.40719048\n",
      "Iteration 36, loss = 0.38750188\n",
      "Iteration 37, loss = 0.48312993\n",
      "Iteration 38, loss = 0.68296163\n",
      "Iteration 39, loss = 0.93528171\n",
      "Iteration 40, loss = 1.40236632\n",
      "Iteration 41, loss = 2.97793010\n",
      "Iteration 42, loss = 2.38633916\n",
      "Iteration 43, loss = 1.19284977\n",
      "Iteration 44, loss = 0.85025760\n",
      "Iteration 45, loss = 0.65279183\n",
      "Iteration 46, loss = 0.56316559\n",
      "Iteration 47, loss = 0.49669561\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 6.98014945\n",
      "Iteration 2, loss = 2.08448190\n",
      "Iteration 3, loss = 1.35784931\n",
      "Iteration 4, loss = 1.16761839\n",
      "Iteration 5, loss = 0.82627736\n",
      "Iteration 6, loss = 1.40464564\n",
      "Iteration 7, loss = 1.52511248\n",
      "Iteration 8, loss = 1.16083926\n",
      "Iteration 9, loss = 1.42224836\n",
      "Iteration 10, loss = 2.19411156\n",
      "Iteration 11, loss = 1.78142641\n",
      "Iteration 12, loss = 1.90825680\n",
      "Iteration 13, loss = 1.25465582\n",
      "Iteration 14, loss = 1.01845876\n",
      "Iteration 15, loss = 0.79886956\n",
      "Iteration 16, loss = 0.57964141\n",
      "Iteration 17, loss = 0.58292877\n",
      "Iteration 18, loss = 0.56009534\n",
      "Iteration 19, loss = 0.52297730\n",
      "Iteration 20, loss = 0.60052386\n",
      "Iteration 21, loss = 0.70276373\n",
      "Iteration 22, loss = 0.61416628\n",
      "Iteration 23, loss = 0.65592930\n",
      "Iteration 24, loss = 0.51477308\n",
      "Iteration 25, loss = 0.44703945\n",
      "Iteration 26, loss = 0.52669380\n",
      "Iteration 27, loss = 0.44691434\n",
      "Iteration 28, loss = 0.41738343\n",
      "Iteration 29, loss = 0.41743329\n",
      "Iteration 30, loss = 0.42832440\n",
      "Iteration 31, loss = 0.45816228\n",
      "Iteration 32, loss = 0.42437556\n",
      "Iteration 33, loss = 0.49822461\n",
      "Iteration 34, loss = 0.46057219\n",
      "Iteration 35, loss = 0.49692684\n",
      "Iteration 36, loss = 0.40138183\n",
      "Iteration 37, loss = 0.51055509\n",
      "Iteration 38, loss = 0.54893395\n",
      "Iteration 39, loss = 0.66113045\n",
      "Iteration 40, loss = 0.58054522\n",
      "Iteration 41, loss = 0.40402087\n",
      "Iteration 42, loss = 0.38385319\n",
      "Iteration 43, loss = 0.49660341\n",
      "Iteration 44, loss = 0.39972111\n",
      "Iteration 45, loss = 0.37184829\n",
      "Iteration 46, loss = 0.36260180\n",
      "Iteration 47, loss = 0.42313803\n",
      "Iteration 48, loss = 0.51378331\n",
      "Iteration 49, loss = 0.66901914\n",
      "Iteration 50, loss = 0.49306440\n",
      "Iteration 51, loss = 0.61635882\n",
      "Iteration 52, loss = 0.65563711\n",
      "Iteration 53, loss = 1.17026523\n",
      "Iteration 54, loss = 0.61308793\n",
      "Iteration 55, loss = 0.44655799\n",
      "Iteration 56, loss = 0.38572251\n",
      "Iteration 57, loss = 0.35870021\n",
      "Iteration 58, loss = 0.37537750\n",
      "Iteration 59, loss = 0.34795980\n",
      "Iteration 60, loss = 0.34615840\n",
      "Iteration 61, loss = 0.38676241\n",
      "Iteration 62, loss = 0.62063816\n",
      "Iteration 63, loss = 0.68409440\n",
      "Iteration 64, loss = 0.55820140\n",
      "Iteration 65, loss = 0.42307272\n",
      "Iteration 66, loss = 0.43321670\n",
      "Iteration 67, loss = 0.50423343\n",
      "Iteration 68, loss = 0.38686221\n",
      "Iteration 69, loss = 0.56102020\n",
      "Iteration 70, loss = 0.47360846\n",
      "Iteration 71, loss = 0.41311681\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.33066296\n",
      "Iteration 2, loss = 2.97133621\n",
      "Iteration 3, loss = 1.25733469\n",
      "Iteration 4, loss = 0.93126940\n",
      "Iteration 5, loss = 0.75387195\n",
      "Iteration 6, loss = 0.91084163\n",
      "Iteration 7, loss = 0.93397567\n",
      "Iteration 8, loss = 1.32749165\n",
      "Iteration 9, loss = 1.31844077\n",
      "Iteration 10, loss = 1.04056910\n",
      "Iteration 11, loss = 0.66993103\n",
      "Iteration 12, loss = 0.49921857\n",
      "Iteration 13, loss = 0.57437219\n",
      "Iteration 14, loss = 0.56363619\n",
      "Iteration 15, loss = 0.48785776\n",
      "Iteration 16, loss = 0.46342624\n",
      "Iteration 17, loss = 0.45646152\n",
      "Iteration 18, loss = 0.47784094\n",
      "Iteration 19, loss = 0.62867877\n",
      "Iteration 20, loss = 0.60577844\n",
      "Iteration 21, loss = 0.45253278\n",
      "Iteration 22, loss = 0.58840478\n",
      "Iteration 23, loss = 0.53542271\n",
      "Iteration 24, loss = 0.47567012\n",
      "Iteration 25, loss = 0.81036474\n",
      "Iteration 26, loss = 0.72026755\n",
      "Iteration 27, loss = 1.21677094\n",
      "Iteration 28, loss = 1.90081029\n",
      "Iteration 29, loss = 1.82543362\n",
      "Iteration 30, loss = 0.96062984\n",
      "Iteration 31, loss = 0.58220448\n",
      "Iteration 32, loss = 0.52698375\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.38229472\n",
      "Iteration 2, loss = 4.09816919\n",
      "Iteration 3, loss = 4.43291311\n",
      "Iteration 4, loss = 2.58479487\n",
      "Iteration 5, loss = 2.15900173\n",
      "Iteration 6, loss = 0.74073911\n",
      "Iteration 7, loss = 0.64012076\n",
      "Iteration 8, loss = 0.58315391\n",
      "Iteration 9, loss = 0.56674718\n",
      "Iteration 10, loss = 0.53361188\n",
      "Iteration 11, loss = 0.64467239\n",
      "Iteration 12, loss = 0.71474495\n",
      "Iteration 13, loss = 0.78517767\n",
      "Iteration 14, loss = 0.47821120\n",
      "Iteration 15, loss = 0.47789093\n",
      "Iteration 16, loss = 0.48023688\n",
      "Iteration 17, loss = 0.63408554\n",
      "Iteration 18, loss = 0.53537162\n",
      "Iteration 19, loss = 0.48141172\n",
      "Iteration 20, loss = 0.61600798\n",
      "Iteration 21, loss = 0.66304069\n",
      "Iteration 22, loss = 0.84185548\n",
      "Iteration 23, loss = 0.82706486\n",
      "Iteration 24, loss = 0.53025470\n",
      "Iteration 25, loss = 0.43770545\n",
      "Iteration 26, loss = 0.62720401\n",
      "Iteration 27, loss = 0.94508945\n",
      "Iteration 28, loss = 1.01799673\n",
      "Iteration 29, loss = 0.59264885\n",
      "Iteration 30, loss = 0.76624272\n",
      "Iteration 31, loss = 0.90023408\n",
      "Iteration 32, loss = 0.66884069\n",
      "Iteration 33, loss = 0.69874702\n",
      "Iteration 34, loss = 0.99441545\n",
      "Iteration 35, loss = 0.89363546\n",
      "Iteration 36, loss = 0.68368571\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.48777548\n",
      "Iteration 2, loss = 4.19110310\n",
      "Iteration 3, loss = 2.50359559\n",
      "Iteration 4, loss = 1.00919970\n",
      "Iteration 5, loss = 0.91906063\n",
      "Iteration 6, loss = 1.07469901\n",
      "Iteration 7, loss = 0.89854638\n",
      "Iteration 8, loss = 0.67655574\n",
      "Iteration 9, loss = 1.04123809\n",
      "Iteration 10, loss = 1.00477274\n",
      "Iteration 11, loss = 0.86409271\n",
      "Iteration 12, loss = 1.01360832\n",
      "Iteration 13, loss = 1.08668743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = 0.96778011\n",
      "Iteration 15, loss = 1.51241852\n",
      "Iteration 16, loss = 0.90877844\n",
      "Iteration 17, loss = 1.25304671\n",
      "Iteration 18, loss = 0.98191994\n",
      "Iteration 19, loss = 0.88902004\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.47241021\n",
      "Iteration 2, loss = 4.15753365\n",
      "Iteration 3, loss = 2.50386680\n",
      "Iteration 4, loss = 0.93609480\n",
      "Iteration 5, loss = 1.00199440\n",
      "Iteration 6, loss = 1.25644855\n",
      "Iteration 7, loss = 1.21772864\n",
      "Iteration 8, loss = 1.18726140\n",
      "Iteration 9, loss = 0.75080161\n",
      "Iteration 10, loss = 0.65923182\n",
      "Iteration 11, loss = 0.54740123\n",
      "Iteration 12, loss = 0.52902704\n",
      "Iteration 13, loss = 0.49403684\n",
      "Iteration 14, loss = 0.48624653\n",
      "Iteration 15, loss = 0.51210856\n",
      "Iteration 16, loss = 0.50939133\n",
      "Iteration 17, loss = 0.65198568\n",
      "Iteration 18, loss = 0.53996079\n",
      "Iteration 19, loss = 0.51019631\n",
      "Iteration 20, loss = 0.65077803\n",
      "Iteration 21, loss = 0.52539790\n",
      "Iteration 22, loss = 0.50197745\n",
      "Iteration 23, loss = 0.50931152\n",
      "Iteration 24, loss = 0.58944763\n",
      "Iteration 25, loss = 0.71183134\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.45449595\n",
      "Iteration 2, loss = 4.21953430\n",
      "Iteration 3, loss = 3.48856891\n",
      "Iteration 4, loss = 1.78692157\n",
      "Iteration 5, loss = 1.21939440\n",
      "Iteration 6, loss = 1.01946150\n",
      "Iteration 7, loss = 0.77785820\n",
      "Iteration 8, loss = 0.80867570\n",
      "Iteration 9, loss = 0.63306732\n",
      "Iteration 10, loss = 0.64495411\n",
      "Iteration 11, loss = 0.92209479\n",
      "Iteration 12, loss = 1.44621353\n",
      "Iteration 13, loss = 1.34706962\n",
      "Iteration 14, loss = 0.68063899\n",
      "Iteration 15, loss = 0.70055653\n",
      "Iteration 16, loss = 0.87941503\n",
      "Iteration 17, loss = 0.80610917\n",
      "Iteration 18, loss = 0.57733717\n",
      "Iteration 19, loss = 0.52670920\n",
      "Iteration 20, loss = 0.57262370\n",
      "Iteration 21, loss = 0.66401361\n",
      "Iteration 22, loss = 0.61520087\n",
      "Iteration 23, loss = 0.54687821\n",
      "Iteration 24, loss = 0.46660760\n",
      "Iteration 25, loss = 0.52763808\n",
      "Iteration 26, loss = 0.89291765\n",
      "Iteration 27, loss = 0.61104553\n",
      "Iteration 28, loss = 0.44197223\n",
      "Iteration 29, loss = 0.53955279\n",
      "Iteration 30, loss = 0.51012823\n",
      "Iteration 31, loss = 0.56528851\n",
      "Iteration 32, loss = 0.47006714\n",
      "Iteration 33, loss = 0.45200542\n",
      "Iteration 34, loss = 0.47049451\n",
      "Iteration 35, loss = 0.42953110\n",
      "Iteration 36, loss = 0.38198418\n",
      "Iteration 37, loss = 0.48031600\n",
      "Iteration 38, loss = 0.54997568\n",
      "Iteration 39, loss = 0.45174958\n",
      "Iteration 40, loss = 0.50355501\n",
      "Iteration 41, loss = 0.65226928\n",
      "Iteration 42, loss = 0.62855145\n",
      "Iteration 43, loss = 0.77559092\n",
      "Iteration 44, loss = 1.11713021\n",
      "Iteration 45, loss = 0.75474946\n",
      "Iteration 46, loss = 0.70633940\n",
      "Iteration 47, loss = 0.45177719\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.82580785\n",
      "Iteration 2, loss = 6.21296228\n",
      "Iteration 3, loss = 4.13271640\n",
      "Iteration 4, loss = 1.93769337\n",
      "Iteration 5, loss = 1.55825885\n",
      "Iteration 6, loss = 1.72425391\n",
      "Iteration 7, loss = 1.58794371\n",
      "Iteration 8, loss = 1.87668452\n",
      "Iteration 9, loss = 1.13643730\n",
      "Iteration 10, loss = 0.80664701\n",
      "Iteration 11, loss = 0.67017713\n",
      "Iteration 12, loss = 0.47403946\n",
      "Iteration 13, loss = 0.45376297\n",
      "Iteration 14, loss = 0.41138845\n",
      "Iteration 15, loss = 0.40565890\n",
      "Iteration 16, loss = 0.39827084\n",
      "Iteration 17, loss = 0.38808461\n",
      "Iteration 18, loss = 0.38280883\n",
      "Iteration 19, loss = 0.38432558\n",
      "Iteration 20, loss = 0.37309777\n",
      "Iteration 21, loss = 0.39056115\n",
      "Iteration 22, loss = 0.39565538\n",
      "Iteration 23, loss = 0.36351482\n",
      "Iteration 24, loss = 0.35252227\n",
      "Iteration 25, loss = 0.35462777\n",
      "Iteration 26, loss = 0.34351697\n",
      "Iteration 27, loss = 0.34089663\n",
      "Iteration 28, loss = 0.33717643\n",
      "Iteration 29, loss = 0.33298845\n",
      "Iteration 30, loss = 0.33400735\n",
      "Iteration 31, loss = 0.32717090\n",
      "Iteration 32, loss = 0.32502619\n",
      "Iteration 33, loss = 0.31894822\n",
      "Iteration 34, loss = 0.32649410\n",
      "Iteration 35, loss = 0.32707305\n",
      "Iteration 36, loss = 0.32166136\n",
      "Iteration 37, loss = 0.30562370\n",
      "Iteration 38, loss = 0.33637152\n",
      "Iteration 39, loss = 0.31478909\n",
      "Iteration 40, loss = 0.31011514\n",
      "Iteration 41, loss = 0.29246978\n",
      "Iteration 42, loss = 0.29996395\n",
      "Iteration 43, loss = 0.29040309\n",
      "Iteration 44, loss = 0.28831728\n",
      "Iteration 45, loss = 0.29158782\n",
      "Iteration 46, loss = 0.28437364\n",
      "Iteration 47, loss = 0.28366983\n",
      "Iteration 48, loss = 0.27748221\n",
      "Iteration 49, loss = 0.27365501\n",
      "Iteration 50, loss = 0.26926464\n",
      "Iteration 51, loss = 0.26752378\n",
      "Iteration 52, loss = 0.26792677\n",
      "Iteration 53, loss = 0.26527716\n",
      "Iteration 54, loss = 0.26141723\n",
      "Iteration 55, loss = 0.25731018\n",
      "Iteration 56, loss = 0.26275855\n",
      "Iteration 57, loss = 0.26542949\n",
      "Iteration 58, loss = 0.26375493\n",
      "Iteration 59, loss = 0.25625132\n",
      "Iteration 60, loss = 0.25370529\n",
      "Iteration 61, loss = 0.24645020\n",
      "Iteration 62, loss = 0.24236762\n",
      "Iteration 63, loss = 0.25003948\n",
      "Iteration 64, loss = 0.23839288\n",
      "Iteration 65, loss = 0.24865188\n",
      "Iteration 66, loss = 0.24481073\n",
      "Iteration 67, loss = 0.25597702\n",
      "Iteration 68, loss = 0.25791451\n",
      "Iteration 69, loss = 0.28568624\n",
      "Iteration 70, loss = 0.23351483\n",
      "Iteration 71, loss = 0.23727812\n",
      "Iteration 72, loss = 0.23267791\n",
      "Iteration 73, loss = 0.22364455\n",
      "Iteration 74, loss = 0.22434502\n",
      "Iteration 75, loss = 0.23093499\n",
      "Iteration 76, loss = 0.22087365\n",
      "Iteration 77, loss = 0.21597502\n",
      "Iteration 78, loss = 0.21771435\n",
      "Iteration 79, loss = 0.21334408\n",
      "Iteration 80, loss = 0.21039398\n",
      "Iteration 81, loss = 0.21113619\n",
      "Iteration 82, loss = 0.20927616\n",
      "Iteration 83, loss = 0.20572351\n",
      "Iteration 84, loss = 0.20262217\n",
      "Iteration 85, loss = 0.20319150\n",
      "Iteration 86, loss = 0.20169599\n",
      "Iteration 87, loss = 0.21045966\n",
      "Iteration 88, loss = 0.29017238\n",
      "Iteration 89, loss = 0.26192844\n",
      "Iteration 90, loss = 0.20185247\n",
      "Iteration 91, loss = 0.19459449\n",
      "Iteration 92, loss = 0.19356919\n",
      "Iteration 93, loss = 0.19779529\n",
      "Iteration 94, loss = 0.19467620\n",
      "Iteration 95, loss = 0.19329151\n",
      "Iteration 96, loss = 0.18791808\n",
      "Iteration 97, loss = 0.18267006\n",
      "Iteration 98, loss = 0.19510473\n",
      "Iteration 99, loss = 0.18552089\n",
      "Iteration 100, loss = 0.17839635\n",
      "Iteration 101, loss = 0.19938276\n",
      "Iteration 102, loss = 0.20074565\n",
      "Iteration 103, loss = 0.18059401\n",
      "Iteration 104, loss = 0.17681126\n",
      "Iteration 105, loss = 0.17398909\n",
      "Iteration 106, loss = 0.17492616\n",
      "Iteration 107, loss = 0.18781011\n",
      "Iteration 108, loss = 0.17537586\n",
      "Iteration 109, loss = 0.17207968\n",
      "Iteration 110, loss = 0.18288176\n",
      "Iteration 111, loss = 0.17658942\n",
      "Iteration 112, loss = 0.21070930\n",
      "Iteration 113, loss = 0.19129341\n",
      "Iteration 114, loss = 0.17328719\n",
      "Iteration 115, loss = 0.17694810\n",
      "Iteration 116, loss = 0.20080905\n",
      "Iteration 117, loss = 0.18885889\n",
      "Iteration 118, loss = 0.18339477\n",
      "Iteration 119, loss = 0.18016255\n",
      "Iteration 120, loss = 0.17894657\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy =  [0.65467626 0.89208633 0.76978417 0.82733813 0.76811594 0.50724638\n",
      " 0.73913043 0.5        0.84057971 0.54347826]\n",
      "Accuracy mean and std : 0.704 (0.137)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.55      0.69       692\n",
      "           1       0.68      0.94      0.79       692\n",
      "\n",
      "    accuracy                           0.75      1384\n",
      "   macro avg       0.79      0.75      0.74      1384\n",
      "weighted avg       0.79      0.75      0.74      1384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KFold cross validation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=10)\n",
    "class_names = ['0','1']\n",
    "y_pred = cross_val_predict(modelMLP3, X, Y, cv = cv)\n",
    "\n",
    "cv_score_for_LR = cross_val_score(modelMLP3, X, Y, cv = 10)\n",
    "print(\"Accuracy = \",cv_score_for_LR)\n",
    "print('Accuracy mean and std : %.3f (%.3f)' % (mean(cv_score_for_LR), std(cv_score_for_LR)))\n",
    "\n",
    "print(classification_report(Y, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf823f7f",
   "metadata": {},
   "source": [
    "## SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cc18838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Counter({1: 5465, 0: 692})\n",
      "After Counter({1: 5465, 0: 692})\n",
      "0.952922077922078\n",
      "0.952922077922078\n",
      "[[ 102   44]\n",
      " [  14 1072]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.70      0.78       146\n",
      "           1       0.96      0.99      0.97      1086\n",
      "\n",
      "    accuracy                           0.95      1232\n",
      "   macro avg       0.92      0.84      0.88      1232\n",
      "weighted avg       0.95      0.95      0.95      1232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "X = data.drop(['TYPE OF BIRTH    '], axis = 1)\n",
    "'''X = data[['AGE', 'WEIGHT', 'BMI', 'KG INCREASED PREGNANCY', 'PREVIOUS TERM PREGNANCIES','PARITY', 'GESTAGIONAL AGE ', 'COUPLE SITUATION ', 'ART', 'AMNIOCENTESIS', 'PREVIOUS CESAREAN',\n",
    "       'COMORBIDITY', 'PREINDUCTION', 'INDUCTION',\n",
    "       'ANESTHESIA ', 'EPISIOTOMY', 'Fetal INTRAPARTUM pH', 'COMPLICATIONS',\n",
    "        'LIQUID_ Hemorr�gico  ',\n",
    "       'LIQUID_ clear        ', 'LIQUID_ stained +    ',\n",
    "       'LIQUID_ stained ++   ', 'LIQUID_ stained +++  ', 'GROUP_ group 1     ',\n",
    "       'GROUP_ group 10    ', 'GROUP_ group 2a    ', 'GROUP_ group 2b    ',\n",
    "       'GROUP_ group 3     ', 'GROUP_ group 4a    ', 'GROUP_ group 4b    ',\n",
    "       'GROUP_ group 5     ', 'GROUP_ group 6     ', 'GROUP_ group 7     ',\n",
    "       'GROUP_ group 8     ', 'GROUP_ group 9     ']]'''\n",
    "\n",
    "Y = data['TYPE OF BIRTH    ']\n",
    "\n",
    "#SMOTETomek \n",
    "from collections import Counter\n",
    "from imblearn.combine import SMOTETomek\n",
    "counter = Counter(Y)\n",
    "print('Before', counter)\n",
    "smtt = SMOTETomek(random_state = 139)\n",
    "x_train_smt, y_train_smt = smtt.fit_resample(X,Y)\n",
    "counter = Counter(Y)\n",
    "print('After', counter)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2,random_state = 109) # 80% training and 20% test\n",
    "\n",
    "\n",
    "# Simple Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "modelLR4 = LogisticRegression()\n",
    "modelLR4.fit(x_train, y_train)\n",
    "print(modelLR4.score(x_test, y_test))\n",
    "\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = modelLR4.predict(x_test)\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "print(ac)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = modelLR4.predict(x_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c50e6c5",
   "metadata": {},
   "source": [
    "### K Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c2b94fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  [0.9788961  0.97727273 0.86688312 0.92857143 0.94155844 0.91071429\n",
      " 0.96428571 0.99512195 0.9398374  0.89918699]\n",
      "Accuracy mean and std : 0.940 (0.038)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.07      0.11       692\n",
      "           1       0.89      0.97      0.93      5465\n",
      "\n",
      "    accuracy                           0.87      6157\n",
      "   macro avg       0.56      0.52      0.52      6157\n",
      "weighted avg       0.82      0.87      0.84      6157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KFold cross validation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cv = KFold(n_splits=10)\n",
    "class_names = ['0','1']\n",
    "y_pred = cross_val_predict(modelLR4, X, Y, cv = cv)\n",
    "\n",
    "cv_score_for_LR = cross_val_score(modelLR4, X, Y, cv = 10)\n",
    "print(\"Accuracy = \",cv_score_for_LR)\n",
    "print('Accuracy mean and std : %.3f (%.3f)' % (mean(cv_score_for_LR), std(cv_score_for_LR)))\n",
    "\n",
    "print(classification_report(Y, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c1bb60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.68      0.72       692\n",
      "           1       0.96      0.97      0.97      5465\n",
      "\n",
      "    accuracy                           0.94      6157\n",
      "   macro avg       0.86      0.83      0.84      6157\n",
      "weighted avg       0.94      0.94      0.94      6157\n",
      "\n",
      "Accuracy =  [0.9788961  0.97727273 0.86688312 0.92857143 0.94155844 0.91071429\n",
      " 0.96428571 0.99512195 0.9398374  0.89918699]\n",
      "Accuracy mean and std : 0.940 (0.038)\n"
     ]
    }
   ],
   "source": [
    "# KFold cross validation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv=StratifiedKFold(n_splits = 10)\n",
    "\n",
    "class_names = ['0','1']\n",
    "y_pred = cross_val_predict(modelLR4, X, Y, cv = cv)\n",
    "\n",
    "print(classification_report(Y, y_pred, target_names=class_names))\n",
    "\n",
    "cv_score_for_LR = cross_val_score(modelLR4, X, Y, cv = 10)\n",
    "print(\"Accuracy = \",cv_score_for_LR)\n",
    "\n",
    "print('Accuracy mean and std : %.3f (%.3f)' % (mean(cv_score_for_LR), std(cv_score_for_LR)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7241a1a7",
   "metadata": {},
   "source": [
    "## MLP SmoteTomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "229e5c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.40959358\n",
      "Iteration 2, loss = 1.35780833\n",
      "Iteration 3, loss = 1.02060212\n",
      "Iteration 4, loss = 0.99470781\n",
      "Iteration 5, loss = 0.67052850\n",
      "Iteration 6, loss = 0.28413283\n",
      "Iteration 7, loss = 0.25098300\n",
      "Iteration 8, loss = 0.23196078\n",
      "Iteration 9, loss = 0.23119444\n",
      "Iteration 10, loss = 0.19373463\n",
      "Iteration 11, loss = 0.19443372\n",
      "Iteration 12, loss = 0.23297512\n",
      "Iteration 13, loss = 0.23082414\n",
      "Iteration 14, loss = 0.17249733\n",
      "Iteration 15, loss = 0.17130275\n",
      "Iteration 16, loss = 0.16893782\n",
      "Iteration 17, loss = 0.23818021\n",
      "Iteration 18, loss = 0.88564590\n",
      "Iteration 19, loss = 1.67820464\n",
      "Iteration 20, loss = 0.66849490\n",
      "Iteration 21, loss = 0.30274106\n",
      "Iteration 22, loss = 0.17406521\n",
      "Iteration 23, loss = 0.14967463\n",
      "Iteration 24, loss = 0.14829886\n",
      "Iteration 25, loss = 0.14779793\n",
      "Iteration 26, loss = 0.14004106\n",
      "Iteration 27, loss = 0.13761784\n",
      "Iteration 28, loss = 0.13833029\n",
      "Iteration 29, loss = 0.15217748\n",
      "Iteration 30, loss = 0.13719402\n",
      "Iteration 31, loss = 0.14623098\n",
      "Iteration 32, loss = 0.14596588\n",
      "Iteration 33, loss = 0.13395138\n",
      "Iteration 34, loss = 0.13837634\n",
      "Iteration 35, loss = 0.13522742\n",
      "Iteration 36, loss = 0.13597377\n",
      "Iteration 37, loss = 0.13741753\n",
      "Iteration 38, loss = 0.14599434\n",
      "Iteration 39, loss = 0.14126049\n",
      "Iteration 40, loss = 0.16940542\n",
      "Iteration 41, loss = 0.14598710\n",
      "Iteration 42, loss = 0.13805868\n",
      "Iteration 43, loss = 0.14603038\n",
      "Iteration 44, loss = 0.13724010\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.9683441558441559\n",
      "0.9683441558441559\n",
      "[[ 113   33]\n",
      " [   6 1080]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.77      0.85       146\n",
      "           1       0.97      0.99      0.98      1086\n",
      "\n",
      "    accuracy                           0.97      1232\n",
      "   macro avg       0.96      0.88      0.92      1232\n",
      "weighted avg       0.97      0.97      0.97      1232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "modelMLP4 = MLPClassifier(hidden_layer_sizes = (6,5), random_state = 42, verbose = True, learning_rate_init = 0.01)\n",
    "modelMLP4.fit(x_train, y_train)\n",
    "print(modelMLP4.score(x_test, y_test))\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = modelMLP4.predict(x_test)\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "print(ac)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = modelMLP4.predict(x_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c01f7a7",
   "metadata": {},
   "source": [
    "### K Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "205def13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.90656259\n",
      "Iteration 2, loss = 1.02076838\n",
      "Iteration 3, loss = 1.43929510\n",
      "Iteration 4, loss = 0.83281415\n",
      "Iteration 5, loss = 0.31650629\n",
      "Iteration 6, loss = 0.26642994\n",
      "Iteration 7, loss = 0.24508205\n",
      "Iteration 8, loss = 0.22467824\n",
      "Iteration 9, loss = 0.22597598\n",
      "Iteration 10, loss = 0.21198779\n",
      "Iteration 11, loss = 0.20591964\n",
      "Iteration 12, loss = 0.23093187\n",
      "Iteration 13, loss = 0.37595673\n",
      "Iteration 14, loss = 0.21360800\n",
      "Iteration 15, loss = 0.21333905\n",
      "Iteration 16, loss = 0.21753323\n",
      "Iteration 17, loss = 0.21598138\n",
      "Iteration 18, loss = 1.10226057\n",
      "Iteration 19, loss = 1.69964904\n",
      "Iteration 20, loss = 1.16170776\n",
      "Iteration 21, loss = 0.30137065\n",
      "Iteration 22, loss = 0.23334424\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.20301987\n",
      "Iteration 2, loss = 0.77715642\n",
      "Iteration 3, loss = 0.89559527\n",
      "Iteration 4, loss = 0.46424826\n",
      "Iteration 5, loss = 0.25669984\n",
      "Iteration 6, loss = 0.26042556\n",
      "Iteration 7, loss = 0.25052831\n",
      "Iteration 8, loss = 0.22104173\n",
      "Iteration 9, loss = 0.20735513\n",
      "Iteration 10, loss = 0.21789516\n",
      "Iteration 11, loss = 0.20229595\n",
      "Iteration 12, loss = 0.19626547\n",
      "Iteration 13, loss = 0.18858951\n",
      "Iteration 14, loss = 0.18030128\n",
      "Iteration 15, loss = 1.74026862\n",
      "Iteration 16, loss = 1.45481619\n",
      "Iteration 17, loss = 0.17550276\n",
      "Iteration 18, loss = 0.19579852\n",
      "Iteration 19, loss = 0.15960252\n",
      "Iteration 20, loss = 0.17657793\n",
      "Iteration 21, loss = 0.15204591\n",
      "Iteration 22, loss = 0.16038658\n",
      "Iteration 23, loss = 0.15115397\n",
      "Iteration 24, loss = 0.15922901\n",
      "Iteration 25, loss = 0.16450329\n",
      "Iteration 26, loss = 0.15377717\n",
      "Iteration 27, loss = 0.14958792\n",
      "Iteration 28, loss = 0.16065957\n",
      "Iteration 29, loss = 0.14955002\n",
      "Iteration 30, loss = 0.17257376\n",
      "Iteration 31, loss = 0.15643585\n",
      "Iteration 32, loss = 0.17079995\n",
      "Iteration 33, loss = 0.16983163\n",
      "Iteration 34, loss = 0.14741920\n",
      "Iteration 35, loss = 0.15169303\n",
      "Iteration 36, loss = 0.15356606\n",
      "Iteration 37, loss = 0.15836442\n",
      "Iteration 38, loss = 0.17650378\n",
      "Iteration 39, loss = 0.19035104\n",
      "Iteration 40, loss = 0.15412915\n",
      "Iteration 41, loss = 0.15276555\n",
      "Iteration 42, loss = 0.14353135\n",
      "Iteration 43, loss = 0.14869720\n",
      "Iteration 44, loss = 0.16028581\n",
      "Iteration 45, loss = 0.15607088\n",
      "Iteration 46, loss = 0.15968320\n",
      "Iteration 47, loss = 0.14726736\n",
      "Iteration 48, loss = 0.15032685\n",
      "Iteration 49, loss = 0.15518177\n",
      "Iteration 50, loss = 0.16394892\n",
      "Iteration 51, loss = 0.16407701\n",
      "Iteration 52, loss = 0.16460914\n",
      "Iteration 53, loss = 0.21363563\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.42411087\n",
      "Iteration 2, loss = 0.54049794\n",
      "Iteration 3, loss = 0.41248312\n",
      "Iteration 4, loss = 0.17090316\n",
      "Iteration 5, loss = 0.12380509\n",
      "Iteration 6, loss = 0.26623019\n",
      "Iteration 7, loss = 0.67214298\n",
      "Iteration 8, loss = 0.50126975\n",
      "Iteration 9, loss = 0.12726833\n",
      "Iteration 10, loss = 0.11390706\n",
      "Iteration 11, loss = 0.11122898\n",
      "Iteration 12, loss = 0.10813284\n",
      "Iteration 13, loss = 0.10899763\n",
      "Iteration 14, loss = 0.10730587\n",
      "Iteration 15, loss = 0.10436009\n",
      "Iteration 16, loss = 0.10025817\n",
      "Iteration 17, loss = 0.09612524\n",
      "Iteration 18, loss = 0.09092674\n",
      "Iteration 19, loss = 0.08908706\n",
      "Iteration 20, loss = 0.09262559\n",
      "Iteration 21, loss = 0.09146427\n",
      "Iteration 22, loss = 0.08418251\n",
      "Iteration 23, loss = 0.08390205\n",
      "Iteration 24, loss = 0.08142849\n",
      "Iteration 25, loss = 0.08323250\n",
      "Iteration 26, loss = 0.07980388\n",
      "Iteration 27, loss = 0.07681579\n",
      "Iteration 28, loss = 0.07905134\n",
      "Iteration 29, loss = 0.07865047\n",
      "Iteration 30, loss = 0.07667871\n",
      "Iteration 31, loss = 0.07279053\n",
      "Iteration 32, loss = 0.07772773\n",
      "Iteration 33, loss = 0.06978014\n",
      "Iteration 34, loss = 0.07159491\n",
      "Iteration 35, loss = 0.07480459\n",
      "Iteration 36, loss = 0.07733489\n",
      "Iteration 37, loss = 0.06847809\n",
      "Iteration 38, loss = 0.07634206\n",
      "Iteration 39, loss = 0.07169769\n",
      "Iteration 40, loss = 0.06620470\n",
      "Iteration 41, loss = 0.07339534\n",
      "Iteration 42, loss = 0.07552007\n",
      "Iteration 43, loss = 0.06837669\n",
      "Iteration 44, loss = 0.07223346\n",
      "Iteration 45, loss = 0.07277982\n",
      "Iteration 46, loss = 0.07199282\n",
      "Iteration 47, loss = 0.07655733\n",
      "Iteration 48, loss = 0.07506661\n",
      "Iteration 49, loss = 0.07322875\n",
      "Iteration 50, loss = 0.07293245\n",
      "Iteration 51, loss = 0.06545965\n",
      "Iteration 52, loss = 0.07060848\n",
      "Iteration 53, loss = 0.06992000\n",
      "Iteration 54, loss = 0.07653278\n",
      "Iteration 55, loss = 0.08085260\n",
      "Iteration 56, loss = 0.07497045\n",
      "Iteration 57, loss = 0.06709267\n",
      "Iteration 58, loss = 0.06872031\n",
      "Iteration 59, loss = 0.06738652\n",
      "Iteration 60, loss = 0.06828245\n",
      "Iteration 61, loss = 0.06859010\n",
      "Iteration 62, loss = 0.06738126\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.90266886\n",
      "Iteration 2, loss = 1.05831359\n",
      "Iteration 3, loss = 0.87256105\n",
      "Iteration 4, loss = 0.95775322\n",
      "Iteration 5, loss = 0.77791440\n",
      "Iteration 6, loss = 0.65188882\n",
      "Iteration 7, loss = 0.29920854\n",
      "Iteration 8, loss = 0.19290376\n",
      "Iteration 9, loss = 0.15878047\n",
      "Iteration 10, loss = 0.16245235\n",
      "Iteration 11, loss = 0.15320043\n",
      "Iteration 12, loss = 0.17579754\n",
      "Iteration 13, loss = 0.18947885\n",
      "Iteration 14, loss = 0.17479332\n",
      "Iteration 15, loss = 0.14026616\n",
      "Iteration 16, loss = 0.18443674\n",
      "Iteration 17, loss = 0.13721042\n",
      "Iteration 18, loss = 0.12505619\n",
      "Iteration 19, loss = 0.13556414\n",
      "Iteration 20, loss = 1.40373972\n",
      "Iteration 21, loss = 1.65797743\n",
      "Iteration 22, loss = 0.77304892\n",
      "Iteration 23, loss = 0.25177722\n",
      "Iteration 24, loss = 0.21320917\n",
      "Iteration 25, loss = 0.13357103\n",
      "Iteration 26, loss = 0.12061558\n",
      "Iteration 27, loss = 0.12144401\n",
      "Iteration 28, loss = 0.12285172\n",
      "Iteration 29, loss = 0.13045591\n",
      "Iteration 30, loss = 0.14833722\n",
      "Iteration 31, loss = 0.12223230\n",
      "Iteration 32, loss = 0.12155793\n",
      "Iteration 33, loss = 0.11673001\n",
      "Iteration 34, loss = 0.11977475\n",
      "Iteration 35, loss = 0.12041901\n",
      "Iteration 36, loss = 0.14615239\n",
      "Iteration 37, loss = 0.12251392\n",
      "Iteration 38, loss = 0.12332856\n",
      "Iteration 39, loss = 0.11824859\n",
      "Iteration 40, loss = 0.12034287\n",
      "Iteration 41, loss = 0.11405418\n",
      "Iteration 42, loss = 0.11565971\n",
      "Iteration 43, loss = 0.12311806\n",
      "Iteration 44, loss = 0.13618668\n",
      "Iteration 45, loss = 0.13435009\n",
      "Iteration 46, loss = 0.12752429\n",
      "Iteration 47, loss = 0.12645059\n",
      "Iteration 48, loss = 0.12850820\n",
      "Iteration 49, loss = 0.14530180\n",
      "Iteration 50, loss = 0.20779435\n",
      "Iteration 51, loss = 0.14213918\n",
      "Iteration 52, loss = 0.12565818\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.99174675\n",
      "Iteration 2, loss = 1.29554128\n",
      "Iteration 3, loss = 1.25409986\n",
      "Iteration 4, loss = 0.67328753\n",
      "Iteration 5, loss = 0.34316069\n",
      "Iteration 6, loss = 0.26803906\n",
      "Iteration 7, loss = 0.26994739\n",
      "Iteration 8, loss = 0.75519708\n",
      "Iteration 9, loss = 0.71554205\n",
      "Iteration 10, loss = 0.23287871\n",
      "Iteration 11, loss = 0.17760354\n",
      "Iteration 12, loss = 0.20721054\n",
      "Iteration 13, loss = 0.24754928\n",
      "Iteration 14, loss = 0.24728921\n",
      "Iteration 15, loss = 0.44635249\n",
      "Iteration 16, loss = 1.12911992\n",
      "Iteration 17, loss = 1.05987381\n",
      "Iteration 18, loss = 1.18189308\n",
      "Iteration 19, loss = 0.49968013\n",
      "Iteration 20, loss = 0.17069590\n",
      "Iteration 21, loss = 0.15600040\n",
      "Iteration 22, loss = 0.15278034\n",
      "Iteration 23, loss = 0.14598510\n",
      "Iteration 24, loss = 0.14712717\n",
      "Iteration 25, loss = 0.14397626\n",
      "Iteration 26, loss = 0.14117000\n",
      "Iteration 27, loss = 0.14498774\n",
      "Iteration 28, loss = 0.14540136\n",
      "Iteration 29, loss = 0.15081378\n",
      "Iteration 30, loss = 0.17911047\n",
      "Iteration 31, loss = 0.15372061\n",
      "Iteration 32, loss = 0.14578146\n",
      "Iteration 33, loss = 0.14381583\n",
      "Iteration 34, loss = 0.15573723\n",
      "Iteration 35, loss = 0.14842868\n",
      "Iteration 36, loss = 0.18130170\n",
      "Iteration 37, loss = 0.14791912\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.29034931\n",
      "Iteration 2, loss = 1.42440110\n",
      "Iteration 3, loss = 1.53019543\n",
      "Iteration 4, loss = 1.03491550\n",
      "Iteration 5, loss = 0.32384670\n",
      "Iteration 6, loss = 0.22131090\n",
      "Iteration 7, loss = 0.24631613\n",
      "Iteration 8, loss = 0.23565502\n",
      "Iteration 9, loss = 0.21926364\n",
      "Iteration 10, loss = 0.24048851\n",
      "Iteration 11, loss = 0.23908439\n",
      "Iteration 12, loss = 0.23428463\n",
      "Iteration 13, loss = 0.23031984\n",
      "Iteration 14, loss = 0.25981899\n",
      "Iteration 15, loss = 1.42153381\n",
      "Iteration 16, loss = 1.38386088\n",
      "Iteration 17, loss = 0.71915275\n",
      "Iteration 18, loss = 0.18459761\n",
      "Iteration 19, loss = 0.16051481\n",
      "Iteration 20, loss = 0.15453195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 0.15779671\n",
      "Iteration 22, loss = 0.15772723\n",
      "Iteration 23, loss = 0.15371868\n",
      "Iteration 24, loss = 0.15977740\n",
      "Iteration 25, loss = 0.15768189\n",
      "Iteration 26, loss = 0.14964561\n",
      "Iteration 27, loss = 0.15091753\n",
      "Iteration 28, loss = 0.15759440\n",
      "Iteration 29, loss = 0.15986342\n",
      "Iteration 30, loss = 0.25084227\n",
      "Iteration 31, loss = 0.17505109\n",
      "Iteration 32, loss = 0.16232823\n",
      "Iteration 33, loss = 0.16214018\n",
      "Iteration 34, loss = 0.16006207\n",
      "Iteration 35, loss = 0.15691366\n",
      "Iteration 36, loss = 0.17116022\n",
      "Iteration 37, loss = 0.14624053\n",
      "Iteration 38, loss = 0.15793672\n",
      "Iteration 39, loss = 0.15751643\n",
      "Iteration 40, loss = 0.14345210\n",
      "Iteration 41, loss = 0.15256590\n",
      "Iteration 42, loss = 0.15458248\n",
      "Iteration 43, loss = 0.15110812\n",
      "Iteration 44, loss = 0.15910525\n",
      "Iteration 45, loss = 0.20196957\n",
      "Iteration 46, loss = 0.36085633\n",
      "Iteration 47, loss = 0.68011620\n",
      "Iteration 48, loss = 0.36099128\n",
      "Iteration 49, loss = 0.20693591\n",
      "Iteration 50, loss = 0.24101213\n",
      "Iteration 51, loss = 0.17950799\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.60392687\n",
      "Iteration 2, loss = 1.02744824\n",
      "Iteration 3, loss = 1.12804444\n",
      "Iteration 4, loss = 0.79031658\n",
      "Iteration 5, loss = 0.34554783\n",
      "Iteration 6, loss = 0.31436708\n",
      "Iteration 7, loss = 0.25668793\n",
      "Iteration 8, loss = 0.26932372\n",
      "Iteration 9, loss = 0.24439801\n",
      "Iteration 10, loss = 0.23355911\n",
      "Iteration 11, loss = 0.26594426\n",
      "Iteration 12, loss = 0.38890451\n",
      "Iteration 13, loss = 1.23934408\n",
      "Iteration 14, loss = 0.55381291\n",
      "Iteration 15, loss = 0.17077644\n",
      "Iteration 16, loss = 0.21726038\n",
      "Iteration 17, loss = 0.18679927\n",
      "Iteration 18, loss = 0.18910820\n",
      "Iteration 19, loss = 0.16292873\n",
      "Iteration 20, loss = 0.15650968\n",
      "Iteration 21, loss = 0.17335010\n",
      "Iteration 22, loss = 0.30023698\n",
      "Iteration 23, loss = 0.71427215\n",
      "Iteration 24, loss = 0.19372625\n",
      "Iteration 25, loss = 0.15382007\n",
      "Iteration 26, loss = 0.14894314\n",
      "Iteration 27, loss = 0.15571894\n",
      "Iteration 28, loss = 0.15833007\n",
      "Iteration 29, loss = 0.15916672\n",
      "Iteration 30, loss = 0.17967707\n",
      "Iteration 31, loss = 0.15442577\n",
      "Iteration 32, loss = 0.15899451\n",
      "Iteration 33, loss = 0.16145295\n",
      "Iteration 34, loss = 0.17640314\n",
      "Iteration 35, loss = 0.16161446\n",
      "Iteration 36, loss = 0.16474757\n",
      "Iteration 37, loss = 0.14802229\n",
      "Iteration 38, loss = 0.16522314\n",
      "Iteration 39, loss = 0.17798752\n",
      "Iteration 40, loss = 0.14524440\n",
      "Iteration 41, loss = 0.15070311\n",
      "Iteration 42, loss = 0.15573355\n",
      "Iteration 43, loss = 0.14761068\n",
      "Iteration 44, loss = 0.15801114\n",
      "Iteration 45, loss = 0.19229371\n",
      "Iteration 46, loss = 0.15758794\n",
      "Iteration 47, loss = 0.15213637\n",
      "Iteration 48, loss = 0.14330583\n",
      "Iteration 49, loss = 0.15017748\n",
      "Iteration 50, loss = 0.18790135\n",
      "Iteration 51, loss = 0.26866661\n",
      "Iteration 52, loss = 0.31155809\n",
      "Iteration 53, loss = 0.44177672\n",
      "Iteration 54, loss = 1.66320620\n",
      "Iteration 55, loss = 0.43010105\n",
      "Iteration 56, loss = 0.25127316\n",
      "Iteration 57, loss = 0.22644642\n",
      "Iteration 58, loss = 0.22732647\n",
      "Iteration 59, loss = 0.21433817\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.77570262\n",
      "Iteration 2, loss = 1.30975085\n",
      "Iteration 3, loss = 1.13432523\n",
      "Iteration 4, loss = 1.07251902\n",
      "Iteration 5, loss = 0.45764306\n",
      "Iteration 6, loss = 0.29954224\n",
      "Iteration 7, loss = 0.31784215\n",
      "Iteration 8, loss = 1.10736707\n",
      "Iteration 9, loss = 0.40341478\n",
      "Iteration 10, loss = 0.20638561\n",
      "Iteration 11, loss = 0.22813869\n",
      "Iteration 12, loss = 0.18709082\n",
      "Iteration 13, loss = 0.17685380\n",
      "Iteration 14, loss = 0.17565080\n",
      "Iteration 15, loss = 0.17023655\n",
      "Iteration 16, loss = 0.16581133\n",
      "Iteration 17, loss = 0.19334454\n",
      "Iteration 18, loss = 0.17233854\n",
      "Iteration 19, loss = 0.17734082\n",
      "Iteration 20, loss = 0.24806264\n",
      "Iteration 21, loss = 0.51589003\n",
      "Iteration 22, loss = 0.27472245\n",
      "Iteration 23, loss = 0.28645786\n",
      "Iteration 24, loss = 0.17574290\n",
      "Iteration 25, loss = 0.16001072\n",
      "Iteration 26, loss = 0.16257526\n",
      "Iteration 27, loss = 0.27041015\n",
      "Iteration 28, loss = 0.15553709\n",
      "Iteration 29, loss = 0.15983607\n",
      "Iteration 30, loss = 0.37059704\n",
      "Iteration 31, loss = 0.57194297\n",
      "Iteration 32, loss = 0.17367236\n",
      "Iteration 33, loss = 0.16706565\n",
      "Iteration 34, loss = 0.16812904\n",
      "Iteration 35, loss = 0.15895032\n",
      "Iteration 36, loss = 0.18323110\n",
      "Iteration 37, loss = 0.15924048\n",
      "Iteration 38, loss = 0.16696870\n",
      "Iteration 39, loss = 0.15106980\n",
      "Iteration 40, loss = 0.15107090\n",
      "Iteration 41, loss = 0.17942062\n",
      "Iteration 42, loss = 0.17427294\n",
      "Iteration 43, loss = 0.14586142\n",
      "Iteration 44, loss = 0.16879263\n",
      "Iteration 45, loss = 0.15390282\n",
      "Iteration 46, loss = 0.15092117\n",
      "Iteration 47, loss = 0.16330717\n",
      "Iteration 48, loss = 0.16295605\n",
      "Iteration 49, loss = 0.20025691\n",
      "Iteration 50, loss = 0.18483547\n",
      "Iteration 51, loss = 0.15892154\n",
      "Iteration 52, loss = 0.21516850\n",
      "Iteration 53, loss = 0.15512470\n",
      "Iteration 54, loss = 0.17094064\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.82121445\n",
      "Iteration 2, loss = 0.86608245\n",
      "Iteration 3, loss = 1.31239660\n",
      "Iteration 4, loss = 0.69191012\n",
      "Iteration 5, loss = 0.33778896\n",
      "Iteration 6, loss = 0.26666945\n",
      "Iteration 7, loss = 0.26908290\n",
      "Iteration 8, loss = 1.04588591\n",
      "Iteration 9, loss = 0.65827902\n",
      "Iteration 10, loss = 0.23105840\n",
      "Iteration 11, loss = 0.21515688\n",
      "Iteration 12, loss = 0.26821326\n",
      "Iteration 13, loss = 0.24161747\n",
      "Iteration 14, loss = 0.26381783\n",
      "Iteration 15, loss = 0.31278674\n",
      "Iteration 16, loss = 0.54494142\n",
      "Iteration 17, loss = 0.42183089\n",
      "Iteration 18, loss = 0.26196350\n",
      "Iteration 19, loss = 0.26244491\n",
      "Iteration 20, loss = 0.33733535\n",
      "Iteration 21, loss = 0.26532375\n",
      "Iteration 22, loss = 0.25172054\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.98105991\n",
      "Iteration 2, loss = 0.86786862\n",
      "Iteration 3, loss = 0.28056426\n",
      "Iteration 4, loss = 0.24005921\n",
      "Iteration 5, loss = 0.22394387\n",
      "Iteration 6, loss = 0.21241291\n",
      "Iteration 7, loss = 0.17871804\n",
      "Iteration 8, loss = 0.18182786\n",
      "Iteration 9, loss = 0.17019041\n",
      "Iteration 10, loss = 0.85763286\n",
      "Iteration 11, loss = 2.39870837\n",
      "Iteration 12, loss = 1.26003785\n",
      "Iteration 13, loss = 0.34987391\n",
      "Iteration 14, loss = 0.22802496\n",
      "Iteration 15, loss = 0.12157160\n",
      "Iteration 16, loss = 0.10155166\n",
      "Iteration 17, loss = 0.09269287\n",
      "Iteration 18, loss = 0.08630905\n",
      "Iteration 19, loss = 0.08511114\n",
      "Iteration 20, loss = 0.08568846\n",
      "Iteration 21, loss = 0.07632717\n",
      "Iteration 22, loss = 0.07328988\n",
      "Iteration 23, loss = 0.07329816\n",
      "Iteration 24, loss = 0.06928064\n",
      "Iteration 25, loss = 0.07219610\n",
      "Iteration 26, loss = 0.06576622\n",
      "Iteration 27, loss = 0.06130296\n",
      "Iteration 28, loss = 0.05866996\n",
      "Iteration 29, loss = 0.05751755\n",
      "Iteration 30, loss = 0.05934340\n",
      "Iteration 31, loss = 0.05704676\n",
      "Iteration 32, loss = 0.05549826\n",
      "Iteration 33, loss = 0.05144668\n",
      "Iteration 34, loss = 0.05290732\n",
      "Iteration 35, loss = 0.10124408\n",
      "Iteration 36, loss = 0.16519176\n",
      "Iteration 37, loss = 0.17851776\n",
      "Iteration 38, loss = 0.10610482\n",
      "Iteration 39, loss = 0.05716003\n",
      "Iteration 40, loss = 0.05932915\n",
      "Iteration 41, loss = 0.05573793\n",
      "Iteration 42, loss = 0.05610177\n",
      "Iteration 43, loss = 0.04811445\n",
      "Iteration 44, loss = 0.04754250\n",
      "Iteration 45, loss = 0.05003163\n",
      "Iteration 46, loss = 0.04681152\n",
      "Iteration 47, loss = 0.04251077\n",
      "Iteration 48, loss = 0.04461051\n",
      "Iteration 49, loss = 0.04084227\n",
      "Iteration 50, loss = 0.04247445\n",
      "Iteration 51, loss = 0.03888497\n",
      "Iteration 52, loss = 0.03750401\n",
      "Iteration 53, loss = 0.03866590\n",
      "Iteration 54, loss = 0.03677832\n",
      "Iteration 55, loss = 0.03603897\n",
      "Iteration 56, loss = 0.03396511\n",
      "Iteration 57, loss = 0.03586322\n",
      "Iteration 58, loss = 0.04315168\n",
      "Iteration 59, loss = 0.03858356\n",
      "Iteration 60, loss = 0.03328207\n",
      "Iteration 61, loss = 0.03007994\n",
      "Iteration 62, loss = 0.03609292\n",
      "Iteration 63, loss = 0.03132384\n",
      "Iteration 64, loss = 0.03302248\n",
      "Iteration 65, loss = 0.03209931\n",
      "Iteration 66, loss = 0.03045319\n",
      "Iteration 67, loss = 0.02941992\n",
      "Iteration 68, loss = 0.02689521\n",
      "Iteration 69, loss = 0.03326584\n",
      "Iteration 70, loss = 0.02701328\n",
      "Iteration 71, loss = 0.02567988\n",
      "Iteration 72, loss = 0.02568928\n",
      "Iteration 73, loss = 0.02390872\n",
      "Iteration 74, loss = 0.03237809\n",
      "Iteration 75, loss = 0.02474469\n",
      "Iteration 76, loss = 0.02376757\n",
      "Iteration 77, loss = 0.02377496\n",
      "Iteration 78, loss = 0.02111548\n",
      "Iteration 79, loss = 0.02606245\n",
      "Iteration 80, loss = 0.02096623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 81, loss = 0.01995337\n",
      "Iteration 82, loss = 0.02158603\n",
      "Iteration 83, loss = 0.02311201\n",
      "Iteration 84, loss = 0.02836062\n",
      "Iteration 85, loss = 0.02682576\n",
      "Iteration 86, loss = 0.02786009\n",
      "Iteration 87, loss = 0.03207882\n",
      "Iteration 88, loss = 0.02675783\n",
      "Iteration 89, loss = 0.02128218\n",
      "Iteration 90, loss = 0.01733497\n",
      "Iteration 91, loss = 0.01921287\n",
      "Iteration 92, loss = 0.01788086\n",
      "Iteration 93, loss = 0.01930923\n",
      "Iteration 94, loss = 0.02030822\n",
      "Iteration 95, loss = 0.02281974\n",
      "Iteration 96, loss = 0.01590416\n",
      "Iteration 97, loss = 0.02325677\n",
      "Iteration 98, loss = 0.02387007\n",
      "Iteration 99, loss = 0.02495369\n",
      "Iteration 100, loss = 0.01567097\n",
      "Iteration 101, loss = 0.01467866\n",
      "Iteration 102, loss = 0.01878768\n",
      "Iteration 103, loss = 0.02017984\n",
      "Iteration 104, loss = 0.01728044\n",
      "Iteration 105, loss = 0.03155810\n",
      "Iteration 106, loss = 0.03178915\n",
      "Iteration 107, loss = 0.02507684\n",
      "Iteration 108, loss = 0.27052384\n",
      "Iteration 109, loss = 0.03691308\n",
      "Iteration 110, loss = 0.03022601\n",
      "Iteration 111, loss = 0.02538157\n",
      "Iteration 112, loss = 0.01673640\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.11566008\n",
      "Iteration 2, loss = 1.24299612\n",
      "Iteration 3, loss = 0.59566321\n",
      "Iteration 4, loss = 0.26043160\n",
      "Iteration 5, loss = 0.24808691\n",
      "Iteration 6, loss = 0.26520963\n",
      "Iteration 7, loss = 0.65342026\n",
      "Iteration 8, loss = 0.49675430\n",
      "Iteration 9, loss = 0.23200852\n",
      "Iteration 10, loss = 0.22403476\n",
      "Iteration 11, loss = 0.20476828\n",
      "Iteration 12, loss = 0.21489565\n",
      "Iteration 13, loss = 1.61895102\n",
      "Iteration 14, loss = 2.15497052\n",
      "Iteration 15, loss = 0.91844080\n",
      "Iteration 16, loss = 0.59995883\n",
      "Iteration 17, loss = 0.28963168\n",
      "Iteration 18, loss = 0.23193428\n",
      "Iteration 19, loss = 0.22796374\n",
      "Iteration 20, loss = 0.20788783\n",
      "Iteration 21, loss = 0.20251791\n",
      "Iteration 22, loss = 0.20295133\n",
      "Iteration 23, loss = 0.19651158\n",
      "Iteration 24, loss = 0.20336586\n",
      "Iteration 25, loss = 0.19443588\n",
      "Iteration 26, loss = 0.20077516\n",
      "Iteration 27, loss = 0.18632862\n",
      "Iteration 28, loss = 0.18792519\n",
      "Iteration 29, loss = 0.18319955\n",
      "Iteration 30, loss = 0.19796157\n",
      "Iteration 31, loss = 0.18783887\n",
      "Iteration 32, loss = 0.19350070\n",
      "Iteration 33, loss = 0.20149695\n",
      "Iteration 34, loss = 0.17341246\n",
      "Iteration 35, loss = 0.17328053\n",
      "Iteration 36, loss = 0.19291592\n",
      "Iteration 37, loss = 0.18273309\n",
      "Iteration 38, loss = 0.19261000\n",
      "Iteration 39, loss = 0.35018648\n",
      "Iteration 40, loss = 0.84124784\n",
      "Iteration 41, loss = 1.37801399\n",
      "Iteration 42, loss = 0.29602128\n",
      "Iteration 43, loss = 0.20800095\n",
      "Iteration 44, loss = 0.19964229\n",
      "Iteration 45, loss = 0.18937183\n",
      "Iteration 46, loss = 0.18712720\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.03762978\n",
      "Iteration 2, loss = 0.41148967\n",
      "Iteration 3, loss = 0.89220548\n",
      "Iteration 4, loss = 0.22279027\n",
      "Iteration 5, loss = 0.20900022\n",
      "Iteration 6, loss = 0.26223748\n",
      "Iteration 7, loss = 0.71481503\n",
      "Iteration 8, loss = 0.70966898\n",
      "Iteration 9, loss = 0.53943804\n",
      "Iteration 10, loss = 0.24566159\n",
      "Iteration 11, loss = 0.21030212\n",
      "Iteration 12, loss = 0.21571429\n",
      "Iteration 13, loss = 0.49796227\n",
      "Iteration 14, loss = 1.53318256\n",
      "Iteration 15, loss = 0.22208349\n",
      "Iteration 16, loss = 0.21238566\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.81401504\n",
      "Iteration 2, loss = 0.99300505\n",
      "Iteration 3, loss = 1.28041386\n",
      "Iteration 4, loss = 0.69624678\n",
      "Iteration 5, loss = 0.36824231\n",
      "Iteration 6, loss = 0.25352420\n",
      "Iteration 7, loss = 0.24251400\n",
      "Iteration 8, loss = 0.22617825\n",
      "Iteration 9, loss = 0.23734321\n",
      "Iteration 10, loss = 0.21715167\n",
      "Iteration 11, loss = 0.18052425\n",
      "Iteration 12, loss = 0.16738862\n",
      "Iteration 13, loss = 0.15241454\n",
      "Iteration 14, loss = 0.15622684\n",
      "Iteration 15, loss = 1.09804900\n",
      "Iteration 16, loss = 0.60070731\n",
      "Iteration 17, loss = 0.24852305\n",
      "Iteration 18, loss = 0.27373778\n",
      "Iteration 19, loss = 0.13751119\n",
      "Iteration 20, loss = 0.19692870\n",
      "Iteration 21, loss = 0.15677858\n",
      "Iteration 22, loss = 0.25272548\n",
      "Iteration 23, loss = 0.14947106\n",
      "Iteration 24, loss = 0.17414986\n",
      "Iteration 25, loss = 0.13411413\n",
      "Iteration 26, loss = 0.13045174\n",
      "Iteration 27, loss = 0.12470583\n",
      "Iteration 28, loss = 0.13493453\n",
      "Iteration 29, loss = 0.12366575\n",
      "Iteration 30, loss = 0.14164851\n",
      "Iteration 31, loss = 0.13052559\n",
      "Iteration 32, loss = 0.12247051\n",
      "Iteration 33, loss = 0.13642225\n",
      "Iteration 34, loss = 0.12473654\n",
      "Iteration 35, loss = 0.15019179\n",
      "Iteration 36, loss = 1.63290943\n",
      "Iteration 37, loss = 1.01026256\n",
      "Iteration 38, loss = 0.21455196\n",
      "Iteration 39, loss = 0.19095444\n",
      "Iteration 40, loss = 0.12255104\n",
      "Iteration 41, loss = 0.11924312\n",
      "Iteration 42, loss = 0.12345728\n",
      "Iteration 43, loss = 0.12031577\n",
      "Iteration 44, loss = 0.13517453\n",
      "Iteration 45, loss = 0.13145344\n",
      "Iteration 46, loss = 0.12222648\n",
      "Iteration 47, loss = 0.12185839\n",
      "Iteration 48, loss = 0.12494677\n",
      "Iteration 49, loss = 0.12417208\n",
      "Iteration 50, loss = 0.13304347\n",
      "Iteration 51, loss = 0.12026184\n",
      "Iteration 52, loss = 0.11674111\n",
      "Iteration 53, loss = 0.12049221\n",
      "Iteration 54, loss = 0.12725927\n",
      "Iteration 55, loss = 0.14313835\n",
      "Iteration 56, loss = 0.12665758\n",
      "Iteration 57, loss = 0.12628432\n",
      "Iteration 58, loss = 0.13483130\n",
      "Iteration 59, loss = 0.12062151\n",
      "Iteration 60, loss = 0.11723490\n",
      "Iteration 61, loss = 0.12186757\n",
      "Iteration 62, loss = 0.12350638\n",
      "Iteration 63, loss = 0.12065512\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.87795462\n",
      "Iteration 2, loss = 1.16478000\n",
      "Iteration 3, loss = 0.96222596\n",
      "Iteration 4, loss = 1.12441874\n",
      "Iteration 5, loss = 0.58990021\n",
      "Iteration 6, loss = 0.34705953\n",
      "Iteration 7, loss = 0.29095365\n",
      "Iteration 8, loss = 0.24042408\n",
      "Iteration 9, loss = 0.26286086\n",
      "Iteration 10, loss = 0.25738366\n",
      "Iteration 11, loss = 0.76109807\n",
      "Iteration 12, loss = 0.69206643\n",
      "Iteration 13, loss = 0.28332663\n",
      "Iteration 14, loss = 0.22665315\n",
      "Iteration 15, loss = 0.21554042\n",
      "Iteration 16, loss = 0.76170353\n",
      "Iteration 17, loss = 0.88907652\n",
      "Iteration 18, loss = 0.28603928\n",
      "Iteration 19, loss = 0.16222238\n",
      "Iteration 20, loss = 0.14281763\n",
      "Iteration 21, loss = 0.13864017\n",
      "Iteration 22, loss = 0.14680107\n",
      "Iteration 23, loss = 0.15292039\n",
      "Iteration 24, loss = 0.15112456\n",
      "Iteration 25, loss = 0.14271194\n",
      "Iteration 26, loss = 0.14037780\n",
      "Iteration 27, loss = 0.13890915\n",
      "Iteration 28, loss = 0.14213861\n",
      "Iteration 29, loss = 0.13844144\n",
      "Iteration 30, loss = 0.15829233\n",
      "Iteration 31, loss = 0.13363594\n",
      "Iteration 32, loss = 0.16219734\n",
      "Iteration 33, loss = 0.15774768\n",
      "Iteration 34, loss = 0.13480337\n",
      "Iteration 35, loss = 0.13705187\n",
      "Iteration 36, loss = 0.15814365\n",
      "Iteration 37, loss = 0.13619535\n",
      "Iteration 38, loss = 0.14531099\n",
      "Iteration 39, loss = 0.14297363\n",
      "Iteration 40, loss = 0.13110593\n",
      "Iteration 41, loss = 0.14091634\n",
      "Iteration 42, loss = 0.13418003\n",
      "Iteration 43, loss = 0.14555989\n",
      "Iteration 44, loss = 0.12916486\n",
      "Iteration 45, loss = 0.24236539\n",
      "Iteration 46, loss = 0.37845886\n",
      "Iteration 47, loss = 0.25775405\n",
      "Iteration 48, loss = 0.25082964\n",
      "Iteration 49, loss = 1.17245974\n",
      "Iteration 50, loss = 0.87490176\n",
      "Iteration 51, loss = 0.16144890\n",
      "Iteration 52, loss = 0.13651300\n",
      "Iteration 53, loss = 0.13117556\n",
      "Iteration 54, loss = 0.14289893\n",
      "Iteration 55, loss = 0.14771513\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.88919788\n",
      "Iteration 2, loss = 0.85078774\n",
      "Iteration 3, loss = 1.07145628\n",
      "Iteration 4, loss = 1.10038065\n",
      "Iteration 5, loss = 0.51167536\n",
      "Iteration 6, loss = 0.23602433\n",
      "Iteration 7, loss = 0.20444244\n",
      "Iteration 8, loss = 0.20622389\n",
      "Iteration 9, loss = 0.23954653\n",
      "Iteration 10, loss = 0.23558078\n",
      "Iteration 11, loss = 0.21207799\n",
      "Iteration 12, loss = 0.24328008\n",
      "Iteration 13, loss = 1.33021343\n",
      "Iteration 14, loss = 1.59960820\n",
      "Iteration 15, loss = 0.55384934\n",
      "Iteration 16, loss = 0.39853183\n",
      "Iteration 17, loss = 0.20139105\n",
      "Iteration 18, loss = 0.16319356\n",
      "Iteration 19, loss = 0.14701376\n",
      "Iteration 20, loss = 0.14861474\n",
      "Iteration 21, loss = 0.14768195\n",
      "Iteration 22, loss = 0.15414714\n",
      "Iteration 23, loss = 0.15681649\n",
      "Iteration 24, loss = 0.15874763\n",
      "Iteration 25, loss = 0.14923813\n",
      "Iteration 26, loss = 0.14981629\n",
      "Iteration 27, loss = 0.14650503\n",
      "Iteration 28, loss = 0.14844281\n",
      "Iteration 29, loss = 0.14539332\n",
      "Iteration 30, loss = 0.16657886\n",
      "Iteration 31, loss = 0.14337129\n",
      "Iteration 32, loss = 0.16653770\n",
      "Iteration 33, loss = 0.16067462\n",
      "Iteration 34, loss = 0.14851244\n",
      "Iteration 35, loss = 0.14570800\n",
      "Iteration 36, loss = 0.20856215\n",
      "Iteration 37, loss = 0.15921830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38, loss = 0.15051351\n",
      "Iteration 39, loss = 0.15394853\n",
      "Iteration 40, loss = 0.14127631\n",
      "Iteration 41, loss = 0.14780336\n",
      "Iteration 42, loss = 0.14602452\n",
      "Iteration 43, loss = 0.15027917\n",
      "Iteration 44, loss = 0.14645248\n",
      "Iteration 45, loss = 0.20495159\n",
      "Iteration 46, loss = 0.92332293\n",
      "Iteration 47, loss = 0.47684740\n",
      "Iteration 48, loss = 0.29644536\n",
      "Iteration 49, loss = 0.22042067\n",
      "Iteration 50, loss = 0.22103848\n",
      "Iteration 51, loss = 0.14188513\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.97164814\n",
      "Iteration 2, loss = 1.27643173\n",
      "Iteration 3, loss = 0.77064986\n",
      "Iteration 4, loss = 0.37967945\n",
      "Iteration 5, loss = 0.34683230\n",
      "Iteration 6, loss = 0.43995636\n",
      "Iteration 7, loss = 0.44893554\n",
      "Iteration 8, loss = 1.19249256\n",
      "Iteration 9, loss = 0.64757184\n",
      "Iteration 10, loss = 0.27650354\n",
      "Iteration 11, loss = 0.18314690\n",
      "Iteration 12, loss = 0.17871991\n",
      "Iteration 13, loss = 0.17149088\n",
      "Iteration 14, loss = 0.20100007\n",
      "Iteration 15, loss = 0.20062170\n",
      "Iteration 16, loss = 0.24424185\n",
      "Iteration 17, loss = 0.20650643\n",
      "Iteration 18, loss = 0.19751632\n",
      "Iteration 19, loss = 0.20319175\n",
      "Iteration 20, loss = 0.21312116\n",
      "Iteration 21, loss = 0.30407721\n",
      "Iteration 22, loss = 0.17181981\n",
      "Iteration 23, loss = 0.23757022\n",
      "Iteration 24, loss = 0.16921654\n",
      "Iteration 25, loss = 0.39026090\n",
      "Iteration 26, loss = 1.67558154\n",
      "Iteration 27, loss = 0.51142246\n",
      "Iteration 28, loss = 0.30003756\n",
      "Iteration 29, loss = 0.23834780\n",
      "Iteration 30, loss = 0.19311545\n",
      "Iteration 31, loss = 0.14499250\n",
      "Iteration 32, loss = 0.14803005\n",
      "Iteration 33, loss = 0.15166234\n",
      "Iteration 34, loss = 0.14470310\n",
      "Iteration 35, loss = 0.15419784\n",
      "Iteration 36, loss = 0.16062386\n",
      "Iteration 37, loss = 0.14264914\n",
      "Iteration 38, loss = 0.13927893\n",
      "Iteration 39, loss = 0.13995479\n",
      "Iteration 40, loss = 0.14349921\n",
      "Iteration 41, loss = 0.13745949\n",
      "Iteration 42, loss = 0.13730172\n",
      "Iteration 43, loss = 0.13862807\n",
      "Iteration 44, loss = 0.15492039\n",
      "Iteration 45, loss = 0.16083179\n",
      "Iteration 46, loss = 0.14520481\n",
      "Iteration 47, loss = 0.13593739\n",
      "Iteration 48, loss = 0.14514944\n",
      "Iteration 49, loss = 0.14329682\n",
      "Iteration 50, loss = 0.20653586\n",
      "Iteration 51, loss = 0.23602974\n",
      "Iteration 52, loss = 0.14564837\n",
      "Iteration 53, loss = 0.13977721\n",
      "Iteration 54, loss = 0.14032272\n",
      "Iteration 55, loss = 0.16018801\n",
      "Iteration 56, loss = 0.14655584\n",
      "Iteration 57, loss = 0.13680857\n",
      "Iteration 58, loss = 0.15004130\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.47771588\n",
      "Iteration 2, loss = 0.35785748\n",
      "Iteration 3, loss = 1.20373707\n",
      "Iteration 4, loss = 1.49998183\n",
      "Iteration 5, loss = 1.17638801\n",
      "Iteration 6, loss = 1.09072846\n",
      "Iteration 7, loss = 0.62756761\n",
      "Iteration 8, loss = 0.28943991\n",
      "Iteration 9, loss = 0.27018879\n",
      "Iteration 10, loss = 0.25525755\n",
      "Iteration 11, loss = 0.26027681\n",
      "Iteration 12, loss = 0.27237895\n",
      "Iteration 13, loss = 0.22157759\n",
      "Iteration 14, loss = 0.21723455\n",
      "Iteration 15, loss = 0.22765602\n",
      "Iteration 16, loss = 0.22898854\n",
      "Iteration 17, loss = 0.21707830\n",
      "Iteration 18, loss = 0.18452281\n",
      "Iteration 19, loss = 0.17148546\n",
      "Iteration 20, loss = 0.17666714\n",
      "Iteration 21, loss = 0.27335548\n",
      "Iteration 22, loss = 1.21648123\n",
      "Iteration 23, loss = 1.21315054\n",
      "Iteration 24, loss = 0.66346930\n",
      "Iteration 25, loss = 0.30855644\n",
      "Iteration 26, loss = 0.20966761\n",
      "Iteration 27, loss = 0.15915398\n",
      "Iteration 28, loss = 0.15383588\n",
      "Iteration 29, loss = 0.16430162\n",
      "Iteration 30, loss = 0.17270597\n",
      "Iteration 31, loss = 0.15584343\n",
      "Iteration 32, loss = 0.15595342\n",
      "Iteration 33, loss = 0.14823812\n",
      "Iteration 34, loss = 0.14813207\n",
      "Iteration 35, loss = 0.16006133\n",
      "Iteration 36, loss = 0.17765577\n",
      "Iteration 37, loss = 0.15117308\n",
      "Iteration 38, loss = 0.14896944\n",
      "Iteration 39, loss = 0.14933508\n",
      "Iteration 40, loss = 0.15653872\n",
      "Iteration 41, loss = 0.14481722\n",
      "Iteration 42, loss = 0.14964807\n",
      "Iteration 43, loss = 0.15314707\n",
      "Iteration 44, loss = 0.16534222\n",
      "Iteration 45, loss = 0.16876006\n",
      "Iteration 46, loss = 0.16011280\n",
      "Iteration 47, loss = 0.14415103\n",
      "Iteration 48, loss = 0.15230452\n",
      "Iteration 49, loss = 0.17218162\n",
      "Iteration 50, loss = 0.20835955\n",
      "Iteration 51, loss = 0.15139917\n",
      "Iteration 52, loss = 0.14878360\n",
      "Iteration 53, loss = 0.15480601\n",
      "Iteration 54, loss = 0.15529424\n",
      "Iteration 55, loss = 0.16070763\n",
      "Iteration 56, loss = 0.15197608\n",
      "Iteration 57, loss = 0.14726117\n",
      "Iteration 58, loss = 0.15914114\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.51978772\n",
      "Iteration 2, loss = 1.14878425\n",
      "Iteration 3, loss = 0.43151411\n",
      "Iteration 4, loss = 0.29483071\n",
      "Iteration 5, loss = 0.33928586\n",
      "Iteration 6, loss = 0.83072516\n",
      "Iteration 7, loss = 0.36661076\n",
      "Iteration 8, loss = 0.29853474\n",
      "Iteration 9, loss = 0.24822594\n",
      "Iteration 10, loss = 0.41492325\n",
      "Iteration 11, loss = 1.78379272\n",
      "Iteration 12, loss = 1.34693437\n",
      "Iteration 13, loss = 0.57947102\n",
      "Iteration 14, loss = 0.31350430\n",
      "Iteration 15, loss = 0.24643458\n",
      "Iteration 16, loss = 0.22758208\n",
      "Iteration 17, loss = 0.24473287\n",
      "Iteration 18, loss = 0.21632534\n",
      "Iteration 19, loss = 0.21184125\n",
      "Iteration 20, loss = 0.22172344\n",
      "Iteration 21, loss = 0.20449432\n",
      "Iteration 22, loss = 0.21216585\n",
      "Iteration 23, loss = 0.20162846\n",
      "Iteration 24, loss = 0.17773195\n",
      "Iteration 25, loss = 0.18042404\n",
      "Iteration 26, loss = 0.16763818\n",
      "Iteration 27, loss = 0.22990041\n",
      "Iteration 28, loss = 0.33161881\n",
      "Iteration 29, loss = 0.18198030\n",
      "Iteration 30, loss = 0.18104927\n",
      "Iteration 31, loss = 0.16149376\n",
      "Iteration 32, loss = 0.15790364\n",
      "Iteration 33, loss = 0.17166079\n",
      "Iteration 34, loss = 0.16628571\n",
      "Iteration 35, loss = 0.17376077\n",
      "Iteration 36, loss = 0.19128738\n",
      "Iteration 37, loss = 0.15263435\n",
      "Iteration 38, loss = 0.16134032\n",
      "Iteration 39, loss = 0.16199240\n",
      "Iteration 40, loss = 0.15683049\n",
      "Iteration 41, loss = 0.26643479\n",
      "Iteration 42, loss = 0.97394114\n",
      "Iteration 43, loss = 0.81218562\n",
      "Iteration 44, loss = 0.20562009\n",
      "Iteration 45, loss = 0.16083486\n",
      "Iteration 46, loss = 0.15555449\n",
      "Iteration 47, loss = 0.15044480\n",
      "Iteration 48, loss = 0.15149854\n",
      "Iteration 49, loss = 0.15253323\n",
      "Iteration 50, loss = 0.14916566\n",
      "Iteration 51, loss = 0.15411271\n",
      "Iteration 52, loss = 0.14823849\n",
      "Iteration 53, loss = 0.15567227\n",
      "Iteration 54, loss = 0.16133104\n",
      "Iteration 55, loss = 0.14827631\n",
      "Iteration 56, loss = 0.14873610\n",
      "Iteration 57, loss = 0.15235610\n",
      "Iteration 58, loss = 0.15154208\n",
      "Iteration 59, loss = 0.14983203\n",
      "Iteration 60, loss = 0.15420210\n",
      "Iteration 61, loss = 0.16013581\n",
      "Iteration 62, loss = 0.16370119\n",
      "Iteration 63, loss = 0.15009410\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.58520356\n",
      "Iteration 2, loss = 0.69282103\n",
      "Iteration 3, loss = 0.34908418\n",
      "Iteration 4, loss = 0.26711031\n",
      "Iteration 5, loss = 0.45093593\n",
      "Iteration 6, loss = 0.53670484\n",
      "Iteration 7, loss = 1.08309977\n",
      "Iteration 8, loss = 0.25645601\n",
      "Iteration 9, loss = 0.22690729\n",
      "Iteration 10, loss = 0.25817336\n",
      "Iteration 11, loss = 0.20841617\n",
      "Iteration 12, loss = 0.19825610\n",
      "Iteration 13, loss = 0.18964123\n",
      "Iteration 14, loss = 0.23091004\n",
      "Iteration 15, loss = 0.19846482\n",
      "Iteration 16, loss = 0.48800122\n",
      "Iteration 17, loss = 0.21064513\n",
      "Iteration 18, loss = 0.78039765\n",
      "Iteration 19, loss = 0.39209170\n",
      "Iteration 20, loss = 0.24329094\n",
      "Iteration 21, loss = 0.17877355\n",
      "Iteration 22, loss = 0.24441914\n",
      "Iteration 23, loss = 1.07016614\n",
      "Iteration 24, loss = 0.41990489\n",
      "Iteration 25, loss = 0.15375139\n",
      "Iteration 26, loss = 0.14612194\n",
      "Iteration 27, loss = 0.15859904\n",
      "Iteration 28, loss = 0.15070535\n",
      "Iteration 29, loss = 0.15048850\n",
      "Iteration 30, loss = 0.14862173\n",
      "Iteration 31, loss = 0.14832089\n",
      "Iteration 32, loss = 0.15426400\n",
      "Iteration 33, loss = 0.15986206\n",
      "Iteration 34, loss = 0.15482896\n",
      "Iteration 35, loss = 0.15359349\n",
      "Iteration 36, loss = 0.17582356\n",
      "Iteration 37, loss = 0.14643638\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.94959570\n",
      "Iteration 2, loss = 0.99405438\n",
      "Iteration 3, loss = 0.30693570\n",
      "Iteration 4, loss = 0.71838183\n",
      "Iteration 5, loss = 2.00404006\n",
      "Iteration 6, loss = 0.75998893\n",
      "Iteration 7, loss = 0.30469863\n",
      "Iteration 8, loss = 0.22477354\n",
      "Iteration 9, loss = 0.19885145\n",
      "Iteration 10, loss = 0.17697913\n",
      "Iteration 11, loss = 0.18093333\n",
      "Iteration 12, loss = 0.17258723\n",
      "Iteration 13, loss = 0.16157724\n",
      "Iteration 14, loss = 0.15363671\n",
      "Iteration 15, loss = 0.15269298\n",
      "Iteration 16, loss = 0.14120870\n",
      "Iteration 17, loss = 0.13355693\n",
      "Iteration 18, loss = 0.11733827\n",
      "Iteration 19, loss = 0.11687323\n",
      "Iteration 20, loss = 0.11313836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 0.10996651\n",
      "Iteration 22, loss = 0.10389587\n",
      "Iteration 23, loss = 0.10294207\n",
      "Iteration 24, loss = 0.08465362\n",
      "Iteration 25, loss = 0.08530181\n",
      "Iteration 26, loss = 0.06965665\n",
      "Iteration 27, loss = 0.07402426\n",
      "Iteration 28, loss = 0.07796792\n",
      "Iteration 29, loss = 0.07372804\n",
      "Iteration 30, loss = 0.06856814\n",
      "Iteration 31, loss = 0.06450901\n",
      "Iteration 32, loss = 0.06146261\n",
      "Iteration 33, loss = 0.05960154\n",
      "Iteration 34, loss = 0.05810631\n",
      "Iteration 35, loss = 0.05425148\n",
      "Iteration 36, loss = 0.05461473\n",
      "Iteration 37, loss = 0.05346897\n",
      "Iteration 38, loss = 0.04831520\n",
      "Iteration 39, loss = 0.04407031\n",
      "Iteration 40, loss = 0.04678838\n",
      "Iteration 41, loss = 0.04182475\n",
      "Iteration 42, loss = 0.05383491\n",
      "Iteration 43, loss = 0.03898719\n",
      "Iteration 44, loss = 0.67707394\n",
      "Iteration 45, loss = 1.84483871\n",
      "Iteration 46, loss = 0.24989543\n",
      "Iteration 47, loss = 0.13215706\n",
      "Iteration 48, loss = 0.10611250\n",
      "Iteration 49, loss = 0.09369331\n",
      "Iteration 50, loss = 0.08578773\n",
      "Iteration 51, loss = 0.07978358\n",
      "Iteration 52, loss = 0.07565518\n",
      "Iteration 53, loss = 0.07197897\n",
      "Iteration 54, loss = 0.06830458\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy =  [0.84415584 0.89123377 0.8262987  0.93668831 0.93344156 0.88798701\n",
      " 0.98701299 0.97398374 0.95447154 0.89593496]\n",
      "Accuracy mean and std : 0.913 (0.051)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.07      0.08       692\n",
      "           1       0.89      0.93      0.91      5465\n",
      "\n",
      "    accuracy                           0.83      6157\n",
      "   macro avg       0.50      0.50      0.49      6157\n",
      "weighted avg       0.80      0.83      0.81      6157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KFold cross validation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=10)\n",
    "class_names = ['0','1']\n",
    "y_pred = cross_val_predict(modelMLP4, X, Y, cv = cv)\n",
    "\n",
    "cv_score_for_LR = cross_val_score(modelMLP4, X, Y, cv = 10)\n",
    "print(\"Accuracy = \",cv_score_for_LR)\n",
    "print('Accuracy mean and std : %.3f (%.3f)' % (mean(cv_score_for_LR), std(cv_score_for_LR)))\n",
    "\n",
    "print(classification_report(Y, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7a5e571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.11566008\n",
      "Iteration 2, loss = 1.24299612\n",
      "Iteration 3, loss = 0.59566321\n",
      "Iteration 4, loss = 0.26043160\n",
      "Iteration 5, loss = 0.24808691\n",
      "Iteration 6, loss = 0.26520963\n",
      "Iteration 7, loss = 0.65342026\n",
      "Iteration 8, loss = 0.49675430\n",
      "Iteration 9, loss = 0.23200852\n",
      "Iteration 10, loss = 0.22403476\n",
      "Iteration 11, loss = 0.20476828\n",
      "Iteration 12, loss = 0.21489565\n",
      "Iteration 13, loss = 1.61895102\n",
      "Iteration 14, loss = 2.15497052\n",
      "Iteration 15, loss = 0.91844080\n",
      "Iteration 16, loss = 0.59995883\n",
      "Iteration 17, loss = 0.28963168\n",
      "Iteration 18, loss = 0.23193428\n",
      "Iteration 19, loss = 0.22796374\n",
      "Iteration 20, loss = 0.20788783\n",
      "Iteration 21, loss = 0.20251791\n",
      "Iteration 22, loss = 0.20295133\n",
      "Iteration 23, loss = 0.19651158\n",
      "Iteration 24, loss = 0.20336586\n",
      "Iteration 25, loss = 0.19443588\n",
      "Iteration 26, loss = 0.20077516\n",
      "Iteration 27, loss = 0.18632862\n",
      "Iteration 28, loss = 0.18792519\n",
      "Iteration 29, loss = 0.18319955\n",
      "Iteration 30, loss = 0.19796157\n",
      "Iteration 31, loss = 0.18783887\n",
      "Iteration 32, loss = 0.19350070\n",
      "Iteration 33, loss = 0.20149695\n",
      "Iteration 34, loss = 0.17341246\n",
      "Iteration 35, loss = 0.17328053\n",
      "Iteration 36, loss = 0.19291592\n",
      "Iteration 37, loss = 0.18273309\n",
      "Iteration 38, loss = 0.19261000\n",
      "Iteration 39, loss = 0.35018648\n",
      "Iteration 40, loss = 0.84124784\n",
      "Iteration 41, loss = 1.37801399\n",
      "Iteration 42, loss = 0.29602128\n",
      "Iteration 43, loss = 0.20800095\n",
      "Iteration 44, loss = 0.19964229\n",
      "Iteration 45, loss = 0.18937183\n",
      "Iteration 46, loss = 0.18712720\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.03762978\n",
      "Iteration 2, loss = 0.41148967\n",
      "Iteration 3, loss = 0.89220548\n",
      "Iteration 4, loss = 0.22279027\n",
      "Iteration 5, loss = 0.20900022\n",
      "Iteration 6, loss = 0.26223748\n",
      "Iteration 7, loss = 0.71481503\n",
      "Iteration 8, loss = 0.70966898\n",
      "Iteration 9, loss = 0.53943804\n",
      "Iteration 10, loss = 0.24566159\n",
      "Iteration 11, loss = 0.21030212\n",
      "Iteration 12, loss = 0.21571429\n",
      "Iteration 13, loss = 0.49796227\n",
      "Iteration 14, loss = 1.53318256\n",
      "Iteration 15, loss = 0.22208349\n",
      "Iteration 16, loss = 0.21238566\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.81401504\n",
      "Iteration 2, loss = 0.99300505\n",
      "Iteration 3, loss = 1.28041386\n",
      "Iteration 4, loss = 0.69624678\n",
      "Iteration 5, loss = 0.36824231\n",
      "Iteration 6, loss = 0.25352420\n",
      "Iteration 7, loss = 0.24251400\n",
      "Iteration 8, loss = 0.22617825\n",
      "Iteration 9, loss = 0.23734321\n",
      "Iteration 10, loss = 0.21715167\n",
      "Iteration 11, loss = 0.18052425\n",
      "Iteration 12, loss = 0.16738862\n",
      "Iteration 13, loss = 0.15241454\n",
      "Iteration 14, loss = 0.15622684\n",
      "Iteration 15, loss = 1.09804900\n",
      "Iteration 16, loss = 0.60070731\n",
      "Iteration 17, loss = 0.24852305\n",
      "Iteration 18, loss = 0.27373778\n",
      "Iteration 19, loss = 0.13751119\n",
      "Iteration 20, loss = 0.19692870\n",
      "Iteration 21, loss = 0.15677858\n",
      "Iteration 22, loss = 0.25272548\n",
      "Iteration 23, loss = 0.14947106\n",
      "Iteration 24, loss = 0.17414986\n",
      "Iteration 25, loss = 0.13411413\n",
      "Iteration 26, loss = 0.13045174\n",
      "Iteration 27, loss = 0.12470583\n",
      "Iteration 28, loss = 0.13493453\n",
      "Iteration 29, loss = 0.12366575\n",
      "Iteration 30, loss = 0.14164851\n",
      "Iteration 31, loss = 0.13052559\n",
      "Iteration 32, loss = 0.12247051\n",
      "Iteration 33, loss = 0.13642225\n",
      "Iteration 34, loss = 0.12473654\n",
      "Iteration 35, loss = 0.15019179\n",
      "Iteration 36, loss = 1.63290943\n",
      "Iteration 37, loss = 1.01026256\n",
      "Iteration 38, loss = 0.21455196\n",
      "Iteration 39, loss = 0.19095444\n",
      "Iteration 40, loss = 0.12255104\n",
      "Iteration 41, loss = 0.11924312\n",
      "Iteration 42, loss = 0.12345728\n",
      "Iteration 43, loss = 0.12031577\n",
      "Iteration 44, loss = 0.13517453\n",
      "Iteration 45, loss = 0.13145344\n",
      "Iteration 46, loss = 0.12222648\n",
      "Iteration 47, loss = 0.12185839\n",
      "Iteration 48, loss = 0.12494677\n",
      "Iteration 49, loss = 0.12417208\n",
      "Iteration 50, loss = 0.13304347\n",
      "Iteration 51, loss = 0.12026184\n",
      "Iteration 52, loss = 0.11674111\n",
      "Iteration 53, loss = 0.12049221\n",
      "Iteration 54, loss = 0.12725927\n",
      "Iteration 55, loss = 0.14313835\n",
      "Iteration 56, loss = 0.12665758\n",
      "Iteration 57, loss = 0.12628432\n",
      "Iteration 58, loss = 0.13483130\n",
      "Iteration 59, loss = 0.12062151\n",
      "Iteration 60, loss = 0.11723490\n",
      "Iteration 61, loss = 0.12186757\n",
      "Iteration 62, loss = 0.12350638\n",
      "Iteration 63, loss = 0.12065512\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.87795462\n",
      "Iteration 2, loss = 1.16478000\n",
      "Iteration 3, loss = 0.96222596\n",
      "Iteration 4, loss = 1.12441874\n",
      "Iteration 5, loss = 0.58990021\n",
      "Iteration 6, loss = 0.34705953\n",
      "Iteration 7, loss = 0.29095365\n",
      "Iteration 8, loss = 0.24042408\n",
      "Iteration 9, loss = 0.26286086\n",
      "Iteration 10, loss = 0.25738366\n",
      "Iteration 11, loss = 0.76109807\n",
      "Iteration 12, loss = 0.69206643\n",
      "Iteration 13, loss = 0.28332663\n",
      "Iteration 14, loss = 0.22665315\n",
      "Iteration 15, loss = 0.21554042\n",
      "Iteration 16, loss = 0.76170353\n",
      "Iteration 17, loss = 0.88907652\n",
      "Iteration 18, loss = 0.28603928\n",
      "Iteration 19, loss = 0.16222238\n",
      "Iteration 20, loss = 0.14281763\n",
      "Iteration 21, loss = 0.13864017\n",
      "Iteration 22, loss = 0.14680107\n",
      "Iteration 23, loss = 0.15292039\n",
      "Iteration 24, loss = 0.15112456\n",
      "Iteration 25, loss = 0.14271194\n",
      "Iteration 26, loss = 0.14037780\n",
      "Iteration 27, loss = 0.13890915\n",
      "Iteration 28, loss = 0.14213861\n",
      "Iteration 29, loss = 0.13844144\n",
      "Iteration 30, loss = 0.15829233\n",
      "Iteration 31, loss = 0.13363594\n",
      "Iteration 32, loss = 0.16219734\n",
      "Iteration 33, loss = 0.15774768\n",
      "Iteration 34, loss = 0.13480337\n",
      "Iteration 35, loss = 0.13705187\n",
      "Iteration 36, loss = 0.15814365\n",
      "Iteration 37, loss = 0.13619535\n",
      "Iteration 38, loss = 0.14531099\n",
      "Iteration 39, loss = 0.14297363\n",
      "Iteration 40, loss = 0.13110593\n",
      "Iteration 41, loss = 0.14091634\n",
      "Iteration 42, loss = 0.13418003\n",
      "Iteration 43, loss = 0.14555989\n",
      "Iteration 44, loss = 0.12916486\n",
      "Iteration 45, loss = 0.24236539\n",
      "Iteration 46, loss = 0.37845886\n",
      "Iteration 47, loss = 0.25775405\n",
      "Iteration 48, loss = 0.25082964\n",
      "Iteration 49, loss = 1.17245974\n",
      "Iteration 50, loss = 0.87490176\n",
      "Iteration 51, loss = 0.16144890\n",
      "Iteration 52, loss = 0.13651300\n",
      "Iteration 53, loss = 0.13117556\n",
      "Iteration 54, loss = 0.14289893\n",
      "Iteration 55, loss = 0.14771513\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.88919788\n",
      "Iteration 2, loss = 0.85078774\n",
      "Iteration 3, loss = 1.07145628\n",
      "Iteration 4, loss = 1.10038065\n",
      "Iteration 5, loss = 0.51167536\n",
      "Iteration 6, loss = 0.23602433\n",
      "Iteration 7, loss = 0.20444244\n",
      "Iteration 8, loss = 0.20622389\n",
      "Iteration 9, loss = 0.23954653\n",
      "Iteration 10, loss = 0.23558078\n",
      "Iteration 11, loss = 0.21207799\n",
      "Iteration 12, loss = 0.24328008\n",
      "Iteration 13, loss = 1.33021343\n",
      "Iteration 14, loss = 1.59960820\n",
      "Iteration 15, loss = 0.55384934\n",
      "Iteration 16, loss = 0.39853183\n",
      "Iteration 17, loss = 0.20139105\n",
      "Iteration 18, loss = 0.16319356\n",
      "Iteration 19, loss = 0.14701376\n",
      "Iteration 20, loss = 0.14861474\n",
      "Iteration 21, loss = 0.14768195\n",
      "Iteration 22, loss = 0.15414714\n",
      "Iteration 23, loss = 0.15681649\n",
      "Iteration 24, loss = 0.15874763\n",
      "Iteration 25, loss = 0.14923813\n",
      "Iteration 26, loss = 0.14981629\n",
      "Iteration 27, loss = 0.14650503\n",
      "Iteration 28, loss = 0.14844281\n",
      "Iteration 29, loss = 0.14539332\n",
      "Iteration 30, loss = 0.16657886\n",
      "Iteration 31, loss = 0.14337129\n",
      "Iteration 32, loss = 0.16653770\n",
      "Iteration 33, loss = 0.16067462\n",
      "Iteration 34, loss = 0.14851244\n",
      "Iteration 35, loss = 0.14570800\n",
      "Iteration 36, loss = 0.20856215\n",
      "Iteration 37, loss = 0.15921830\n",
      "Iteration 38, loss = 0.15051351\n",
      "Iteration 39, loss = 0.15394853\n",
      "Iteration 40, loss = 0.14127631\n",
      "Iteration 41, loss = 0.14780336\n",
      "Iteration 42, loss = 0.14602452\n",
      "Iteration 43, loss = 0.15027917\n",
      "Iteration 44, loss = 0.14645248\n",
      "Iteration 45, loss = 0.20495159\n",
      "Iteration 46, loss = 0.92332293\n",
      "Iteration 47, loss = 0.47684740\n",
      "Iteration 48, loss = 0.29644536\n",
      "Iteration 49, loss = 0.22042067\n",
      "Iteration 50, loss = 0.22103848\n",
      "Iteration 51, loss = 0.14188513\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.97164814\n",
      "Iteration 2, loss = 1.27643173\n",
      "Iteration 3, loss = 0.77064986\n",
      "Iteration 4, loss = 0.37967945\n",
      "Iteration 5, loss = 0.34683230\n",
      "Iteration 6, loss = 0.43995636\n",
      "Iteration 7, loss = 0.44893554\n",
      "Iteration 8, loss = 1.19249256\n",
      "Iteration 9, loss = 0.64757184\n",
      "Iteration 10, loss = 0.27650354\n",
      "Iteration 11, loss = 0.18314690\n",
      "Iteration 12, loss = 0.17871991\n",
      "Iteration 13, loss = 0.17149088\n",
      "Iteration 14, loss = 0.20100007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.20062170\n",
      "Iteration 16, loss = 0.24424185\n",
      "Iteration 17, loss = 0.20650643\n",
      "Iteration 18, loss = 0.19751632\n",
      "Iteration 19, loss = 0.20319175\n",
      "Iteration 20, loss = 0.21312116\n",
      "Iteration 21, loss = 0.30407721\n",
      "Iteration 22, loss = 0.17181981\n",
      "Iteration 23, loss = 0.23757022\n",
      "Iteration 24, loss = 0.16921654\n",
      "Iteration 25, loss = 0.39026090\n",
      "Iteration 26, loss = 1.67558154\n",
      "Iteration 27, loss = 0.51142246\n",
      "Iteration 28, loss = 0.30003756\n",
      "Iteration 29, loss = 0.23834780\n",
      "Iteration 30, loss = 0.19311545\n",
      "Iteration 31, loss = 0.14499250\n",
      "Iteration 32, loss = 0.14803005\n",
      "Iteration 33, loss = 0.15166234\n",
      "Iteration 34, loss = 0.14470310\n",
      "Iteration 35, loss = 0.15419784\n",
      "Iteration 36, loss = 0.16062386\n",
      "Iteration 37, loss = 0.14264914\n",
      "Iteration 38, loss = 0.13927893\n",
      "Iteration 39, loss = 0.13995479\n",
      "Iteration 40, loss = 0.14349921\n",
      "Iteration 41, loss = 0.13745949\n",
      "Iteration 42, loss = 0.13730172\n",
      "Iteration 43, loss = 0.13862807\n",
      "Iteration 44, loss = 0.15492039\n",
      "Iteration 45, loss = 0.16083179\n",
      "Iteration 46, loss = 0.14520481\n",
      "Iteration 47, loss = 0.13593739\n",
      "Iteration 48, loss = 0.14514944\n",
      "Iteration 49, loss = 0.14329682\n",
      "Iteration 50, loss = 0.20653586\n",
      "Iteration 51, loss = 0.23602974\n",
      "Iteration 52, loss = 0.14564837\n",
      "Iteration 53, loss = 0.13977721\n",
      "Iteration 54, loss = 0.14032272\n",
      "Iteration 55, loss = 0.16018801\n",
      "Iteration 56, loss = 0.14655584\n",
      "Iteration 57, loss = 0.13680857\n",
      "Iteration 58, loss = 0.15004130\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.47771588\n",
      "Iteration 2, loss = 0.35785748\n",
      "Iteration 3, loss = 1.20373707\n",
      "Iteration 4, loss = 1.49998183\n",
      "Iteration 5, loss = 1.17638801\n",
      "Iteration 6, loss = 1.09072846\n",
      "Iteration 7, loss = 0.62756761\n",
      "Iteration 8, loss = 0.28943991\n",
      "Iteration 9, loss = 0.27018879\n",
      "Iteration 10, loss = 0.25525755\n",
      "Iteration 11, loss = 0.26027681\n",
      "Iteration 12, loss = 0.27237895\n",
      "Iteration 13, loss = 0.22157759\n",
      "Iteration 14, loss = 0.21723455\n",
      "Iteration 15, loss = 0.22765602\n",
      "Iteration 16, loss = 0.22898854\n",
      "Iteration 17, loss = 0.21707830\n",
      "Iteration 18, loss = 0.18452281\n",
      "Iteration 19, loss = 0.17148546\n",
      "Iteration 20, loss = 0.17666714\n",
      "Iteration 21, loss = 0.27335548\n",
      "Iteration 22, loss = 1.21648123\n",
      "Iteration 23, loss = 1.21315054\n",
      "Iteration 24, loss = 0.66346930\n",
      "Iteration 25, loss = 0.30855644\n",
      "Iteration 26, loss = 0.20966761\n",
      "Iteration 27, loss = 0.15915398\n",
      "Iteration 28, loss = 0.15383588\n",
      "Iteration 29, loss = 0.16430162\n",
      "Iteration 30, loss = 0.17270597\n",
      "Iteration 31, loss = 0.15584343\n",
      "Iteration 32, loss = 0.15595342\n",
      "Iteration 33, loss = 0.14823812\n",
      "Iteration 34, loss = 0.14813207\n",
      "Iteration 35, loss = 0.16006133\n",
      "Iteration 36, loss = 0.17765577\n",
      "Iteration 37, loss = 0.15117308\n",
      "Iteration 38, loss = 0.14896944\n",
      "Iteration 39, loss = 0.14933508\n",
      "Iteration 40, loss = 0.15653872\n",
      "Iteration 41, loss = 0.14481722\n",
      "Iteration 42, loss = 0.14964807\n",
      "Iteration 43, loss = 0.15314707\n",
      "Iteration 44, loss = 0.16534222\n",
      "Iteration 45, loss = 0.16876006\n",
      "Iteration 46, loss = 0.16011280\n",
      "Iteration 47, loss = 0.14415103\n",
      "Iteration 48, loss = 0.15230452\n",
      "Iteration 49, loss = 0.17218162\n",
      "Iteration 50, loss = 0.20835955\n",
      "Iteration 51, loss = 0.15139917\n",
      "Iteration 52, loss = 0.14878360\n",
      "Iteration 53, loss = 0.15480601\n",
      "Iteration 54, loss = 0.15529424\n",
      "Iteration 55, loss = 0.16070763\n",
      "Iteration 56, loss = 0.15197608\n",
      "Iteration 57, loss = 0.14726117\n",
      "Iteration 58, loss = 0.15914114\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.51978772\n",
      "Iteration 2, loss = 1.14878425\n",
      "Iteration 3, loss = 0.43151411\n",
      "Iteration 4, loss = 0.29483071\n",
      "Iteration 5, loss = 0.33928586\n",
      "Iteration 6, loss = 0.83072516\n",
      "Iteration 7, loss = 0.36661076\n",
      "Iteration 8, loss = 0.29853474\n",
      "Iteration 9, loss = 0.24822594\n",
      "Iteration 10, loss = 0.41492325\n",
      "Iteration 11, loss = 1.78379272\n",
      "Iteration 12, loss = 1.34693437\n",
      "Iteration 13, loss = 0.57947102\n",
      "Iteration 14, loss = 0.31350430\n",
      "Iteration 15, loss = 0.24643458\n",
      "Iteration 16, loss = 0.22758208\n",
      "Iteration 17, loss = 0.24473287\n",
      "Iteration 18, loss = 0.21632534\n",
      "Iteration 19, loss = 0.21184125\n",
      "Iteration 20, loss = 0.22172344\n",
      "Iteration 21, loss = 0.20449432\n",
      "Iteration 22, loss = 0.21216585\n",
      "Iteration 23, loss = 0.20162846\n",
      "Iteration 24, loss = 0.17773195\n",
      "Iteration 25, loss = 0.18042404\n",
      "Iteration 26, loss = 0.16763818\n",
      "Iteration 27, loss = 0.22990041\n",
      "Iteration 28, loss = 0.33161881\n",
      "Iteration 29, loss = 0.18198030\n",
      "Iteration 30, loss = 0.18104927\n",
      "Iteration 31, loss = 0.16149376\n",
      "Iteration 32, loss = 0.15790364\n",
      "Iteration 33, loss = 0.17166079\n",
      "Iteration 34, loss = 0.16628571\n",
      "Iteration 35, loss = 0.17376077\n",
      "Iteration 36, loss = 0.19128738\n",
      "Iteration 37, loss = 0.15263435\n",
      "Iteration 38, loss = 0.16134032\n",
      "Iteration 39, loss = 0.16199240\n",
      "Iteration 40, loss = 0.15683049\n",
      "Iteration 41, loss = 0.26643479\n",
      "Iteration 42, loss = 0.97394114\n",
      "Iteration 43, loss = 0.81218562\n",
      "Iteration 44, loss = 0.20562009\n",
      "Iteration 45, loss = 0.16083486\n",
      "Iteration 46, loss = 0.15555449\n",
      "Iteration 47, loss = 0.15044480\n",
      "Iteration 48, loss = 0.15149854\n",
      "Iteration 49, loss = 0.15253323\n",
      "Iteration 50, loss = 0.14916566\n",
      "Iteration 51, loss = 0.15411271\n",
      "Iteration 52, loss = 0.14823849\n",
      "Iteration 53, loss = 0.15567227\n",
      "Iteration 54, loss = 0.16133104\n",
      "Iteration 55, loss = 0.14827631\n",
      "Iteration 56, loss = 0.14873610\n",
      "Iteration 57, loss = 0.15235610\n",
      "Iteration 58, loss = 0.15154208\n",
      "Iteration 59, loss = 0.14983203\n",
      "Iteration 60, loss = 0.15420210\n",
      "Iteration 61, loss = 0.16013581\n",
      "Iteration 62, loss = 0.16370119\n",
      "Iteration 63, loss = 0.15009410\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.58520356\n",
      "Iteration 2, loss = 0.69282103\n",
      "Iteration 3, loss = 0.34908418\n",
      "Iteration 4, loss = 0.26711031\n",
      "Iteration 5, loss = 0.45093593\n",
      "Iteration 6, loss = 0.53670484\n",
      "Iteration 7, loss = 1.08309977\n",
      "Iteration 8, loss = 0.25645601\n",
      "Iteration 9, loss = 0.22690729\n",
      "Iteration 10, loss = 0.25817336\n",
      "Iteration 11, loss = 0.20841617\n",
      "Iteration 12, loss = 0.19825610\n",
      "Iteration 13, loss = 0.18964123\n",
      "Iteration 14, loss = 0.23091004\n",
      "Iteration 15, loss = 0.19846482\n",
      "Iteration 16, loss = 0.48800122\n",
      "Iteration 17, loss = 0.21064513\n",
      "Iteration 18, loss = 0.78039765\n",
      "Iteration 19, loss = 0.39209170\n",
      "Iteration 20, loss = 0.24329094\n",
      "Iteration 21, loss = 0.17877355\n",
      "Iteration 22, loss = 0.24441914\n",
      "Iteration 23, loss = 1.07016614\n",
      "Iteration 24, loss = 0.41990489\n",
      "Iteration 25, loss = 0.15375139\n",
      "Iteration 26, loss = 0.14612194\n",
      "Iteration 27, loss = 0.15859904\n",
      "Iteration 28, loss = 0.15070535\n",
      "Iteration 29, loss = 0.15048850\n",
      "Iteration 30, loss = 0.14862173\n",
      "Iteration 31, loss = 0.14832089\n",
      "Iteration 32, loss = 0.15426400\n",
      "Iteration 33, loss = 0.15986206\n",
      "Iteration 34, loss = 0.15482896\n",
      "Iteration 35, loss = 0.15359349\n",
      "Iteration 36, loss = 0.17582356\n",
      "Iteration 37, loss = 0.14643638\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.94959570\n",
      "Iteration 2, loss = 0.99405438\n",
      "Iteration 3, loss = 0.30693570\n",
      "Iteration 4, loss = 0.71838183\n",
      "Iteration 5, loss = 2.00404006\n",
      "Iteration 6, loss = 0.75998893\n",
      "Iteration 7, loss = 0.30469863\n",
      "Iteration 8, loss = 0.22477354\n",
      "Iteration 9, loss = 0.19885145\n",
      "Iteration 10, loss = 0.17697913\n",
      "Iteration 11, loss = 0.18093333\n",
      "Iteration 12, loss = 0.17258723\n",
      "Iteration 13, loss = 0.16157724\n",
      "Iteration 14, loss = 0.15363671\n",
      "Iteration 15, loss = 0.15269298\n",
      "Iteration 16, loss = 0.14120870\n",
      "Iteration 17, loss = 0.13355693\n",
      "Iteration 18, loss = 0.11733827\n",
      "Iteration 19, loss = 0.11687323\n",
      "Iteration 20, loss = 0.11313836\n",
      "Iteration 21, loss = 0.10996651\n",
      "Iteration 22, loss = 0.10389587\n",
      "Iteration 23, loss = 0.10294207\n",
      "Iteration 24, loss = 0.08465362\n",
      "Iteration 25, loss = 0.08530181\n",
      "Iteration 26, loss = 0.06965665\n",
      "Iteration 27, loss = 0.07402426\n",
      "Iteration 28, loss = 0.07796792\n",
      "Iteration 29, loss = 0.07372804\n",
      "Iteration 30, loss = 0.06856814\n",
      "Iteration 31, loss = 0.06450901\n",
      "Iteration 32, loss = 0.06146261\n",
      "Iteration 33, loss = 0.05960154\n",
      "Iteration 34, loss = 0.05810631\n",
      "Iteration 35, loss = 0.05425148\n",
      "Iteration 36, loss = 0.05461473\n",
      "Iteration 37, loss = 0.05346897\n",
      "Iteration 38, loss = 0.04831520\n",
      "Iteration 39, loss = 0.04407031\n",
      "Iteration 40, loss = 0.04678838\n",
      "Iteration 41, loss = 0.04182475\n",
      "Iteration 42, loss = 0.05383491\n",
      "Iteration 43, loss = 0.03898719\n",
      "Iteration 44, loss = 0.67707394\n",
      "Iteration 45, loss = 1.84483871\n",
      "Iteration 46, loss = 0.24989543\n",
      "Iteration 47, loss = 0.13215706\n",
      "Iteration 48, loss = 0.10611250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49, loss = 0.09369331\n",
      "Iteration 50, loss = 0.08578773\n",
      "Iteration 51, loss = 0.07978358\n",
      "Iteration 52, loss = 0.07565518\n",
      "Iteration 53, loss = 0.07197897\n",
      "Iteration 54, loss = 0.06830458\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.11566008\n",
      "Iteration 2, loss = 1.24299612\n",
      "Iteration 3, loss = 0.59566321\n",
      "Iteration 4, loss = 0.26043160\n",
      "Iteration 5, loss = 0.24808691\n",
      "Iteration 6, loss = 0.26520963\n",
      "Iteration 7, loss = 0.65342026\n",
      "Iteration 8, loss = 0.49675430\n",
      "Iteration 9, loss = 0.23200852\n",
      "Iteration 10, loss = 0.22403476\n",
      "Iteration 11, loss = 0.20476828\n",
      "Iteration 12, loss = 0.21489565\n",
      "Iteration 13, loss = 1.61895102\n",
      "Iteration 14, loss = 2.15497052\n",
      "Iteration 15, loss = 0.91844080\n",
      "Iteration 16, loss = 0.59995883\n",
      "Iteration 17, loss = 0.28963168\n",
      "Iteration 18, loss = 0.23193428\n",
      "Iteration 19, loss = 0.22796374\n",
      "Iteration 20, loss = 0.20788783\n",
      "Iteration 21, loss = 0.20251791\n",
      "Iteration 22, loss = 0.20295133\n",
      "Iteration 23, loss = 0.19651158\n",
      "Iteration 24, loss = 0.20336586\n",
      "Iteration 25, loss = 0.19443588\n",
      "Iteration 26, loss = 0.20077516\n",
      "Iteration 27, loss = 0.18632862\n",
      "Iteration 28, loss = 0.18792519\n",
      "Iteration 29, loss = 0.18319955\n",
      "Iteration 30, loss = 0.19796157\n",
      "Iteration 31, loss = 0.18783887\n",
      "Iteration 32, loss = 0.19350070\n",
      "Iteration 33, loss = 0.20149695\n",
      "Iteration 34, loss = 0.17341246\n",
      "Iteration 35, loss = 0.17328053\n",
      "Iteration 36, loss = 0.19291592\n",
      "Iteration 37, loss = 0.18273309\n",
      "Iteration 38, loss = 0.19261000\n",
      "Iteration 39, loss = 0.35018648\n",
      "Iteration 40, loss = 0.84124784\n",
      "Iteration 41, loss = 1.37801399\n",
      "Iteration 42, loss = 0.29602128\n",
      "Iteration 43, loss = 0.20800095\n",
      "Iteration 44, loss = 0.19964229\n",
      "Iteration 45, loss = 0.18937183\n",
      "Iteration 46, loss = 0.18712720\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.03762978\n",
      "Iteration 2, loss = 0.41148967\n",
      "Iteration 3, loss = 0.89220548\n",
      "Iteration 4, loss = 0.22279027\n",
      "Iteration 5, loss = 0.20900022\n",
      "Iteration 6, loss = 0.26223748\n",
      "Iteration 7, loss = 0.71481503\n",
      "Iteration 8, loss = 0.70966898\n",
      "Iteration 9, loss = 0.53943804\n",
      "Iteration 10, loss = 0.24566159\n",
      "Iteration 11, loss = 0.21030212\n",
      "Iteration 12, loss = 0.21571429\n",
      "Iteration 13, loss = 0.49796227\n",
      "Iteration 14, loss = 1.53318256\n",
      "Iteration 15, loss = 0.22208349\n",
      "Iteration 16, loss = 0.21238566\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.81401504\n",
      "Iteration 2, loss = 0.99300505\n",
      "Iteration 3, loss = 1.28041386\n",
      "Iteration 4, loss = 0.69624678\n",
      "Iteration 5, loss = 0.36824231\n",
      "Iteration 6, loss = 0.25352420\n",
      "Iteration 7, loss = 0.24251400\n",
      "Iteration 8, loss = 0.22617825\n",
      "Iteration 9, loss = 0.23734321\n",
      "Iteration 10, loss = 0.21715167\n",
      "Iteration 11, loss = 0.18052425\n",
      "Iteration 12, loss = 0.16738862\n",
      "Iteration 13, loss = 0.15241454\n",
      "Iteration 14, loss = 0.15622684\n",
      "Iteration 15, loss = 1.09804900\n",
      "Iteration 16, loss = 0.60070731\n",
      "Iteration 17, loss = 0.24852305\n",
      "Iteration 18, loss = 0.27373778\n",
      "Iteration 19, loss = 0.13751119\n",
      "Iteration 20, loss = 0.19692870\n",
      "Iteration 21, loss = 0.15677858\n",
      "Iteration 22, loss = 0.25272548\n",
      "Iteration 23, loss = 0.14947106\n",
      "Iteration 24, loss = 0.17414986\n",
      "Iteration 25, loss = 0.13411413\n",
      "Iteration 26, loss = 0.13045174\n",
      "Iteration 27, loss = 0.12470583\n",
      "Iteration 28, loss = 0.13493453\n",
      "Iteration 29, loss = 0.12366575\n",
      "Iteration 30, loss = 0.14164851\n",
      "Iteration 31, loss = 0.13052559\n",
      "Iteration 32, loss = 0.12247051\n",
      "Iteration 33, loss = 0.13642225\n",
      "Iteration 34, loss = 0.12473654\n",
      "Iteration 35, loss = 0.15019179\n",
      "Iteration 36, loss = 1.63290943\n",
      "Iteration 37, loss = 1.01026256\n",
      "Iteration 38, loss = 0.21455196\n",
      "Iteration 39, loss = 0.19095444\n",
      "Iteration 40, loss = 0.12255104\n",
      "Iteration 41, loss = 0.11924312\n",
      "Iteration 42, loss = 0.12345728\n",
      "Iteration 43, loss = 0.12031577\n",
      "Iteration 44, loss = 0.13517453\n",
      "Iteration 45, loss = 0.13145344\n",
      "Iteration 46, loss = 0.12222648\n",
      "Iteration 47, loss = 0.12185839\n",
      "Iteration 48, loss = 0.12494677\n",
      "Iteration 49, loss = 0.12417208\n",
      "Iteration 50, loss = 0.13304347\n",
      "Iteration 51, loss = 0.12026184\n",
      "Iteration 52, loss = 0.11674111\n",
      "Iteration 53, loss = 0.12049221\n",
      "Iteration 54, loss = 0.12725927\n",
      "Iteration 55, loss = 0.14313835\n",
      "Iteration 56, loss = 0.12665758\n",
      "Iteration 57, loss = 0.12628432\n",
      "Iteration 58, loss = 0.13483130\n",
      "Iteration 59, loss = 0.12062151\n",
      "Iteration 60, loss = 0.11723490\n",
      "Iteration 61, loss = 0.12186757\n",
      "Iteration 62, loss = 0.12350638\n",
      "Iteration 63, loss = 0.12065512\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.87795462\n",
      "Iteration 2, loss = 1.16478000\n",
      "Iteration 3, loss = 0.96222596\n",
      "Iteration 4, loss = 1.12441874\n",
      "Iteration 5, loss = 0.58990021\n",
      "Iteration 6, loss = 0.34705953\n",
      "Iteration 7, loss = 0.29095365\n",
      "Iteration 8, loss = 0.24042408\n",
      "Iteration 9, loss = 0.26286086\n",
      "Iteration 10, loss = 0.25738366\n",
      "Iteration 11, loss = 0.76109807\n",
      "Iteration 12, loss = 0.69206643\n",
      "Iteration 13, loss = 0.28332663\n",
      "Iteration 14, loss = 0.22665315\n",
      "Iteration 15, loss = 0.21554042\n",
      "Iteration 16, loss = 0.76170353\n",
      "Iteration 17, loss = 0.88907652\n",
      "Iteration 18, loss = 0.28603928\n",
      "Iteration 19, loss = 0.16222238\n",
      "Iteration 20, loss = 0.14281763\n",
      "Iteration 21, loss = 0.13864017\n",
      "Iteration 22, loss = 0.14680107\n",
      "Iteration 23, loss = 0.15292039\n",
      "Iteration 24, loss = 0.15112456\n",
      "Iteration 25, loss = 0.14271194\n",
      "Iteration 26, loss = 0.14037780\n",
      "Iteration 27, loss = 0.13890915\n",
      "Iteration 28, loss = 0.14213861\n",
      "Iteration 29, loss = 0.13844144\n",
      "Iteration 30, loss = 0.15829233\n",
      "Iteration 31, loss = 0.13363594\n",
      "Iteration 32, loss = 0.16219734\n",
      "Iteration 33, loss = 0.15774768\n",
      "Iteration 34, loss = 0.13480337\n",
      "Iteration 35, loss = 0.13705187\n",
      "Iteration 36, loss = 0.15814365\n",
      "Iteration 37, loss = 0.13619535\n",
      "Iteration 38, loss = 0.14531099\n",
      "Iteration 39, loss = 0.14297363\n",
      "Iteration 40, loss = 0.13110593\n",
      "Iteration 41, loss = 0.14091634\n",
      "Iteration 42, loss = 0.13418003\n",
      "Iteration 43, loss = 0.14555989\n",
      "Iteration 44, loss = 0.12916486\n",
      "Iteration 45, loss = 0.24236539\n",
      "Iteration 46, loss = 0.37845886\n",
      "Iteration 47, loss = 0.25775405\n",
      "Iteration 48, loss = 0.25082964\n",
      "Iteration 49, loss = 1.17245974\n",
      "Iteration 50, loss = 0.87490176\n",
      "Iteration 51, loss = 0.16144890\n",
      "Iteration 52, loss = 0.13651300\n",
      "Iteration 53, loss = 0.13117556\n",
      "Iteration 54, loss = 0.14289893\n",
      "Iteration 55, loss = 0.14771513\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.88919788\n",
      "Iteration 2, loss = 0.85078774\n",
      "Iteration 3, loss = 1.07145628\n",
      "Iteration 4, loss = 1.10038065\n",
      "Iteration 5, loss = 0.51167536\n",
      "Iteration 6, loss = 0.23602433\n",
      "Iteration 7, loss = 0.20444244\n",
      "Iteration 8, loss = 0.20622389\n",
      "Iteration 9, loss = 0.23954653\n",
      "Iteration 10, loss = 0.23558078\n",
      "Iteration 11, loss = 0.21207799\n",
      "Iteration 12, loss = 0.24328008\n",
      "Iteration 13, loss = 1.33021343\n",
      "Iteration 14, loss = 1.59960820\n",
      "Iteration 15, loss = 0.55384934\n",
      "Iteration 16, loss = 0.39853183\n",
      "Iteration 17, loss = 0.20139105\n",
      "Iteration 18, loss = 0.16319356\n",
      "Iteration 19, loss = 0.14701376\n",
      "Iteration 20, loss = 0.14861474\n",
      "Iteration 21, loss = 0.14768195\n",
      "Iteration 22, loss = 0.15414714\n",
      "Iteration 23, loss = 0.15681649\n",
      "Iteration 24, loss = 0.15874763\n",
      "Iteration 25, loss = 0.14923813\n",
      "Iteration 26, loss = 0.14981629\n",
      "Iteration 27, loss = 0.14650503\n",
      "Iteration 28, loss = 0.14844281\n",
      "Iteration 29, loss = 0.14539332\n",
      "Iteration 30, loss = 0.16657886\n",
      "Iteration 31, loss = 0.14337129\n",
      "Iteration 32, loss = 0.16653770\n",
      "Iteration 33, loss = 0.16067462\n",
      "Iteration 34, loss = 0.14851244\n",
      "Iteration 35, loss = 0.14570800\n",
      "Iteration 36, loss = 0.20856215\n",
      "Iteration 37, loss = 0.15921830\n",
      "Iteration 38, loss = 0.15051351\n",
      "Iteration 39, loss = 0.15394853\n",
      "Iteration 40, loss = 0.14127631\n",
      "Iteration 41, loss = 0.14780336\n",
      "Iteration 42, loss = 0.14602452\n",
      "Iteration 43, loss = 0.15027917\n",
      "Iteration 44, loss = 0.14645248\n",
      "Iteration 45, loss = 0.20495159\n",
      "Iteration 46, loss = 0.92332293\n",
      "Iteration 47, loss = 0.47684740\n",
      "Iteration 48, loss = 0.29644536\n",
      "Iteration 49, loss = 0.22042067\n",
      "Iteration 50, loss = 0.22103848\n",
      "Iteration 51, loss = 0.14188513\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.97164814\n",
      "Iteration 2, loss = 1.27643173\n",
      "Iteration 3, loss = 0.77064986\n",
      "Iteration 4, loss = 0.37967945\n",
      "Iteration 5, loss = 0.34683230\n",
      "Iteration 6, loss = 0.43995636\n",
      "Iteration 7, loss = 0.44893554\n",
      "Iteration 8, loss = 1.19249256\n",
      "Iteration 9, loss = 0.64757184\n",
      "Iteration 10, loss = 0.27650354\n",
      "Iteration 11, loss = 0.18314690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.17871991\n",
      "Iteration 13, loss = 0.17149088\n",
      "Iteration 14, loss = 0.20100007\n",
      "Iteration 15, loss = 0.20062170\n",
      "Iteration 16, loss = 0.24424185\n",
      "Iteration 17, loss = 0.20650643\n",
      "Iteration 18, loss = 0.19751632\n",
      "Iteration 19, loss = 0.20319175\n",
      "Iteration 20, loss = 0.21312116\n",
      "Iteration 21, loss = 0.30407721\n",
      "Iteration 22, loss = 0.17181981\n",
      "Iteration 23, loss = 0.23757022\n",
      "Iteration 24, loss = 0.16921654\n",
      "Iteration 25, loss = 0.39026090\n",
      "Iteration 26, loss = 1.67558154\n",
      "Iteration 27, loss = 0.51142246\n",
      "Iteration 28, loss = 0.30003756\n",
      "Iteration 29, loss = 0.23834780\n",
      "Iteration 30, loss = 0.19311545\n",
      "Iteration 31, loss = 0.14499250\n",
      "Iteration 32, loss = 0.14803005\n",
      "Iteration 33, loss = 0.15166234\n",
      "Iteration 34, loss = 0.14470310\n",
      "Iteration 35, loss = 0.15419784\n",
      "Iteration 36, loss = 0.16062386\n",
      "Iteration 37, loss = 0.14264914\n",
      "Iteration 38, loss = 0.13927893\n",
      "Iteration 39, loss = 0.13995479\n",
      "Iteration 40, loss = 0.14349921\n",
      "Iteration 41, loss = 0.13745949\n",
      "Iteration 42, loss = 0.13730172\n",
      "Iteration 43, loss = 0.13862807\n",
      "Iteration 44, loss = 0.15492039\n",
      "Iteration 45, loss = 0.16083179\n",
      "Iteration 46, loss = 0.14520481\n",
      "Iteration 47, loss = 0.13593739\n",
      "Iteration 48, loss = 0.14514944\n",
      "Iteration 49, loss = 0.14329682\n",
      "Iteration 50, loss = 0.20653586\n",
      "Iteration 51, loss = 0.23602974\n",
      "Iteration 52, loss = 0.14564837\n",
      "Iteration 53, loss = 0.13977721\n",
      "Iteration 54, loss = 0.14032272\n",
      "Iteration 55, loss = 0.16018801\n",
      "Iteration 56, loss = 0.14655584\n",
      "Iteration 57, loss = 0.13680857\n",
      "Iteration 58, loss = 0.15004130\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.47771588\n",
      "Iteration 2, loss = 0.35785748\n",
      "Iteration 3, loss = 1.20373707\n",
      "Iteration 4, loss = 1.49998183\n",
      "Iteration 5, loss = 1.17638801\n",
      "Iteration 6, loss = 1.09072846\n",
      "Iteration 7, loss = 0.62756761\n",
      "Iteration 8, loss = 0.28943991\n",
      "Iteration 9, loss = 0.27018879\n",
      "Iteration 10, loss = 0.25525755\n",
      "Iteration 11, loss = 0.26027681\n",
      "Iteration 12, loss = 0.27237895\n",
      "Iteration 13, loss = 0.22157759\n",
      "Iteration 14, loss = 0.21723455\n",
      "Iteration 15, loss = 0.22765602\n",
      "Iteration 16, loss = 0.22898854\n",
      "Iteration 17, loss = 0.21707830\n",
      "Iteration 18, loss = 0.18452281\n",
      "Iteration 19, loss = 0.17148546\n",
      "Iteration 20, loss = 0.17666714\n",
      "Iteration 21, loss = 0.27335548\n",
      "Iteration 22, loss = 1.21648123\n",
      "Iteration 23, loss = 1.21315054\n",
      "Iteration 24, loss = 0.66346930\n",
      "Iteration 25, loss = 0.30855644\n",
      "Iteration 26, loss = 0.20966761\n",
      "Iteration 27, loss = 0.15915398\n",
      "Iteration 28, loss = 0.15383588\n",
      "Iteration 29, loss = 0.16430162\n",
      "Iteration 30, loss = 0.17270597\n",
      "Iteration 31, loss = 0.15584343\n",
      "Iteration 32, loss = 0.15595342\n",
      "Iteration 33, loss = 0.14823812\n",
      "Iteration 34, loss = 0.14813207\n",
      "Iteration 35, loss = 0.16006133\n",
      "Iteration 36, loss = 0.17765577\n",
      "Iteration 37, loss = 0.15117308\n",
      "Iteration 38, loss = 0.14896944\n",
      "Iteration 39, loss = 0.14933508\n",
      "Iteration 40, loss = 0.15653872\n",
      "Iteration 41, loss = 0.14481722\n",
      "Iteration 42, loss = 0.14964807\n",
      "Iteration 43, loss = 0.15314707\n",
      "Iteration 44, loss = 0.16534222\n",
      "Iteration 45, loss = 0.16876006\n",
      "Iteration 46, loss = 0.16011280\n",
      "Iteration 47, loss = 0.14415103\n",
      "Iteration 48, loss = 0.15230452\n",
      "Iteration 49, loss = 0.17218162\n",
      "Iteration 50, loss = 0.20835955\n",
      "Iteration 51, loss = 0.15139917\n",
      "Iteration 52, loss = 0.14878360\n",
      "Iteration 53, loss = 0.15480601\n",
      "Iteration 54, loss = 0.15529424\n",
      "Iteration 55, loss = 0.16070763\n",
      "Iteration 56, loss = 0.15197608\n",
      "Iteration 57, loss = 0.14726117\n",
      "Iteration 58, loss = 0.15914114\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.51978772\n",
      "Iteration 2, loss = 1.14878425\n",
      "Iteration 3, loss = 0.43151411\n",
      "Iteration 4, loss = 0.29483071\n",
      "Iteration 5, loss = 0.33928586\n",
      "Iteration 6, loss = 0.83072516\n",
      "Iteration 7, loss = 0.36661076\n",
      "Iteration 8, loss = 0.29853474\n",
      "Iteration 9, loss = 0.24822594\n",
      "Iteration 10, loss = 0.41492325\n",
      "Iteration 11, loss = 1.78379272\n",
      "Iteration 12, loss = 1.34693437\n",
      "Iteration 13, loss = 0.57947102\n",
      "Iteration 14, loss = 0.31350430\n",
      "Iteration 15, loss = 0.24643458\n",
      "Iteration 16, loss = 0.22758208\n",
      "Iteration 17, loss = 0.24473287\n",
      "Iteration 18, loss = 0.21632534\n",
      "Iteration 19, loss = 0.21184125\n",
      "Iteration 20, loss = 0.22172344\n",
      "Iteration 21, loss = 0.20449432\n",
      "Iteration 22, loss = 0.21216585\n",
      "Iteration 23, loss = 0.20162846\n",
      "Iteration 24, loss = 0.17773195\n",
      "Iteration 25, loss = 0.18042404\n",
      "Iteration 26, loss = 0.16763818\n",
      "Iteration 27, loss = 0.22990041\n",
      "Iteration 28, loss = 0.33161881\n",
      "Iteration 29, loss = 0.18198030\n",
      "Iteration 30, loss = 0.18104927\n",
      "Iteration 31, loss = 0.16149376\n",
      "Iteration 32, loss = 0.15790364\n",
      "Iteration 33, loss = 0.17166079\n",
      "Iteration 34, loss = 0.16628571\n",
      "Iteration 35, loss = 0.17376077\n",
      "Iteration 36, loss = 0.19128738\n",
      "Iteration 37, loss = 0.15263435\n",
      "Iteration 38, loss = 0.16134032\n",
      "Iteration 39, loss = 0.16199240\n",
      "Iteration 40, loss = 0.15683049\n",
      "Iteration 41, loss = 0.26643479\n",
      "Iteration 42, loss = 0.97394114\n",
      "Iteration 43, loss = 0.81218562\n",
      "Iteration 44, loss = 0.20562009\n",
      "Iteration 45, loss = 0.16083486\n",
      "Iteration 46, loss = 0.15555449\n",
      "Iteration 47, loss = 0.15044480\n",
      "Iteration 48, loss = 0.15149854\n",
      "Iteration 49, loss = 0.15253323\n",
      "Iteration 50, loss = 0.14916566\n",
      "Iteration 51, loss = 0.15411271\n",
      "Iteration 52, loss = 0.14823849\n",
      "Iteration 53, loss = 0.15567227\n",
      "Iteration 54, loss = 0.16133104\n",
      "Iteration 55, loss = 0.14827631\n",
      "Iteration 56, loss = 0.14873610\n",
      "Iteration 57, loss = 0.15235610\n",
      "Iteration 58, loss = 0.15154208\n",
      "Iteration 59, loss = 0.14983203\n",
      "Iteration 60, loss = 0.15420210\n",
      "Iteration 61, loss = 0.16013581\n",
      "Iteration 62, loss = 0.16370119\n",
      "Iteration 63, loss = 0.15009410\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.58520356\n",
      "Iteration 2, loss = 0.69282103\n",
      "Iteration 3, loss = 0.34908418\n",
      "Iteration 4, loss = 0.26711031\n",
      "Iteration 5, loss = 0.45093593\n",
      "Iteration 6, loss = 0.53670484\n",
      "Iteration 7, loss = 1.08309977\n",
      "Iteration 8, loss = 0.25645601\n",
      "Iteration 9, loss = 0.22690729\n",
      "Iteration 10, loss = 0.25817336\n",
      "Iteration 11, loss = 0.20841617\n",
      "Iteration 12, loss = 0.19825610\n",
      "Iteration 13, loss = 0.18964123\n",
      "Iteration 14, loss = 0.23091004\n",
      "Iteration 15, loss = 0.19846482\n",
      "Iteration 16, loss = 0.48800122\n",
      "Iteration 17, loss = 0.21064513\n",
      "Iteration 18, loss = 0.78039765\n",
      "Iteration 19, loss = 0.39209170\n",
      "Iteration 20, loss = 0.24329094\n",
      "Iteration 21, loss = 0.17877355\n",
      "Iteration 22, loss = 0.24441914\n",
      "Iteration 23, loss = 1.07016614\n",
      "Iteration 24, loss = 0.41990489\n",
      "Iteration 25, loss = 0.15375139\n",
      "Iteration 26, loss = 0.14612194\n",
      "Iteration 27, loss = 0.15859904\n",
      "Iteration 28, loss = 0.15070535\n",
      "Iteration 29, loss = 0.15048850\n",
      "Iteration 30, loss = 0.14862173\n",
      "Iteration 31, loss = 0.14832089\n",
      "Iteration 32, loss = 0.15426400\n",
      "Iteration 33, loss = 0.15986206\n",
      "Iteration 34, loss = 0.15482896\n",
      "Iteration 35, loss = 0.15359349\n",
      "Iteration 36, loss = 0.17582356\n",
      "Iteration 37, loss = 0.14643638\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.94959570\n",
      "Iteration 2, loss = 0.99405438\n",
      "Iteration 3, loss = 0.30693570\n",
      "Iteration 4, loss = 0.71838183\n",
      "Iteration 5, loss = 2.00404006\n",
      "Iteration 6, loss = 0.75998893\n",
      "Iteration 7, loss = 0.30469863\n",
      "Iteration 8, loss = 0.22477354\n",
      "Iteration 9, loss = 0.19885145\n",
      "Iteration 10, loss = 0.17697913\n",
      "Iteration 11, loss = 0.18093333\n",
      "Iteration 12, loss = 0.17258723\n",
      "Iteration 13, loss = 0.16157724\n",
      "Iteration 14, loss = 0.15363671\n",
      "Iteration 15, loss = 0.15269298\n",
      "Iteration 16, loss = 0.14120870\n",
      "Iteration 17, loss = 0.13355693\n",
      "Iteration 18, loss = 0.11733827\n",
      "Iteration 19, loss = 0.11687323\n",
      "Iteration 20, loss = 0.11313836\n",
      "Iteration 21, loss = 0.10996651\n",
      "Iteration 22, loss = 0.10389587\n",
      "Iteration 23, loss = 0.10294207\n",
      "Iteration 24, loss = 0.08465362\n",
      "Iteration 25, loss = 0.08530181\n",
      "Iteration 26, loss = 0.06965665\n",
      "Iteration 27, loss = 0.07402426\n",
      "Iteration 28, loss = 0.07796792\n",
      "Iteration 29, loss = 0.07372804\n",
      "Iteration 30, loss = 0.06856814\n",
      "Iteration 31, loss = 0.06450901\n",
      "Iteration 32, loss = 0.06146261\n",
      "Iteration 33, loss = 0.05960154\n",
      "Iteration 34, loss = 0.05810631\n",
      "Iteration 35, loss = 0.05425148\n",
      "Iteration 36, loss = 0.05461473\n",
      "Iteration 37, loss = 0.05346897\n",
      "Iteration 38, loss = 0.04831520\n",
      "Iteration 39, loss = 0.04407031\n",
      "Iteration 40, loss = 0.04678838\n",
      "Iteration 41, loss = 0.04182475\n",
      "Iteration 42, loss = 0.05383491\n",
      "Iteration 43, loss = 0.03898719\n",
      "Iteration 44, loss = 0.67707394\n",
      "Iteration 45, loss = 1.84483871\n",
      "Iteration 46, loss = 0.24989543\n",
      "Iteration 47, loss = 0.13215706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48, loss = 0.10611250\n",
      "Iteration 49, loss = 0.09369331\n",
      "Iteration 50, loss = 0.08578773\n",
      "Iteration 51, loss = 0.07978358\n",
      "Iteration 52, loss = 0.07565518\n",
      "Iteration 53, loss = 0.07197897\n",
      "Iteration 54, loss = 0.06830458\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy =  [0.84415584 0.89123377 0.8262987  0.93668831 0.93344156 0.88798701\n",
      " 0.98701299 0.97398374 0.95447154 0.89593496]\n",
      "Accuracy mean and std : 0.913 (0.051)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.70      0.64       692\n",
      "           1       0.96      0.94      0.95      5465\n",
      "\n",
      "    accuracy                           0.91      6157\n",
      "   macro avg       0.78      0.82      0.80      6157\n",
      "weighted avg       0.92      0.91      0.92      6157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KFold cross validation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv=StratifiedKFold(n_splits = 10)\n",
    "\n",
    "class_names = ['0','1']\n",
    "y_pred = cross_val_predict(modelMLP4, X, Y, cv = cv)\n",
    "\n",
    "cv_score_for_LR = cross_val_score(modelMLP4, X, Y, cv = 10)\n",
    "print(\"Accuracy = \",cv_score_for_LR)\n",
    "\n",
    "print('Accuracy mean and std : %.3f (%.3f)' % (mean(cv_score_for_LR), std(cv_score_for_LR)))\n",
    "\n",
    "print(classification_report(Y, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f40a79",
   "metadata": {},
   "source": [
    "# ROC Curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d44d9afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# predict probabilities\n",
    "pred_prob1 = modelLR1.predict_proba(x_test)\n",
    "pred_prob2 = modelMLP1.predict_proba(x_test)\n",
    "pred_prob3 = modelLR2.predict_proba(x_test)\n",
    "pred_prob4 = modelMLP2.predict_proba(x_test)\n",
    "pred_prob5 = modelLR3.predict_proba(x_test)\n",
    "pred_prob6 = modelMLP3.predict_proba(x_test)\n",
    "pred_prob7 = modelLR4.predict_proba(x_test)\n",
    "pred_prob8 = modelMLP4.predict_proba(x_test)\n",
    "'''pred_prob9 = modelMLP1.predict_proba(x_test_smt)\n",
    "pred_prob10 = modelABC.predict_proba(x_test_smt)\n",
    "pred_prob11 = modelGNB.predict_proba(x_test_smt)'''\n",
    "\n",
    "# roc curve for models\n",
    "fpr1, tpr1, thresh1 = roc_curve(y_test, pred_prob1[:,1], pos_label = 1)\n",
    "fpr2, tpr2, thresh2 = roc_curve(y_test, pred_prob2[:,1], pos_label = 1)\n",
    "fpr3, tpr3, thresh3 = roc_curve(y_test, pred_prob3[:,1], pos_label = 1)\n",
    "fpr4, tpr4, thresh4 = roc_curve(y_test, pred_prob4[:,1], pos_label = 1)\n",
    "fpr5, tpr5, thresh5 = roc_curve(y_test, pred_prob5[:,1], pos_label = 1)\n",
    "fpr6, tpr6, thresh6 = roc_curve(y_test, pred_prob6[:,1], pos_label = 1)\n",
    "fpr7, tpr7, thresh7 = roc_curve(y_test, pred_prob7[:,1], pos_label = 1)\n",
    "fpr8, tpr8, thresh8 = roc_curve(y_test, pred_prob8[:,1], pos_label = 1)\n",
    "'''fpr9, tpr9, thresh9 = roc_curve(y_test_smt, pred_prob9[:,1], pos_label = 1)\n",
    "fpr10, tpr10, thresh10 = roc_curve(y_test_smt, pred_prob10[:,1], pos_label = 1)\n",
    "fpr11, tpr11, thresh11 = roc_curve(y_test_smt, pred_prob11[:,1], pos_label = 1)'''\n",
    "\n",
    "# roc curve for tpr = fpr \n",
    "random_probs = [0 for i in range(len(y_test))]\n",
    "p_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee555136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import seaborn as sns\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "# plot roc curves\n",
    "plt.plot(fpr1, tpr1, linestyle = '--', color = 'blue', label = 'Without Preprocess LR')\n",
    "plt.plot(fpr2, tpr2, linestyle = '--', color = 'brown', label = 'Without Preprocess MLP')\n",
    "plt.plot(fpr3, tpr3, linestyle = '--', color = 'darkorange', label = 'SMOTE Balancing LR')\n",
    "plt.plot(fpr4, tpr4, linestyle = '--', color = 'purple', label = 'SMOTE Balancing MLP')\n",
    "plt.plot(fpr5, tpr5, linestyle = '--', color = 'gray', label = 'NearMiss Balancing LR')\n",
    "plt.plot(fpr6, tpr6, linestyle = '--', color = 'red', label = 'NearMiss Balancing MLP')\n",
    "plt.plot(fpr7, tpr7, linestyle = '--', color = 'magenta', label = 'SMOTETomek Balancing LR')\n",
    "plt.plot(fpr8, tpr8, linestyle = '--', color = 'cyan', label = 'SMOTETomek Balancing MLP')\n",
    "'''plt.plot(fpr9, tpr9, linestyle = '--', color = 'teal', label = 'MLP')\n",
    "plt.plot(fpr10, tpr10, linestyle = '--', color = 'brown', label = 'ABC')\n",
    "plt.plot(fpr11, tpr11, linestyle = '--', color = 'gray', label = 'GNB')'''\n",
    "plt.plot(p_fpr, p_tpr, linestyle = '--', color = 'black')\n",
    "# title\n",
    "plt.title('ROC curve',fontsize=14)\n",
    "# x label\n",
    "plt.xlabel('False Positive Rate',fontsize=14)\n",
    "# y label\n",
    "plt.ylabel('True Positive rate',fontsize=14)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "#plt.savefig('Cesarean ROC.jpg',dpi=300)\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b954f6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic: f1=0.975 auc=0.990\n",
      "Logistic: f1=0.937 auc=0.956\n",
      "Logistic: f1=0.945 auc=0.990\n",
      "Logistic: f1=0.974 auc=0.993\n",
      "Logistic: f1=0.910 auc=0.993\n",
      "Logistic: f1=0.895 auc=0.974\n",
      "Logistic: f1=0.974 auc=0.989\n",
      "Logistic: f1=0.982 auc=0.990\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# precision-recall curve and f1\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# predict probabilities\n",
    "lr_probs = modelLR1.predict_proba(x_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# predict class values\n",
    "yhat = modelLR1.predict(x_test)\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\n",
    "lr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)\n",
    "# summarize scores\n",
    "print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "# plot the precision-recall curves\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Without Preprocess LR')\n",
    "\n",
    "\n",
    "\n",
    "# predict probabilities\n",
    "lr_probs = modelMLP1.predict_proba(x_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# predict class values\n",
    "yhat = modelMLP1.predict(x_test)\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\n",
    "lr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)\n",
    "# summarize scores\n",
    "print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "# plot the precision-recall curves\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Without Preprocess MLP')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# predict probabilities\n",
    "lr_probs = modelLR2.predict_proba(x_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# predict class values\n",
    "yhat = modelLR2.predict(x_test)\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\n",
    "lr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)\n",
    "# summarize scores\n",
    "print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "# plot the precision-recall curves\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='SMOTE Balancing LR')\n",
    "\n",
    "\n",
    "\n",
    "# predict probabilities\n",
    "lr_probs = modelMLP2.predict_proba(x_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# predict class values\n",
    "yhat = modelMLP2.predict(x_test)\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\n",
    "lr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)\n",
    "# summarize scores\n",
    "print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "# plot the precision-recall curves\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='SMOTE Balancing MLP')\n",
    "\n",
    "\n",
    "\n",
    "# predict probabilities\n",
    "lr_probs = modelLR3.predict_proba(x_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# predict class values\n",
    "yhat = modelLR3.predict(x_test)\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\n",
    "lr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)\n",
    "# summarize scores\n",
    "print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "# plot the precision-recall curves\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='NearMiss Balancing LR')\n",
    "\n",
    "\n",
    "\n",
    "# predict probabilities\n",
    "lr_probs = modelMLP3.predict_proba(x_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# predict class values\n",
    "yhat = modelMLP3.predict(x_test)\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\n",
    "lr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)\n",
    "# summarize scores\n",
    "print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "# plot the precision-recall curves\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='NearMiss Balancing MLP')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# predict probabilities\n",
    "lr_probs = modelLR4.predict_proba(x_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# predict class values\n",
    "yhat = modelLR4.predict(x_test)\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\n",
    "lr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)\n",
    "# summarize scores\n",
    "print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "# plot the precision-recall curves\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='SMOTETomek Balancing LR')\n",
    "\n",
    "\n",
    "\n",
    "# predict probabilities\n",
    "lr_probs = modelMLP4.predict_proba(x_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# predict class values\n",
    "yhat = modelMLP4.predict(x_test)\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\n",
    "lr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)\n",
    "# summarize scores\n",
    "print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "# plot the precision-recall curves\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='SMOTETomek Balancing MLP')\n",
    "\n",
    "# No Skill\n",
    "no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall',fontsize=14)\n",
    "pyplot.ylabel('Precision',fontsize=14)\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "plt.grid()\n",
    "# show the plot\n",
    "#plt.savefig('Cesarean Preciion Recall.jpg',dpi=300)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e04d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
